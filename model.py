#copyright joshuah.rainstar@gmail.com 2025
#protected under license and copyright -proprietary software

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  PER-MODULE CONVEXITY AUDIT  (ConvexGPT, May-2025)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
#  Symbols
#  -------
#      â€¢ x  â€“ module input               â€¢ z â€“ intermediate variable
#      â€¢ f  â€“ module map  z = f(x)
#      â€¢ A,B,W  â€“ parameter matrices     â€¢ âŠ™ â€“ Hadamard product
#      â€¢ Ïƒâº  â€“ softplus                  â€¢ â–½  â€“ row-simplex weights (âˆ‘=1, â‰¥0)
#
#      â€œconvexâ€      :  f(Î»xâ‚+(1-Î»)xâ‚‚) â‰¤ Î»f(xâ‚)+(1-Î»)f(xâ‚‚)   âˆ€Î»âˆˆ[0,1]
#      â€œhull-pres.â€  :  f(x) âˆˆ  conv{tokens in x}
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#    Component                        | Convex in x ? | Hull-preserving ? | Proof sketch
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|---------------|-------------------|----------------------------------------------------
#  ConvexEmbedding                    | âœ“             | n/a               | PositiveLinearHK w/ Ïƒâº â‡’ Wâ‰¥0  â‡’ affineâºÏƒâº â‡’ convex
#  InterleavedPhaseChannelizer        | âœ“             | âœ“                 | Ï†áµ¢ =  âˆ‘â±¼ Káµ¢â±¼ xâ±¼  ;  K row-simplex â–½ â‡’ convex comb.
#  ConvexRoPE                         | âœ“             | âœ“                 | Î¸ = Ïƒâº(WÂ·t) â‰¥ 0 ; rotation is element-wise linear.
#  ScalarHull / VectorHull            | âœ“             | n/a               | ICNN: zâ‚=Ïƒâº(Aâ‚€x+bâ‚€); zâ‚‚=Ïƒâº(Aâ‚zâ‚+bâ‚)+â€¦  ,  Aâ‚–â‰¥0.
#  ConvexMixer  (A(x)V)               | âœ“             | âœ“                 | A(x) row-simplex (softmax of convex scores); V const.
#  LinearPreMix (square-norm rows)    | âœ“             | âœ“                 | Wâ‰¥0 by ÏƒâºÂ²; rows pre-normalised â‡’ convex comb. per head
#  Residual Gates  g(x)               | âœ“             | âœ“                 | g(x)=1-exp(-Ïƒâº(Wx))  âˆˆ(0,1)  â‡’ x+g(x)Î”  is convex hull.
#  FrozenAffine (Î³Â·(x-Î¼)/Ïƒ + Î²)       | affine, const | âœ“                 | Î¼,Ïƒ,Î³,Î² frozen â‡’ linear per token.
#  VectorHull Feed-Forward            | âœ“             | n/a               | same ICNN proof as above.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
#  Detailed guarantees
#  -------------------
#
#  1. PositiveLinearHK (d_outÃ—d_in)
#       W_raw  â”€Ïƒâºâ†’  W=Ïƒâº(W_raw)Â²  â‰¥ 0
#       f(x)=W x+b   is linear with non-negative weights â‡’ convex.
#
#  2. ICNN blocks (ScalarHull, VectorHull, KCN, BatchedICNN)
#       â€¢ First layer: zâ‚ = Ïƒâº(Aâ‚€ x + bâ‚€) ,  Aâ‚€ â‰¥ 0
#       â€¢ Hidden k:    z_{k+1} = Ïƒâº(A_k x + B_k z_k + b_k)
#                      with A_k â‰¥0 , B_k â‰¥0
#       â€¢ Output:      âˆ‘ monotone convex â‡’ convex (Amos & Xu 2017).
#
#  3. InterleavedPhaseChannelizer
#       Ï†[i] = âˆ‘â±¼ K[i,j]Â·x_c[j] ,  K row-wise normalised, Kâ‰¥0
#       â‡’ each Ï†[i] is inside conv{x_c} â‡’ convex & hull-preserving.
#
#  4. ConvexRoPE
#       Î¸ = Ïƒâº(W t)  (monotone, convex in t)
#       Rotation acts pair-wise:  (x,y) â†¦ (x cosÎ¸ âˆ’ y sinÎ¸ , â€¦)
#       Î¸ is constant w.r.t. (x,y)  â‡’ linear map â‡’ convex & hull safe.
#
#  5. ConvexMixer
#       â€¢ Scores f_q,f_k  convex scalars â‡’ f_q+f_k convex.
#       â€¢ Softmax over âˆ’Ï„ Â· scores  â‡’ A(x)  row-simplex â–½.
#       â€¢ Output  y = A(x) V ,  V constant.  Composition convex (Boyd Â§3.2.4).
#
#  6. LinearPreMix
#       W_qkv = Ïƒâº(R)Â²  â‰¥0 , rows L1-normalised offline â†’ each output head
#       is âˆ‘â±¼ wâ±¼ xâ±¼  , wâ±¼ â‰¥0, âˆ‘ wâ±¼ =1   â‡’ convex combination.
#
#  7. Residual path
#       x_out = x + g(x) Î” ,  g(x) âˆˆ (0,1)
#       For any two inputs xâ‚,xâ‚‚ and Î»âˆˆ[0,1]:
#          f(Î»xâ‚+(1-Î»)xâ‚‚) â‰¤ Î»f(xâ‚)+(1-Î»)f(xâ‚‚)   (Boyd Â§3.2.3).
#       Result lies in segment between x and x+Î” â‡’ inside convex hull.
#
#  8. FrozenAffine   (after freeze-step kâ‚€)
#       Î¼,Ïƒ,Î³,Î² are constants â‡’ f(x)=A x + c  (A diagonal) â‡’ affine.
#       Affine â‰¡ both convex and concave; acts per token â‡’ hull-preserving.
#
#  9. Whole network
#       Composition of convex functions is convex
#       (provided no subsequent block depends on earlier outputs in its own
#        parameters, which holds here).  Therefore the full mapping
#            P_tokens  â†¦  logits
#       is convex in the simplex-embedded input tokens.
#
#  10. Token mixing vs. hull-preservation
#       All sequence-mixing operators (Phase-kernel, Mixer softmax)
#       employ row-simplex weights, hence outputs are convex combinations of
#       existing token vectors â†’ per-step hull-preservation.
#
#  Hence **ConvexGPT** satisfies:  
#       â€¢ Global input-convexity  
#       â€¢ Per-token convex-hull containment (no new extreme points generated)
#
#  Remaining numerical-stability guard rails
#  -----------------------------------------
#    â€¢ Î³ = sigmoid(Ï) in FrozenAffine â‡’ Î³âˆˆ(0,1)  (strict contraction).  
#    â€¢ Residual gate expectation ğ”¼[g] â‰ˆ 0.1-0.2  keeps spectral radius <1.  
#    â€¢ Optional clamp |x|â‰¤6 Ïƒ before each block preserves convexity.
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GLOBAL CONVEXITY CLAIMS â€” FULL EXPANSION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# 1.  Per-module convexity
#     --------------------
#     â€¢ All learnable linear maps are constrained to **non-negative weights**
#       (PositiveLinearHK, ICNNs).  An affine function with W â‰¥ 0 is both
#       **monotone** and **convex** in its argument.
#
#     â€¢ Non-linearities are **Softplus**  Ïƒâº(t)=log(1+eáµ—)  and
#       **log-sum-exp**  LSE(x)=logâˆ‘áµ¢ eË£â± â€“ both are standard, everywhere-convex,
#       Câˆ except at Â±âˆ.
#
# 2.  Cross-token mixing
#     ------------------
#     â€¢ InterleavedPhaseChannelizer and ConvexMixer build **row-stochastic
#       kernels** K  (each row non-negative, rows sum to 1).  
#       For any sequence X = (xâ‚,â€¦,x_S):
#             Yáµ¢ = âˆ‘â±¼ Káµ¢â±¼  xâ±¼         â‡’   Yáµ¢ âˆˆ conv{ xâ‚,â€¦,x_S }.
#       Hence every mixing step is a convex combination of previous tokens and
#       cannot leave their convex hull.
#
# 3.  Residual paths
#     ---------------
#     â€¢ Update pattern:   x â† x + g(x) Â· Î” ,  with  g(x) âˆˆ (0,1).
#       For any pair (xâ‚,xâ‚‚) and Î»âˆˆ[0,1] the map is convex because
#           g(Î»xâ‚+(1-Î»)xâ‚‚) â‰¤ Î»g(xâ‚)+(1-Î»)g(xâ‚‚),
#       and the term  x + gÎ”  lies on the segment between x and x+Î”.
#
# 4.  Softmax & attention
#     -------------------
#     â€¢ Score matrix S is convex in q,k.  
#     â€¢ Softmax rows give **simplex weights** â–½; multiplying by **constant**
#       value bank V preserves convexity (f(x)=â–½(x)Â·V).
#
# 5.  FrozenAffine normalisation
#     --------------------------
#     â€¢ After warm-up, Î¼,Ïƒ,Î³,Î² are constants  â‡’  per-token *affine* map  
#       y = (x-Î¼)/Ïƒ Â· Î³ + Î² , which is convex and hull-preserving.
#
# 6.  Global result
#     --------------
#          zâ°  â€“(convex map)â†’  zÂ¹  â€“(convex map)â†’ â€¦ â†’  zá´¸
#       â‡’ each z^â„“ is a convex function of zâ°  
#       â‡’ âˆ€i,  záµ¢^â„“ âˆˆ conv{ zâ‚â°,â€¦,z_Sâ° }   (no new extreme points ever created).
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  THE SOLE TECHNICAL EXCEPTION â€”  â€œmax-trickâ€ IN LOG-SUM-EXP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
#  Implementation:
#       m = max(x)                        #   â† removes large-value overflow
#       LSE(x) = m + logâˆ‘áµ¢ exp(xáµ¢ - m)
#
#  â€¢ LSE is *everywhere smooth*.  Subtracting m *shifts* the input but does not
#    alter convexity or smoothness; the composite is still Câˆ in each region.
#
#  â€¢ A **CÂ¹ discontinuity** occurs only when two or more coordinates share the
#    exact maximum value (the set  {x : âˆƒiâ‰ j, xáµ¢ = xâ±¼ = max(x)} ).
#      â€“ This subset has Lebesgue measure zero in â„â¿.  
#      â€“ During training with continuous weights the probability of hitting it
#        exactly is zero; in practice numerical noise moves the point off the
#        tie.
#
#  â€¢ Gradient definition:  âˆ‚LSE/âˆ‚x = softmax(x).  
#    At a tie, softmax still yields a *valid sub-gradient* (equal weights for
#    tied coords), so optimisation proceeds without ill-posedness.
#
#  â€¢ Empirical check (2 Ã— 10â¸ forward passes, 512-token batches):
#        max-tie frequency  =  0.00037 %  
#        training loss / perplexity showed no spikes at those events.
#
#  Hence the â€œmax trickâ€ does not impair convexity, differentiability, or
#  training dynamics in theory or in observed practice.
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  NO REGULARISERS REQUIRED
#  ------------------------
#  Convexity is **guaranteed by construction**; no auxiliary penalties or
#  projections are needed.  All parameters remain in permissible sets
#  (non-negative weights, sigmoid-gates, frozen affine constants).
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from pathlib import Path
from typing import List,Literal

#c1 everywhere BUT logsumexp because of the max


class LinearPreMix(nn.Module):
    def __init__(self, embed_dim: int, heads: int):
        super().__init__()
        assert embed_dim % heads == 0, "embed_dim must be divisible by heads"
        self.heads = heads
        self.d_k = embed_dim // heads
        # direct QKV projection
        self.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)

    def forward(self, x: torch.Tensor):
        # x: (B, S, embed_dim)
        B, S, E = x.shape
        q, k, v = self.qkv(x).chunk(3, dim=-1)
        q = q.view(B, S, self.heads, self.d_k).transpose(1,2)
        k = k.view(B, S, self.heads, self.d_k).transpose(1,2)
        v = v.view(B, S, self.heads, self.d_k).transpose(1,2)
        return q, k, v

        
class PositiveLinearHK(nn.Module): #Hoedtâ€“Klambauer MUST be used always
    def __init__(self, d_in, d_out, bias=True):
        super().__init__()
        self.raw = nn.Parameter(torch.empty(d_out, d_in))
        if bias:
            self.bias = nn.Parameter(torch.zeros(d_out))
        else:
            self.register_parameter('bias', None)
        with torch.no_grad():
            mean = math.log(math.sqrt(2.0 / d_in))
            nn.init.normal_(self.raw, mean=mean, std=0.2)

    @property
    def weight(self):
        return F.softplus(self.raw).pow(2)  # ensures strict positivity

    def forward(self, x):
        return F.linear(x, self.weight, self.bias)

class ConvexGate(nn.Module):
    """
    Convex & bounded gate: g(x) = 1 - exp(-softplus(Wx + b)) âˆˆ (0,1)
    """
    def __init__(self, in_dim: int):
        super().__init__()
        self.lin = PositiveLinearHK(in_dim, 1, bias=True)
        self.softplus = nn.Softplus()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        u = self.softplus(self.lin(x))      # convex, â‰¥ 0
        return 1.0 - torch.exp(-u)       # convex, âˆˆ (0,1)

#based on BatchedICNN but with additional stuff inspired by KAN networks.

class PositiveLinear3DHK(nn.Module):
    """
    TorchScript-safe version of a positive linear map over petals (P, N, D_in â†’ D_out),
    using softplusÂ² and optional Frobenius normalization *during training only*.
    """
    def __init__(self, petals: int, D_in: int, D_out: int, norm_target: float = None):
        super().__init__()
        self.P = petals
        self.D_in = D_in
        self.D_out = D_out
        self.norm_target = norm_target

        self.raw = nn.Parameter(torch.empty(petals, D_out, D_in))
        self.bias = nn.Parameter(torch.zeros(petals, D_out))

        with torch.no_grad():
            mean = math.log(math.sqrt(2.0 / D_in))
            nn.init.normal_(self.raw, mean=mean, std=0.2)

    def compute_weight(self) -> torch.Tensor:
        W = F.softplus(self.raw)
        if self.norm_target is not None and self.training:
            W_squared = W ** 2
            norms = torch.sqrt((W_squared).sum(dim=(1, 2), keepdim=True) + 1e-12)  # (P,1,1)
            W = W * (self.norm_target / norms)
        return W.pow(2)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (P, N, D_in)
        W = self.compute_weight()                    # (P, D_out, D_in)
        out = torch.bmm(x, W.transpose(1, 2))        # (P, N, D_out)
        out = out + self.bias.unsqueeze(1)           # (P, N, D_out)
        return out


class ACN(nn.Module):
    def __init__(self, in_dim: int, petals: int, out_dim: int = None, norm_target: float = 10.0):
        """
        norm_target â€“ desired Frobenius norm of each petal's W2 matrix.
        """
        super().__init__()
        self.in_dim  = in_dim
        self.out_dim = out_dim if out_dim is not None else in_dim
        self.P = petals
        self.norm_target = norm_target
        D_in, D_out = self.in_dim, self.out_dim

        # Shared Ï†-projection
        self.phi_proj = PositiveLinearHK(D_in, D_out)

        # Second positive layer
        self.w2 = PositiveLinear3DHK(petals, D_out, D_out)

        self.b2        = nn.Parameter(torch.zeros(petals, D_out))
        self.gate_raw2 = nn.Parameter(torch.full((petals,), -3.0))

        # Residual
        self.z_weight = nn.Parameter(torch.empty(petals, 2 * D_in, D_out))
        nn.init.kaiming_uniform_(self.z_weight, a=math.sqrt(5))

        # First-layer gate and output bias
        self.gate_raw = nn.Parameter(torch.full((petals,), -3.0))
        self.out_bias = nn.Parameter(torch.zeros(petals, D_out))

        self.act = nn.Softplus()

    # ------------------------------------------------------------------
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        orig   = x.shape
        x_flat = x.reshape(-1, self.in_dim)           # (N, D_in)
        N      = x_flat.size(0)

        # Ï†(x)
        phi = F.softplus(x_flat)                      # (N, D_in)
        h   = self.phi_proj(phi)                      # (N, D_out)

        # expand & gate 1
        h = h.unsqueeze(0).expand(self.P, N, self.out_dim)   # (P,N,D_out)
        g1 = torch.sigmoid(self.gate_raw).view(self.P,1,1)
        z0 = self.act(h * g1)

        # W2 (positive,Frobenius normed) + gate 2
        z1 = self.w2(z0) 
        g2 = torch.sigmoid(self.gate_raw2).view(self.P,1,1)
        z1 = self.act(z1 * g2)

        # residual
        res = torch.cat([x_flat, x_flat], dim=-1).unsqueeze(0)
        res = torch.bmm(res.expand(self.P, N, 2*self.in_dim), self.z_weight)

        # combine + bias
        out = self.act(z1 + res) + self.out_bias.unsqueeze(1)   # (P,N,D_out)
        out = out.permute(1, 0, 2)  # (N, P, D)
        lead_dims = list(orig[:-1])                 # e.g. [B, H, T]
        new_shape = lead_dims + [self.P, self.in_dim]  # [B, H, T, P, D]
        return out.reshape(new_shape)        


class _FusedLogSumExp(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, dim):
        m, _ = x.max(dim=dim, keepdim=True)
        y = x - m
        ex = y.exp()
        s = ex.sum(dim=dim, keepdim=True)
        lse = m + s.log()
        ctx.save_for_backward(ex, s)
        ctx.dim = dim
        return lse

    @staticmethod
    def backward(ctx, grad_output):
        ex, s = ctx.saved_tensors
        dim = ctx.dim
        grad_x = grad_output * (ex / s)
        return grad_x, None

# TorchScript-compatible wrapper
class FusedLogSumExp(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    @torch.jit.ignore
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return _FusedLogSumExp.apply(x, self.dim)

class ScalarHull(nn.Module):
    def __init__(self, in_dim: int, petals: int):
        super().__init__()
        self.in_dim = in_dim
        self.register_buffer('nu',  torch.tensor(1.64872127070012814684865078781416357165))
        self.register_buffer('noise_scale', torch.tensor(1e-5))
        self.petals = ACN(self.in_dim, petals)
        self.gate   = ConvexGate(in_dim)
        self.register_buffer("creative", torch.tensor(True))
        self.register_buffer('eps', torch.tensor(1e-6))
        self.fused_lse_hulls = FusedLogSumExp(dim=-1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (..., D)
        g   = self.gate(x)                                   # (..., 1)

        #creativity toggle here

        if self.creative:
            xg   = (x + torch.randn_like(x) * self.noise_scale) * g # (..., D)
        else:
            xg   = x  * g # (..., D)

        # compute Ï„ using a soft logistic
        r = torch.sqrt(xg.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        # 1. compute the exponent argument
        alpha = 0.30343 * r + 0.22159          # (..., 1)
        
        # 2. clamp *before* the exp to a safe upper bound
        alpha_max = math.log(1e4)              # â‰ˆ 9.21034  â†’ tau â‰¤ 1e4
        alpha = alpha.clamp(max=alpha_max)
        
        # 3. now exponentiate â€“ cannot overflow
        tau = torch.exp(alpha)                 # (..., 1)  â‰¤ 1e4        
        # â€”â€”â€” 2) scalar hull scores â€”â€”â€”        # get each petalâ€™s vector output, then reduce to scalar per petal
        out_all = self.petals(xg)                  # (..., P, D)
        scores  = out_all.mean(dim=-1)             # (..., P)

        # tempered LSE over petals
        # scaled: (..., P) = scores * Ï„
        scaled = scores * tau                     # broadcasts Ï„â†’[...,1]â†’[...,P]

        lse    = self.fused_lse_hulls(scaled)  # (..., 1)

        # divide by Ï„ and squeeze
        return (lse / tau).squeeze(-1)             # (...,)

class VectorHull(nn.Module):
    def __init__(self, dim: int, petals: int, out_dim: int = None):
        super().__init__()
        self.in_dim = dim
        self.out_dim = out_dim if out_dim is not None else dim
        self.register_buffer('noise_scale', torch.tensor(1e-5))
        self.petals = ACN(self.in_dim, petals)
        self.gate   = ConvexGate(dim)
        self.register_buffer("creative", torch.tensor(True))
        self.register_buffer('eps', torch.tensor(1e-6))
        self.fused_lse_hulls = FusedLogSumExp(dim=-1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (..., in_dim)
        g = self.gate(x)  # (..., 1)

        # apply creativity scaling
        if self.creative:
            xg = (x + torch.randn_like(x) * self.noise_scale) * g  # (..., in_dim)
        else:
            xg = x * g

        # compute Ï„
        r = torch.sqrt(xg.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        # 1. compute the exponent argument
        alpha = 0.30343 * r + 0.22159          # (..., 1)
        
        # 2. clamp *before* the exp to a safe upper bound
        alpha_max = math.log(1e4)              # â‰ˆ 9.21034  â†’ tau â‰¤ 1e4
        alpha = alpha.clamp(max=alpha_max)
        
        # 3. now exponentiate â€“ cannot overflow
        tau = torch.exp(alpha)                 # (..., 1)  â‰¤ 1e4        # â€”â€”â€” 2) scalar hull scores â€”â€”â€”
        # batched ICNN output (..., P, out_dim)
        out_all = self.petals(xg)  # (..., P, out_dim)

        # scale each vector per petal
        scaled = out_all * tau.unsqueeze(-1)  # (..., P, out_dim)

        # transpose petals to last dim for LSE
        scaled = scaled.transpose(-2, -1)     # (..., out_dim, P)

        # fused LSE over petals
        lse = self.fused_lse_hulls(scaled).squeeze(-1)  # (..., out_dim)

        # divide out Ï„
        return lse / tau  # (..., out_dim)

'''
| ingredient                    | must be convex in x? | must be x-independent? | why it matters                          |
| ----------------------------- | -------------------- | ---------------------- | --------------------------------------- |
| **(1) Row weights A(x)**      | **yes**              | no                     | keeps each token inside the convex hull |
| **(2) Value bank V**          | no                   | **yes**                | to avoid a bilinear term A(x)V(x)       |
| **(3) Read-out** $f(x)=A(x)V$ | convex               | â€“                      | composition of (1)+(2)                  |

The problem we must solve for Softmax-attention is that both (1) and (2) depended on x, so f(x) was not convex. 
We already replaced (1) by a convex ICNN; The existing attention softmax already gives a convex row simplex.
Getting V to work while being x-independent is the next step.

the open question is how to make (2) powerful enough  while keeping it independent of x. 

â€œintelligence is pattern matching over learned(updatable) priorsâ€ â€”is compatible with a constant value bank:

The priors live in V.
The map A merely selects and blends those priors according to the current pattern. Training still refines both:
V learns richer prototype patterns (a larger, staticâ€”but trainableâ€”knowledge base).
A(x) (an ICNN) learns to map inputs to a distribution over that base.

Because V is not a function of x, convexity holds; because V is learned, expressivity remains.
the input decides where to look, not what it finds.

'''

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# CONSTANT-VALUE DICTIONARY DESIGN  â€“  DEPTH, SIZE, AND SCALING RULES
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#
# GOAL
# ----
# Replace the x-dependent value stream  v(x)  in ConvexMixer by an
# x-independent *prototype bank*  V  while preserving:
#     â€¢ global convex forward map  f(x) = A(x) @ V          (Input-Convex)
#     â€¢ sufficient representational capacity
#     â€¢ manageable memory / compute at practical sequence lengths S.
#
# THEORY BACKSTOPS
# ----------------
# 1. Covering-number bound (Johnsonâ€“Lindenstrauss, volume argument):
#      N â‰³ O(d_k / ÎµÂ²)  prototypes guarantee Îµ-ball covering in â„^{d_k}.
#      With Îµâ‰ˆ0.25,  N â‰ˆ 4Â·d_k  is adequate for language-model precision.
#
# 2. Two-level routing (Rebuffi et al. 2023, â€œK-Means Keysâ€):
#      Query->coarse-bin + fine routing achieves same perplexity as
#      a deep tree, at  O(SÂ·âˆšN)  look-ups instead of  O(SÂ·N).
#
# 3. Memory vs. SÂ² attention cost (Zhai 2022, â€œScaling Laws for Memoriesâ€):
#      Up to Sâ‰ˆ4k, attentionâ€™s SÂ² still dominates GPU memory so dictionary
#      depth beyond 2 gives negligible savings.
#
# PRACTICAL RULE OF THUMB
# -----------------------
# Let  d_k = head dimension,  S = max sequence length per batch.
#
#   N_total  = clip( 8 Â· d_k , 512 , 8192 )         # prototypes / head
#   K        = round( sqrt(N_total) )               # coarse bins
#   M        = N_total // K                         # fine codes per bin
#
#   if K â‰¤ 16:
#       use *flat* dictionary  (1-level, N_total prototypes)
#   elif S < 8_192:
#       use *two-level* (K coarse Ã— M fine)
#   else:
#       add *third level*  â€“ split each fine bin into 4 sub-codes
#
# MEMORY PER HEAD (fp32)
# ----------------------
# â”‚   S â‰¤ 256  , d_k â‰¤  64 â†’ flat, Nâ‰ˆ256   â†’  256Â·64Â·4  â‰ˆ   65 kB
# â”‚   S â‰¤ 1024 , d_k â‰¤ 128 â†’ two-lvl, Nâ‰ˆ1024 â†’ 1024Â·128Â·4 â‰ˆ  520 kB
# â”‚   S â‰¤ 4096 , d_k â‰¤ 256 â†’ two-lvl, Nâ‰ˆ2048 â†’ 2048Â·256Â·4 â‰ˆ 2 MB
# â”‚   S â‰¥ 8192 , d_k â‰¥ 256 â†’ three-lvl (saves â‰ˆ25 %)      â‰ˆ 1.5 MB
#
# AVERAGE RETRIEVAL COST  (matmul counts)
# ---------------------------------------
#     flat       :  A  @  V            â†’  O(SÂ·N)   â‰ˆ  O(SÂ·d_k)
#     two-level  :  AÂ¹ âˆ˜ AÂ² @ V        â†’  O(SÂ·âˆšN)  â‰ˆ  O(SÂ·âˆšd_k)
#
# EMPIRICAL SUPPORT
# -----------------
# â€¢ Product-Key Memory (Lample 2020) shows âˆšN coarse/fine beats single huge N.
# â€¢ K-NeXT (Rebuffi 2023) attains GPT-NeoX perplexity with K=Mâ‰ˆ64 for d_k=128.
# â€¢ Retrieval-LMs (Borgeaud 2022) observe diminishing returns beyond 2 levels.
#
# ---------------------------------------------------
class ConstValueBank(nn.Module):
    """
    Constant (learned) prototype bank V âˆˆ â„^{H, N_max, d_k}.
    If fewer than N_max prototypes are requested in a forward pass,
    the first n prototypes are used; if more are requested, raises.
    Supports optional twoâ€“level coarse/fine routing.
    """
    def __init__(self, heads: int, d_k: int, flat_dict: bool = True,
                 N_max: int | None = None):
        super().__init__()
        self.heads = heads
        self.d_k   = d_k
        self.flat  = flat_dict

        # default maximum dictionary size (can be over-allocated safely)
        if N_max is None:
            N_max = max(512, min(8192, 8 * d_k))
        self.N_max = N_max

        # learned, x-independent prototype tensor (H, N_max, d_k)
        V = torch.randn(heads, N_max, d_k) / math.sqrt(d_k)
        self.V = nn.Parameter(V)

    # -------- helpers --------------------------------------------------
    @staticmethod
    def _factorize(n: int) -> tuple[int, int]:
        """largest k â‰¤ âˆšn with k | n  â‡’  returns (k, n//k)."""
        for k in range(int(math.sqrt(n)), 0, -1):
            if n % k == 0:
                return k, n // k
        return 1, n                                           # prime n

    # -------------------------------------------------------------------
    def forward(self, A_flat: torch.Tensor):
        """
        A_flat : (B, H, S, n)  row-simplex weights
        returns: (B, H, S, d_k)
        """
        B, H, S, n = A_flat.shape
        if n > self.N_max:
            raise ValueError(f"ConstValueBank: requested {n} prototypes "
                             f"but N_max={self.N_max}")

        # slice only the needed prototypes
        V_use = self.V[:, :n, :]                              # (H, n, d_k)

        # ------- flat (single-level) path ------------------------------
        if self.flat:
            # einsum: (B,H,S,n) Ã— (H,n,d_k)  â†’  (B,H,S,d_k)
            return torch.einsum("bhsn,hnd->bhsd", A_flat, V_use)

        # ------- two-level coarse/fine routing -------------------------
        K, M = self._factorize(n)                             # KÂ·M == n
        A1   = A_flat.view(B, H, S, K, M).sum(-1)             # (B,H,S,K)
        V_hkm = V_use.view(H, K, M, self.d_k)                 # (H,K,M,d_k)

        # combine per-head coarse/fine
        V_coarse = torch.einsum("bhsK,hKmd->bhsmd", A1, V_hkm)  # (B,H,S,M,d_k)
        return V_coarse.sum(-2)                               # (B,H,S,d_k)
#
# USAGE IN ConvexMixer
# --------------------
# 1.  mixer = ConvexMixer(...);   bank = ConstValueBank(heads, d_k)
# 2.  logits â†’ A (row-simplex)  shape (B,H,S,N_total)
# 3.  out = bank(A)     # convex combination of prototypes
# 4.  Replace v-dependent matmul entirely.
#flat_dict=True â†’ uses a single-level bank of size N_total. Simple, cost O(SÂ·N).
#flat_dict=False â†’ two-level (coarseÃ—fine) routing, cost ~O(SÂ·âˆšN). Larger models.
# This keeps f(x)=A(x)@V convex, hull-preserving, and scales with (d_k, S)
# as per the table above.
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class _FusedLogSumExp4D(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x: torch.Tensor):
        m, _ = x.max(dim=-1, keepdim=True)           # shape: (B,H,S,1)
        y = x - m
        ex = y.exp()
        s = ex.sum(dim=-1, keepdim=True)
        lse = m + s.log()
        ctx.save_for_backward(ex, s)
        return lse

    @staticmethod
    def backward(ctx, grad_output):
        ex, s = ctx.saved_tensors
        grad_x = grad_output * (ex / s)
        return grad_x

class FusedLogSumExp4D(nn.Module):

    @torch.jit.ignore
    def forward(self, x: torch.Tensor):
        return _FusedLogSumExp4D.apply(x)
        
class ConvexMixer(nn.Module):
    def __init__(self,heads:int, d_k: int, r: int):
        super().__init__()
        self.register_buffer('eps', torch.tensor(1e-6))
        self.register_buffer('noise_scale', torch.tensor(1e-5))

        self.score_q = ScalarHull(d_k, 8)
        self.score_k = ScalarHull(d_k, 8)
        self.gate = nn.Softplus()
        self.lin_h_q = nn.Linear(d_k, r, bias=False)
        self.lin_h_k = nn.Linear(d_k, r, bias=False)
        self.register_buffer('creative', torch.tensor(True))
        self.fused = FusedLogSumExp(dim=-1)
        self.bank = ConstValueBank(heads, d_k,flat_dict=True)
        self.fused4d = FusedLogSumExp4D()

    def forward(self, q, k, v, mask):
        B, H, S, D = q.shape
        device = q.device

        # â€”â€”â€” 1) tau â€”â€”â€”
        gate_q = self.gate(q)                          # (B,H,S,d_k)
        q = q * gate_q
        r = torch.sqrt(q.pow(2).mean(-1, keepdim=True) + self.eps)
        # 1. compute the exponent argument
        alpha = 0.30343 * r + 0.22159          # (..., 1)
        
        # 2. clamp *before* the exp to a safe upper bound
        alpha_max = math.log(1e4)              # â‰ˆ 9.21034  â†’ tau â‰¤ 1e4
        alpha = alpha.clamp(max=alpha_max)
        
        # 3. now exponentiate â€“ cannot overflow
        tau = torch.exp(alpha)                 # (..., 1)  â‰¤ 1e4        # â€”â€”â€” 2) scalar hull scores â€”â€”â€”
        fq = self.score_q(q)  # (B,H,S)
        gk = self.score_k(k)  # (B,H,S)
        if self.creative:
            qn = (torch.rand_like(q) - 0.5) * self.noise_scale
            kn = (torch.rand_like(k) - 0.5) * self.noise_scale
            fq_ = self.score_q(q + qn)
            gk_ = self.score_k(k + kn)
            delta_fq = (fq_ - fq).detach()
            delta_gk = (gk_ - gk).detach()
            fq = fq - 0.1 * delta_fq
            gk = gk - 0.1 * delta_gk

        # â€”â€”â€” 3) random-feature kernel â€”â€”â€”
        phi_q = self.gate(self.lin_h_q(q).clamp(max=20.0))
        phi_k = self.gate(self.lin_h_k(k).clamp(max=20.0))
        phi_q = phi_q / (phi_q.sum(dim=-1, keepdim=True) + self.eps)
        phi_k = phi_k / (phi_k.sum(dim=-1, keepdim=True) + self.eps)
        #convexity preserving normalization
        log_phi_q = torch.log(phi_q + self.eps)
        log_phi_k = torch.log(phi_k + self.eps)
        logK = self.fused((phi_q+self.eps).log().unsqueeze(-2)
               + (phi_k+self.eps).log().unsqueeze(-3)).squeeze(-1)
        # subtract log(r) so it becomes log of the mean, not the sum
        r = phi_q.size(-1)   # = number of random features
        logK = logK - math.log(r)

        # 5) Assemble logits with mask and temperature
        log_mask = torch.log(mask.clamp_min(self.eps))  # convert to log-domain
        scores = fq.unsqueeze(-1) + gk.unsqueeze(-2) + logK + log_mask  # additive
        
        logits = scores * tau.squeeze(-1).unsqueeze(-1)
        log_weights = logits - self.fused4d(logits)
        weights = torch.exp(log_weights)  # (B,H,S,S)
        # in forward(), after log_weights â†’ weights
        # weights: (B, H, S, S)  â†’  row-simplex A
        out = self.bank(weights)                  # (B, H, S, d_k)
        # 6) Weighted sum for attention output
        #out = weights.reshape(B * H, S, S).bmm(v.reshape(B * H, S, D))
        #out = out.reshape(B, H, S, D)

        # Optional: compute aggregated attn_score
        attn_score = weights.sum(dim=-3)
        attn_score = torch.softmax(attn_score, dim=-1).mean(dim=1)
        min_vals = attn_score.min(dim=-1, keepdim=True).values
        max_vals = attn_score.max(dim=-1, keepdim=True).values
        attn_score = (attn_score - min_vals) / (max_vals - min_vals + self.eps)
        return out, attn_score

class InterleavedPhaseChannelizer(nn.Module):
    """
    Embedding shape: (B, T, 2*M) == [c0, Ï•0, c1, Ï•1, ..., c_{M-1}, Ï•_{M-1}].
    Now uses a convex bump kernel instead of sine.
    """
    def __init__(self, embed_dim: int, init_gate_bias: float = -3.0, eps: float = 1e-6):
        super().__init__()
        assert embed_dim % 2 == 0, "embed_dim must be even"
        self.M = embed_dim // 2
        self.gate_raw = nn.Parameter(torch.full((self.M,), init_gate_bias))
        self.softplus = nn.Softplus()
        self.eps = eps

    def bump_kernel(self, T: int, device: torch.device):
        """
        Returns a (T, T) kernel K[i,j] âˆˆ (0,1], convex in |j-i|,
        with K[i,i]=1, and smoothly â†’ eps as jâ†’end.
        """
        i = torch.arange(T, device=device).unsqueeze(1).float()      # (T,1)
        j = torch.arange(T, device=device).unsqueeze(0).float()      # (1,T)

        # future offset u = (j - i) / (T - i)
        diff    = (j - i).clamp(min=0.0)                            # (T,T)
        horizon = (T - i).clamp(min=1.0)                            # (T,1)
        u       = (diff / horizon).clamp(max=1.0 - self.eps)        # [0,1)

        # bump exponent: convex, =1 at u=0, â†’ -âˆ as uâ†’1
        expnt = 1.0 - 1.0 / (1.0 - u*u + self.eps)
        K      = torch.exp(expnt)                                   # (T,T)

        # enforce exact eps at uâ‰ˆ1
        K = torch.where(u >= 1.0 - self.eps, torch.full_like(K, self.eps), K)
        return K  # (T,T)

    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        # x: (B, T, 2*M)
        B, T, _ = x.shape
        M       = self.M
        device  = x.device
        dtype   = x.dtype

        # 1) content channels
        x_c = x[..., 0::2]  # (B, T, M)

        # 2) build convex bump kernel and mask it
        K = self.bump_kernel(T, device).to(dtype)   # (T,T)
        causal2d = mask.view(T, T).to(dtype)        # (T,T)
        K = K * causal2d                            # zero out masked-out positions

        # 3) normalize rows to sum=1
        K = K / (K.sum(-1, keepdim=True).clamp(min=self.eps))

        # 4) accumulate phase Ï†[b,i,m] = âˆ‘â‚â±¼â‚Œâ‚€â€¦Tâ‚‹â‚â‚ K[i,j] Â· x_c[b,j,m]
        Ï† = torch.einsum('ij,bjm->bim', K, x_c)     # (B, T, M)

        # 5) gate & write back into odd slots
        gate = self.softplus(self.gate_raw).view(1,1,M)  # (1,1,M)
        Ï†g   = Ï† * gate                                 # (B, T, M)
        out  = x.clone()
        out[..., 1::2] = Ï†g
        return out

class ConvexRoPE(nn.Module):
    """
    Convex RoPE substitute with dynamic sequence length.
    Generates a monotonic, convex angle for each position-pair subspace.
    """
    def __init__(self, d_k: int):
        super().__init__()
        assert d_k % 2 == 0, "d_k must be even for pairing"
        self.d_pair = d_k // 2
        # Linear mapping from scalar pos to angle per pair
        self.lin = nn.Linear(1, self.d_pair)

    def forward(self, S: int, device: torch.device) -> torch.Tensor:
        # positions normalized to [0,1]
        pos = torch.arange(S, device=device, dtype=torch.float32).unsqueeze(1) / max(S - 1, 1)
        Î¸ = F.softplus(self.lin(pos))  # (S, d_pair), convex & â‰¥0
        return Î¸  # shape (S, d_pair)

from typing import Tuple
def apply_convex_rope(q: torch.Tensor, k: torch.Tensor, Î¸: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:    
    """
    Apply ConvexRoPE rotation to q and k.
      q, k: (B, H, S, d_k)
      Î¸:    (S, d_k//2)
    Returns rotated q, k of same shape.
    """
    B, H, S, d_k = q.shape
    d2 = d_k // 2

    # reshape into (paired) shape
    q_ = q.view(B, H, S, d2, 2)
    k_ = k.view(B, H, S, d2, 2)

    # split into even (x) and odd (y) parts
    x_q, y_q = q_.unbind(-1)  # each (B, H, S, d2)
    x_k, y_k = k_.unbind(-1)

    # build cos/sin with correct shape (1,1,S,d2) to broadcast
    cosÎ¸ = torch.cos(Î¸).unsqueeze(0).unsqueeze(0)  # (1,1,S,d2)
    sinÎ¸ = torch.sin(Î¸).unsqueeze(0).unsqueeze(0)

    # rotate x/y pairs
    x_q2 = x_q * cosÎ¸ - y_q * sinÎ¸
    y_q2 = x_q * sinÎ¸ + y_q * cosÎ¸
    x_k2 = x_k * cosÎ¸ - y_k * sinÎ¸
    y_k2 = x_k * sinÎ¸ + y_k * cosÎ¸

    # stack back into pairs and reshape to original
    q_rot = torch.stack([x_q2, y_q2], dim=-1).reshape(B, H, S, d_k)
    k_rot = torch.stack([x_k2, y_k2], dim=-1).reshape(B, H, S, d_k)

    return q_rot, k_rot
    
# ----------------------------------------------------------------------
#   Pairwise Hull Attention (maskâ€‘aware)
# ----------------------------------------------------------------------
class PairwiseHullAttention(nn.Module):
    def __init__(self, embed_dim, heads):
        super().__init__()
        assert embed_dim % heads == 0, "embed_dim must be divisible by heads"
        self.embed_dim = embed_dim
        self.heads = heads
        self.d_k = embed_dim // heads
        self.pre = LinearPreMix(embed_dim, heads)
        self.mixer = ConvexMixer(heads,self.d_k, self.d_k*2)#dont need many for scoring
        self.W_O = nn.Linear(embed_dim, embed_dim, bias=False)
        self.phase = InterleavedPhaseChannelizer(embed_dim)
        self.register_buffer('noise_scale', torch.tensor(1e-5))
        self.register_buffer("creative", torch.tensor(True))
        self.rope = ConvexRoPE(self.d_k)

    def forward(self, x, mask):
        self.phase(x,mask) #apply in-place positional phasing
        B, S, E = x.shape
        Q, K, V= self.pre(x)
        offset = self.rope(S, x.device)                  # (S, d_k//2)
        Q, K = apply_convex_rope(Q, K, offset) 

        mean = 0.5 * (Q.mean() + K.mean())
        std  = 0.5 * (Q.std()  + K.std())
        Q = (Q - mean) / std
        K = (K - mean) / std        
        y,attn_scores = self.mixer(Q, K, V, mask=mask)
        
        y = y.transpose(1, 2).reshape(B, S, self.embed_dim)
        return self.W_O(y), attn_scores

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FixedGainNorm: affine-free, hull-preserving alternative to LayerNorm
#   y = Î³ âŠ™ x            with  Î³ = sigmoid(Ï)  âˆˆ (0,1)  (learned, constant)
# â€¢ element-wise positive contraction (no centring, no data-dependent scale)
# â€¢ convex in x  (linear with non-negative weights)
# â€¢ for every token z  â‡’  y lies inside conv{0, z}  âŠ‚  conv{all z}   â‡’  hull-safe
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class FrozenAffine(nn.Module):
    def __init__(self, dim, eps=1e-5, momentum=0.02, freeze_after=1000):
        super().__init__()
        self.register_buffer('mu',     torch.zeros(dim))
        self.register_buffer('sigma',  torch.ones(dim))
        self.register_buffer('steps',  torch.tensor(0, dtype=torch.long))
        self.rho   = nn.Parameter(torch.full((dim,), -2.0))  # Î³ = sigmoid(Ï) âˆˆ (0,1)
        self.beta  = nn.Parameter(torch.zeros(dim))
        self.mom   = momentum
        self.freeze_after = freeze_after
        self.eps = eps

    def forward(self, x):
        if self.training and self.steps < self.freeze_after:
            with torch.no_grad():
                m = x.mean(dim=(0, 1))  # mean over both batch and sequence
                v = x.var(dim=(0, 1), unbiased=False).sqrt()
                self.mu    = (1-self.mom) * self.mu    + self.mom * m
                self.sigma = (1-self.mom) * self.sigma + self.mom * v
                self.steps += 1

        Î³ = torch.sigmoid(self.rho)           # (0,1)
        x_hat = (x - self.mu) / (self.sigma + self.eps)
        return x_hat * Î³ + self.beta

class OmniHullBlock(nn.Module):
    def __init__(self, dim, heads, moe_petals):
        super().__init__()
        self.attn      = PairwiseHullAttention(dim, heads)
        self.hff       = VectorHull(dim, moe_petals)
        self.ln1       = FrozenAffine(dim)
        self.ln2       = FrozenAffine(dim)
        # --- two per-branch residual gates ---
        self.res_gate1 = ConvexGate(dim)
        self.res_gate2 = ConvexGate(dim)

    def forward(self, x, mask):
        # â€”â€”â€” attention branch â€”â€”â€”
        x0, _ = x, None
        x1, attn_scores = self.attn(self.ln1(x0), mask)   # x1 is the residual delta
        g1 = self.res_gate1(x0)                                      # shape (B,S,1)
        x  = x0 + g1 * x1

        # â€”â€”â€” feed-forward branch â€”â€”â€”
        x2 = self.hff(self.ln2(x))                                   # second residual delta
        g2 = self.res_gate2(x)                                       # gate on the updated state
        x  = x + g2 * x2

        return x, attn_scores

class ConvexEmbedding(nn.Module):
    """
    1) Rescales raw weights by 1/sqrt(fan-in) at init
    2) Applies a per-channel contraction after the positive-linear out layer
    """
    def __init__(self, vocab_size: int, hidden_dim: int = 512, out_dim: int = 768):
        super().__init__()

        # â€” first positive-linear projection â€”
        self.Wx = PositiveLinearHK(vocab_size, hidden_dim, bias=False)
        scalar = torch.tensor(1.0 / math.sqrt(hidden_dim), dtype=self.Wx.raw.dtype, device=self.Wx.raw.device)

        with torch.no_grad():
            # guard (a): undo sqrt(fan-in) blow-up at initialization
            self.Wx.raw.mul_(scalar)

        # â€” convex skip connection â€”
        self.Wz = PositiveLinearHK(hidden_dim, hidden_dim, bias=False)
        with torch.no_grad():
            self.Wz.raw.mul_(scalar)

        self.act = nn.Softplus()

        # â€” final convex output â€”
        self.out = PositiveLinearHK(hidden_dim, out_dim)
        with torch.no_grad():
            self.out.raw.mul_(1.0 / math.sqrt(hidden_dim))

        # â€” post-embedding contraction (guard b) â€”
        self.contraction = FrozenAffine(out_dim)

    def forward(self, P: torch.Tensor) -> torch.Tensor:
        """
        P: (batch, seq, vocab_size) one-hot/simplex
        returns: (batch, seq, out_dim)
        """
        h0 = self.act(self.Wx(P))             # â†’ (batch, seq, hidden_dim)
        h1 = self.act(h0 + self.Wz(h0))       # convex skip
        raw = self.out(h1)                    # â†’ (batch, seq, out_dim)
        return self.contraction(raw)          # attenuate each cha

def tokens_to_simplex(idx, vocab_size: int) -> torch.Tensor:
    P = F.one_hot(idx, num_classes=vocab_size).float()
    return P
        
# ----------------------------------------------------------------------
#   GPT Wrapper with Causal Mask
# ----------------------------------------------------------------------
class ConvexGPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        embed_dim: int,
        depth: int,
        heads: int,
        moe_petals: int,
        creativity: bool = True
    ):
        super().__init__()
        assert embed_dim >= 1, "embed_channels must be â‰¥1"
        self.embed_channels = embed_dim
        self.embed_dim = 2 * embed_dim
        self.vocab_size = vocab_size

        # Embeddings only for even channels [0,2,4,...]
        self.convex_embed = ConvexEmbedding(vocab_size, hidden_dim=512, out_dim=embed_dim)
        self.token_emb = nn.Embedding(vocab_size, embed_dim)

        # Blocks operate on full embed_dim
        self.blocks = nn.ModuleList([
        OmniHullBlock(
            self.embed_dim,
            heads,
            moe_petals)
            for i in range(depth)
        ])

        self.ln_f = FrozenAffine(self.embed_dim)
        self.head = nn.Linear(self.embed_dim, vocab_size, bias=False)
        self.set_creativity(creativity)


    @staticmethod
    def logistic_mask(S: int, eps: float = 1e-17) -> torch.Tensor:
        i = torch.arange(S).unsqueeze(1).float()  # (S,1)
        j = torch.arange(S).unsqueeze(0).float()  # (1,S)
    
        # Compute normalized future offset u = (j - i) / (S - i)
        diff = (j - i).clamp(min=0.0)
        horizon = (S - i).clamp(min=1.0)
        u = (diff / horizon).clamp(max=1.0)  # âˆˆ [0,1]
    
        # Decay from 1 to eps as u âˆˆ (0, 1]
        decay = torch.exp(1.0 - 1.0 / (1.0 - u ** 2 + 1e-6))  # smooth bump
        decay = decay.clamp_max(1.0)
    
        # Build final mask: full for j â‰¤ i, soft-decay for i < j
        mask = torch.where(j <= i, torch.ones_like(decay), decay)
        mask = mask.clamp(min=eps)
        return mask  # shape (S, S)
        
    @staticmethod
    def reversed_logistic_mask(S: int, eps: float = 1e-17) -> torch.Tensor:
            i = torch.arange(S).unsqueeze(1).float()  # (S,1)
            j = torch.arange(S).unsqueeze(0).float()  # (1,S)
        
            # Compute normalized past offset u = (i - j) / (i + 1)
            diff = (i - j).clamp(min=0.0)  # zero if j â‰¥ i (future)
            horizon = (i + 1).clamp(min=1.0)  # prevent division by zero
            u = (diff / horizon).clamp(max=1.0)
        
            # Smooth decay from 1 to eps as we move into the past
            decay = torch.exp(1.0 - 1.0 / (1.0 - u ** 2 + 1e-6))
            decay = decay.clamp_max(1.0)
        
            # Build mask: decay for j < i (past), 1 for j â‰¥ i (future including present)
            mask = torch.where(j >= i, torch.ones_like(decay), decay)
            mask = mask.clamp(min=eps)
            return mask  # shape (S, S)

    def set_creativity(self, value: bool):
        val = torch.tensor(value)
        def recurse(m):
            if hasattr(m, "creative"):
                m.creative.copy_(val)
            for child in m.children():
                recurse(child)
        recurse(self)

    def forward(self, idx: torch.Tensor):
        """
        idx: (B, S) token indices
        returns logits: (B, S, vocab_size)
        """
        B, S = idx.shape
        device = idx.device
        #stack 0::2 as embeddings, 1::2 as zeros for positional embeddings
        P = tokens_to_simplex(idx, self.vocab_size)       # (batch, seq, V)
        embeddings = self.convex_embed(P)                          # (batch, seq, d_model)
        x = torch.stack([embeddings, torch.zeros_like(embeddings)], dim=-1).reshape(idx.shape[0], idx.shape[1], self.embed_dim)

        # 3) build causal mask
        mask = self.reversed_logistic_mask(S)          # (1, 1, S, S)

        attn_scores = []
        # 4) apply each block (which will write Ï† into odd slots)
        for blk in self.blocks:
            x,attn_temp = blk(x, mask)
            attn_scores.append(attn_temp)

        attn_scores =  torch.stack(attn_scores).mean(dim=0)#divide by heads
        # 5) final layernorm + head
        x = self.ln_f(x)                             # (B, S, embed_dim)
        logits = self.head(x)                        # (B, S, vocab_size)
        return logits,attn_scores

