{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code copyright joshuah.rainstar@gmail.com joshuah rainstar 2025\n",
    "ConvexGPT License Applies- Accept License or exit this page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare the Shakespeare dataset for character-level language modeling.\n",
    "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
    "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
    "encoder and decoder and some other related info.\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    base_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    base_dir = Path(os.getcwd())  # fallback if __file__ is not defined (e.g. in REPL)\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join(os.path.dirname(base_dir), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(os.path.dirname(base_dir), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.dirname(base_dir), 'val.bin'))\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(os.path.dirname(base_dir), 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMyAv_7rxOyW",
    "outputId": "7408c403-b451-4519-d44d-bb88495af62f"
   },
   "outputs": [],
   "source": [
    "#copyright joshuah.rainstar@gmail.com 2025\n",
    "#protected under license and copyright -proprietary software\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List,Literal\n",
    "\n",
    "#c1 everywhere BUT logsumexp because of the max\n",
    "\n",
    "        \n",
    "class S4DFFT(nn.Module):\n",
    "    \"\"\"\n",
    "    Diagonal State‑Space (S4D) layer with length‑agnostic FFT or recurrent scan.\n",
    "\n",
    "      x : (B,T,D)  ➜  y : (B,T,D)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        N: int          = 64,          # # diagonal modes\n",
    "        init: str       = \"hippoD\",    # 'hippoD' | 'inverse' | 'linear'\n",
    "        short_thresh: int = 512,       # switch to recurrent if T ≤ this\n",
    "        tau_min: float  = 1e-4,        # clamp on exp(log_tau)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert N % 2 == 0, \"N must be even (conjugate pairs).\"\n",
    "\n",
    "        self.d_model, self.N = d_model, N\n",
    "        self.tau_min = tau_min\n",
    "\n",
    "        # unconstrained parameters for N/2 distinct modes\n",
    "        self.log_tau = nn.Parameter(torch.randn(N // 2))\n",
    "        self.freq    = nn.Parameter(torch.randn(N // 2))\n",
    "        self.B       = nn.Parameter(torch.randn(N // 2))\n",
    "        self.C       = nn.Parameter(torch.randn(N // 2))\n",
    "\n",
    "        # input/output projections\n",
    "        self.in_proj  = nn.Linear(d_model, N // 2, bias=False)\n",
    "        self.out_proj = nn.Linear(N // 2, d_model, bias=False)\n",
    "\n",
    "        # learnable global time‑scale Δt  (log‑domain)\n",
    "        self.log_dt = nn.Parameter(torch.zeros(()))\n",
    "\n",
    "        self._init_modes(init)\n",
    "\n",
    "    # ----- initialisers --------------------------------------------------\n",
    "    def _init_modes(self, kind: Literal[\"hippoD\", \"inverse\", \"linear\"]):\n",
    "        n = torch.arange(self.N // 2)\n",
    "        with torch.no_grad(): \n",
    "            self.log_tau.fill_(math.log(0.5))\n",
    "            if kind == \"hippoD\":\n",
    "                self.freq.copy_(math.pi * (2*n + 1) / 2)\n",
    "            elif kind == \"inverse\":\n",
    "                self.freq.copy_((self.N / math.pi) / (2*n + 1))\n",
    "            elif kind == \"linear\":\n",
    "                self.freq.copy_(math.pi * n)\n",
    "            else:\n",
    "                raise ValueError(kind)\n",
    "            nn.init.normal_(self.B,  mean=1.0, std=0.2)\n",
    "            nn.init.normal_(self.C,  std=1.0 / math.sqrt(self.N/2))\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Real‑only kernel builder\n",
    "    # ---------------------------------------------------------------------------\n",
    "    def _kernel_fft(self, T: int):\n",
    "        \"\"\"\n",
    "        Return RFFT(K) where K is the real convolution kernel of length T.\n",
    "          output: (N, L/2+1) complex\n",
    "        Everything up to the final rfft is real‑typed.\n",
    "        \"\"\"\n",
    "        L   = self._next_pow_two(2 * T)\n",
    "\n",
    "        dt   = torch.exp(self.log_dt)                      # scalar\n",
    "        tau  = torch.exp(self.log_tau).clamp(min=self.tau_min)   # (N/2,)\n",
    "        angle = self.freq * dt                                   # (N/2,)\n",
    "\n",
    "        # |lam|  = exp(-tau*dt)            (real)\n",
    "        # arg(lam)= angle                  (real)\n",
    "        lam_mag = torch.exp(-tau * dt)                         # (N/2,)\n",
    "        log_gain = (self.B.abs() + 1e-9).log() + \\\n",
    "                  (self.C.abs() + 1e-9).log()                 # (N/2,)\n",
    "\n",
    "        i = torch.arange(T, device=tau.device)                 # (T,)\n",
    "\n",
    "        # amplitude term  (N/2,T)   — still real\n",
    "        log_lam_mag = lam_mag.log()\n",
    "        scaled_i = i[None] * log_lam_mag[:, None]\n",
    "        amp = torch.exp(log_gain[:, None] + scaled_i)\n",
    "\n",
    "        # phase term\n",
    "        phase = i[None] * angle[:, None]                       # (N/2,T)\n",
    "\n",
    "        K_half = amp * torch.cos(phase)                        # (N/2,T) real\n",
    "\n",
    "        # build full length‑N kernel (conjugate pair ⇒ symmetry in mode index)\n",
    "        K_full = torch.cat([K_half, K_half.flip(0)], dim=0)     # (N,T) real\n",
    "\n",
    "        return torch.fft.rfft(K_full, n=L, dim=-1)             # (N,L/2+1) complex\n",
    "\n",
    "    @staticmethod\n",
    "    def _next_pow_two(n: int) -> int:\n",
    "        # smallest power of two ≥ n, in O(1) bit ops\n",
    "        # (from Hacker’s Delight)\n",
    "        n = n - 1\n",
    "        n = n | (n >> 1)\n",
    "        n = n | (n >> 2)\n",
    "        n = n | (n >> 4)\n",
    "        n = n | (n >> 8)\n",
    "        n = n | (n >> 16)\n",
    "        n = n | (n >> 16)\n",
    "\n",
    "        # if you worry about >32‐bit dims, add: n |= (n >> 32)\n",
    "        return n + 1\n",
    "\n",
    "    # ----- forward (FFT or scan) ----------------------------------------\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, _ = x.shape\n",
    "        x_proj  = self.in_proj(x)                               # (B,T,N/2)\n",
    "        x_modes = torch.cat([x_proj, x_proj.flip(-1)], dim=-1)  # (B,T,N)  real\n",
    "\n",
    "        L  = self._next_pow_two(2 * T)\n",
    "        Uf = torch.fft.rfft(x_modes, n=L, dim=1).transpose(1, 2)   # (B,N,L/2+1)\n",
    "\n",
    "        Kf = self._kernel_fft(T)                                   # (N,L/2+1)\n",
    "        Yf = Uf * Kf[None]                                         # broadcast\n",
    "\n",
    "        y_modes = torch.fft.irfft(Yf, n=L, dim=2)[..., :T]          # (B,N,T)\n",
    "        y_modes = y_modes.transpose(1, 2)                          # (B,T,N)\n",
    "        y       = y_modes[..., : self.N // 2]                       # (B,T,N/2)\n",
    "        return self.out_proj(y)\n",
    "\n",
    "class S4PreMix(nn.Module):\n",
    "    def __init__(self, embed_dim: int, heads: int):\n",
    "        super().__init__()\n",
    "        # compute per-head and inner dimensions\n",
    "        assert embed_dim % heads == 0, \"embed_dim must be divisible by heads\"\n",
    "        self.heads = heads\n",
    "        self.d_k = embed_dim // heads\n",
    "        assert self.d_k % 2 == 0 , \"self.d_dk must be divisible by 2\"\n",
    "\n",
    "        # choose number of modes = d_k\n",
    "        self.N_modes = self.d_k//2 #cannot meaningfully use more than self.dk, optimizing for half- a low pass. \n",
    "        # S4D preprocessing\n",
    "        self.s4d = S4DFFT(d_model=self.d_k, N=self.N_modes)\n",
    "        # QKV projection at inner_dim = embed_dim\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B, S, embed_dim)\n",
    "        B, S, E = x.shape\n",
    "        # apply per-head S4 after projecting to embed_dim\n",
    "        x = x.view(B * self.heads, S, self.d_k)\n",
    "        x = self.s4d(x)\n",
    "        x = x.view(B, S, E)\n",
    "        # compute QKV\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        # reshape to heads\n",
    "        q = q.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        k = k.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        v = v.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        return q, k, v\n",
    "\n",
    "class LinearPreMix(nn.Module):\n",
    "    def __init__(self, embed_dim: int, heads: int):\n",
    "        super().__init__()\n",
    "        assert embed_dim % heads == 0, \"embed_dim must be divisible by heads\"\n",
    "        self.heads = heads\n",
    "        self.d_k = embed_dim // heads\n",
    "        # direct QKV projection\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B, S, embed_dim)\n",
    "        B, S, E = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        q = q.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        k = k.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        v = v.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        return q, k, v\n",
    "\n",
    "class BatchedICNN(nn.Module):\n",
    "    def __init__(self, in_dim: int, petals: int):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.P = petals\n",
    "        D = in_dim\n",
    "        # layer dims: D → 2D → D\n",
    "        self.d1, self.d2 = 2 * D, D\n",
    "\n",
    "        # first-layer weights: (P, d1, D)\n",
    "        self.weight_raw_0 = nn.Parameter(self._init_weight(petals, self.d1, D))\n",
    "        self.bias_0       = nn.Parameter(torch.zeros(petals, self.d1))\n",
    "\n",
    "        # second-layer weights: (P, d2, d1)\n",
    "        self.weight_raw_1 = nn.Parameter(self._init_weight(petals, self.d2, self.d1))\n",
    "        self.bias_1       = nn.Parameter(torch.zeros(petals, self.d2))\n",
    "\n",
    "        # per-petal residual projection weight: maps 2D → D: shape (P, d1, d2)\n",
    "        self.z_weight = nn.Parameter(torch.empty(petals, self.d1, self.d2))\n",
    "        nn.init.kaiming_uniform_(self.z_weight, a=math.sqrt(5))\n",
    "\n",
    "        # gating scalars\n",
    "        self.gate_raw_0 = nn.Parameter(torch.full((petals,), -3.0))\n",
    "        self.gate_raw_1 = nn.Parameter(torch.full((petals,), -3.0))\n",
    "\n",
    "        self.output_bias = nn.Parameter(torch.zeros(petals, D))\n",
    "        self.act = nn.Softplus()\n",
    "\n",
    "    def _init_weight(self, petals: int, d_out: int, d_in: int) -> torch.Tensor:\n",
    "        w = torch.empty(petals, d_out, d_in)\n",
    "        with torch.no_grad():\n",
    "            mean = math.log(math.sqrt(2.0 / d_in))\n",
    "            nn.init.normal_(w, mean=mean, std=0.2)\n",
    "        return w\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (..., D)\n",
    "        orig = x.shape\n",
    "        x_flat = x.reshape(-1, self.in_dim)          # (N, D)\n",
    "        N = x_flat.size(0)\n",
    "\n",
    "        # prepare weights & gates\n",
    "        w0 = F.softplus(self.weight_raw_0).pow(2)    # (P, 2D, D)\n",
    "        w1 = F.softplus(self.weight_raw_1).pow(2)    # (P, D, 2D)\n",
    "        g0 = torch.sigmoid(self.gate_raw_0).view(self.P, 1, 1)  # (P,1,1)\n",
    "        g1 = torch.sigmoid(self.gate_raw_1).view(self.P, 1, 1)\n",
    "\n",
    "        # ----- first layer across petals -----\n",
    "        x_in = x_flat.unsqueeze(0).expand(self.P, N, self.in_dim)       # (P, N, D)\n",
    "        x_w0 = torch.bmm(x_in, w0.transpose(1,2))                       # (P, N, 2D)\n",
    "        x_w0 = x_w0 + self.bias_0.unsqueeze(1)                          # (P, N, 2D)\n",
    "        z0   = self.act(x_w0 * g0)                                      # (P, N, 2D)\n",
    "\n",
    "        # ----- second layer -----\n",
    "        x_w1 = torch.bmm(z0, w1.transpose(1,2))                         # (P, N, D)\n",
    "        x_w1 = x_w1 + self.bias_1.unsqueeze(1)                          # (P, N, D)\n",
    "\n",
    "        # ----- residual path via bmm -----\n",
    "        # z_weight: (P, 2D, D), z0: (P, N, 2D) → z_mapped: (P, N, D)\n",
    "        z_mapped = torch.bmm(z0, self.z_weight)                         # (P, N, D)\n",
    "\n",
    "        # combine, activate, add final bias\n",
    "        z1 = self.act(x_w1 * g1 + z_mapped)                             # (P, N, D)\n",
    "        out = z1 + self.output_bias.unsqueeze(1)                        # (P, N, D)\n",
    "\n",
    "        # reshape back to original leading dims + (P, D)\n",
    "        out = out.permute(1, 0, 2)  # (N, P, D)\n",
    "        lead_dims = list(orig[:-1])                 # e.g. [B, H, T]\n",
    "        new_shape = lead_dims + [self.P, self.in_dim]  # [B, H, T, P, D]\n",
    "        return out.reshape(new_shape)\n",
    "\n",
    "'''\n",
    "Canonical φ(x) builds φ ∈ ℝ^{B, D_in, K}.\n",
    "\n",
    "When K=8, D_in=8192, B=128 → φ alone = 8.3M floats.\n",
    "FFT variant avoids this explicit K-expansion and instead processes one padded array per batch element (ℝ^{B, L} and L ≈ D_in).\n",
    "caveat: it must be trained. it varies by d_in only. so you need pre-learned kernels.\n",
    "in practice the error is fairly contained. \n",
    "\n",
    "naively, our cost is O(N * D_in * D_in*2 * K) for this model since d_out is always twice d_in and K is always 8:\n",
    "that's a quadratic term times 16: O(N*embed_dim^2*16).\n",
    "This fft form scales as : O(N * D_in * (log D_in + K))\n",
    "In practice, this converges on quasi-linear growth and enables much more massive KAN utilization with minimal cost, provided\n",
    "you pre-learn the kernels. however, since x varies, how do you prelearn the kernel? simple. you zero-mean it. this ensures the kernels are scale invariant.\n",
    "This means our more economical KAN module can be quickly instantiated with desired behaviors that approximate nonlinear transformations.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "D_in = 64\n",
    "D_out = 128\n",
    "K = 8\n",
    "batch_size = 128\n",
    "epochs = 5000\n",
    "lr = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def wolf_update(p: torch.Tensor,\n",
    "                g: torch.Tensor,\n",
    "                state_p: torch.Tensor,\n",
    "                lr: float):\n",
    "    # define your constants here instead of capturing them\n",
    "    etcerta: float = 0.367879441\n",
    "    et:      float = 1.0 - etcerta\n",
    "\n",
    "    # same logic as before\n",
    "    update    = state_p * et + g * etcerta\n",
    "    new_state = state_p * et + update * etcerta\n",
    "    sign_agree = torch.sign(update) * torch.sign(g)\n",
    "    update    = update + (torch.rand_like(update)*2 - 1) * etcerta * update\n",
    "    p_new     = torch.where(sign_agree > 0, p - lr * update, p)\n",
    "    return p_new, new_state\n",
    "from torch.optim.optimizer import Optimizer\n",
    "class Wolf(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p]['p'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = closure() if closure is not None else None\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state_p = self.state[p]['p']\n",
    "                p_new, new_state = wolf_update(p.data, p.grad, state_p, lr)\n",
    "                p.data.copy_(p_new)\n",
    "                state_p.copy_(new_state)\n",
    "        return loss\n",
    "\n",
    "class FixedLayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps)\n",
    "\n",
    "# Canonical φ(x) with fixed shifts\n",
    "class CanonicalPhi(nn.Module):\n",
    "    def __init__(self, shifts, W, b, D_in):\n",
    "        super().__init__()\n",
    "        self.shifts = shifts\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.norm = FixedLayerNorm(D_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm(x)  # Normalize x\n",
    "        xb = x_norm.unsqueeze(-1) + self.shifts.view(1, 1, K)  # (N, D_in, K)\n",
    "        phi = F.softplus(xb)  # (N, D_in, K)\n",
    "        return torch.einsum(\"nik,dik->nd\", phi, self.W) + self.b  # (N, D_out)\n",
    "\n",
    "\n",
    "\n",
    "# FFT encoder with decoder (trainable linear layer)\n",
    "class FFTSharedKFilters(nn.Module):\n",
    "    def __init__(self, K_shifts, D_in, shifts_values):\n",
    "        super().__init__()\n",
    "        self.K_shifts = K_shifts\n",
    "        self.D_in = D_in\n",
    "        self.D_out = 2 * D_in\n",
    "        self.L = D_in\n",
    "        self.Lf = self.L // 2 + 1\n",
    "\n",
    "        self.log_amp_shifts = nn.Parameter(torch.randn(self.K_shifts, self.Lf) * 0.01)\n",
    "        self.phase_shifts = nn.Parameter(torch.randn(self.K_shifts, self.Lf) * 0.01)\n",
    "        self.final_bias = nn.Parameter(torch.zeros(self.D_out))\n",
    "        self.norm = FixedLayerNorm(D_in)  # Apply normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        x_norm = self.norm(x)  # Normalize x\n",
    "\n",
    "        Xf = torch.fft.rfft(x_norm, n=self.L, dim=-1)  # (N, Lf)\n",
    "        Kf_shifts = torch.exp(self.log_amp_shifts) * torch.exp(-1j * self.phase_shifts)\n",
    "        Yf_k = Xf.unsqueeze(1) * Kf_shifts.unsqueeze(0)  # (N, K_shifts, Lf)\n",
    "        x_time_k = torch.fft.irfft(Yf_k, n=self.L, dim=-1)  # (N, K_shifts, D_in)\n",
    "        phi_k_d = F.softplus(x_time_k)  # (N, K_shifts, D_in)\n",
    "        S_n_learned = phi_k_d.sum(dim=[1, 2])  # (N,)\n",
    "        return S_n_learned.unsqueeze(-1) + self.final_bias.unsqueeze(0)  # (N, D_out)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "shifts = torch.linspace(0.0, 1.0, 8).to(device)\n",
    "\n",
    "# Initialize models and training targets\n",
    "fft_model = FFTSharedKFilters(8, D_in, D_out).to(device)\n",
    "W_ref = nn.Parameter(torch.ones(D_out, D_in, 8), requires_grad=False).to(device)\n",
    "b_ref = nn.Parameter(torch.zeros(D_out), requires_grad=False).to(device)\n",
    "canonical_phi_model = CanonicalPhi(shifts, W_ref, b_ref, D_in).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(fft_model.parameters(), lr=3e-2)\n",
    "losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    x = torch.randn(batch_size, D_in, device=device)\n",
    "    with torch.no_grad():\n",
    "        target  = canonical_phi_model(x)\n",
    "\n",
    "    pred = fft_model(x)\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    losses.append(loss.item())\n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}, loss {loss.item():.4e}\")\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, label=\"FFT Encoder + Linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training FFT Encoder to Match φ(x) with Trainable Linear Layer\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Report parameters\n",
    "param_count = sum(p.numel() for p in fft_model.parameters())\n",
    "print(f\"Trainable parameters: {param_count}\")\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "class KCN(nn.Module):\n",
    "    def __init__(self, in_dim: int, petals: int, out_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.in_dim  = in_dim\n",
    "        self.out_dim = out_dim if out_dim is not None else in_dim\n",
    "        self.P       = petals\n",
    "        self.k       = 8 #empirically and theoretically, shouldnt exceed min(batch,petals), not important\n",
    "        D_in, D_out = self.in_dim, self.out_dim\n",
    "\n",
    "        # — edge-wise basis weights — (P, D_out, D_in, k)\n",
    "        # we'll later flatten the last two dims to match BatchedICNN’s use of bmm\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.stack([\n",
    "                self._init_weight(1, D_out, D_in)[0] for _ in range(self.k)\n",
    "            ], dim=-1)  # shape: (D_out, D_in, k)\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.zeros(D_out))\n",
    "\n",
    "\n",
    "        self.weight2 = nn.Parameter(self._init_weight(petals, D_out, D_out))\n",
    "        self.bias2   = nn.Parameter(torch.zeros(petals, D_out))\n",
    "        self.gate_raw2 = nn.Parameter(torch.full((petals,), -3.0))\n",
    "\n",
    "        # shared Softplus shifts: shape (1, D_in, k)\n",
    "        shifts = torch.linspace(-1.0, 1.0, self.k ).view(1, 1, self.k )  # shape: (1, 1, k)\n",
    "        self.register_buffer(\"shifts\", shifts)              # broadcast when used\n",
    "\n",
    "        # residual projection exactly like BatchedICNN: (P, 2*D_in, D_out)\n",
    "        self.z_weight = nn.Parameter(torch.empty(petals, 2 * D_in, D_out))\n",
    "        nn.init.kaiming_uniform_(self.z_weight, a=math.sqrt(5))\n",
    "\n",
    "        # gating scalars per petal\n",
    "        self.gate_raw = nn.Parameter(torch.full((petals,), -3.0))\n",
    "\n",
    "        # final bias per petal/output\n",
    "        self.output_bias = nn.Parameter(torch.zeros(petals, D_out))\n",
    "        self.act = nn.Softplus()\n",
    "   \n",
    "    \n",
    "    def _init_weight(self, petals: int, d_out: int, d_in: int) -> torch.Tensor:\n",
    "        w = torch.empty(petals, d_out, d_in)\n",
    "        with torch.no_grad():\n",
    "            mean = math.log(math.sqrt(2.0 / d_in))\n",
    "            nn.init.normal_(w, mean=mean, std=0.2)\n",
    "        return w\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (..., D_in)\n",
    "        orig = x.shape\n",
    "        x_flat = x.reshape(-1, self.in_dim)       # (N, D_in)\n",
    "        N = x_flat.size(0)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 1) compute basis activations φ(x): (N, D_in, k)\n",
    "        xb  = x_flat.unsqueeze(-1) + self.shifts       # (N, D_in, k)\n",
    "        phi = F.softplus(xb)                           # (N, D_in, k)\n",
    "\n",
    "        # flatten φ for bmm\n",
    "        # reshape to (N, D_in * k)\n",
    "        phi_flat = phi.reshape(N, self.in_dim * self.k)  \n",
    "\n",
    "        # reshape weight to (D_in * k, D_out)\n",
    "        weight_flat = self.weight.reshape(self.in_dim * self.k, self.out_dim)\n",
    "\n",
    "        # shared projection\n",
    "        x_proj_shared = phi_flat @ weight_flat + self.bias  # (N, D_out)\n",
    "\n",
    "        # duplicate to all petals\n",
    "        x_proj = x_proj_shared.unsqueeze(0).expand(self.P, N, self.out_dim)  # (P, N, D_out)\n",
    "        g1 = torch.sigmoid(self.gate_raw).view(self.P, 1, 1)\n",
    "        z0 = self.act(x_proj * g1)  # (P, N, D_out)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 3) second projection\n",
    "        w2 = F.softplus(self.weight2).pow(2).transpose(1, 2)  # (P, D_out, D_out)\n",
    "        z1 = torch.bmm(z0, w2) + self.bias2.unsqueeze(1)      # (P, N, D_out)\n",
    "        g2 = torch.sigmoid(self.gate_raw2).view(self.P, 1, 1)\n",
    "        z1 = self.act(z1 * g2)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 4) residual projection\n",
    "        x_res_in = torch.cat([x_flat, x_flat], dim=-1)                       # (N, 2*D_in)\n",
    "        x_res_exp = x_res_in.unsqueeze(0).expand(self.P, N, 2 * self.in_dim)  # (P, N, 2*D_in)\n",
    "        x_res = torch.bmm(x_res_exp, self.z_weight)                          # (P, N, D_out)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 5) combine and output\n",
    "        z_final = self.act(z1 + x_res) + self.output_bias.unsqueeze(1)       # (P, N, D_out)\n",
    "        out = z_final.permute(1, 0, 2)                                       # (N, P, D_out)\n",
    "        new_shape = list(orig[:-1]) + [self.P, self.out_dim]\n",
    "        return out.reshape(new_shape)      \n",
    "\n",
    "\n",
    "class ConvexGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Convex & bounded gate: g(x) = 1 - exp(-softplus(Wx + b)) ∈ (0,1)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_dim, 1, bias=True)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = self.softplus(self.lin(x))      # convex, ≥ 0\n",
    "        return 1.0 - torch.exp(-u)       # convex, ∈ (0,1)\n",
    "\n",
    "class _FusedLogSumExp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, dim):\n",
    "        m, _ = x.max(dim=dim, keepdim=True)\n",
    "        y = x - m\n",
    "        ex = y.exp()\n",
    "        s = ex.sum(dim=dim, keepdim=True)\n",
    "        lse = m + s.log()\n",
    "        ctx.save_for_backward(ex, s)\n",
    "        ctx.dim = dim\n",
    "        return lse\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ex, s = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_x = grad_output * (ex / s)\n",
    "        return grad_x, None\n",
    "\n",
    "# TorchScript-compatible wrapper\n",
    "class FusedLogSumExp(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return _FusedLogSumExp.apply(x, self.dim)\n",
    "\n",
    "\n",
    "\n",
    "class ScalarHull(nn.Module):\n",
    "    def __init__(self, in_dim: int, petals: int):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.register_buffer('nu',  torch.tensor(1.64872127070012814684865078781416357165))\n",
    "        self.register_buffer('noise_scale', torch.tensor(1e-5))\n",
    "        self.petals = KCN(self.in_dim, petals)\n",
    "        self.gate   = ConvexGate(in_dim)\n",
    "        self.register_buffer(\"creative\", torch.tensor(True))\n",
    "        self.register_buffer('eps', torch.tensor(1e-6))\n",
    "        self.fused_lse_hulls = FusedLogSumExp(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (..., D)\n",
    "        g   = self.gate(x)                                   # (..., 1)\n",
    "\n",
    "        #creativity toggle here\n",
    "\n",
    "        if self.creative:\n",
    "            xg   = (x + torch.randn_like(x) * self.noise_scale) * g # (..., D)\n",
    "        else:\n",
    "            xg   = x  * g # (..., D)\n",
    "\n",
    "        # compute τ using a soft logistic\n",
    "        r = torch.sqrt(xg.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        tau = torch.exp(0.30343 * r + 0.22159)\n",
    "        # get each petal’s vector output, then reduce to scalar per petal\n",
    "        out_all = self.petals(xg)                  # (..., P, D)\n",
    "        scores  = out_all.mean(dim=-1)             # (..., P)\n",
    "\n",
    "        # tempered LSE over petals\n",
    "        # scaled: (..., P) = scores * τ\n",
    "        scaled = scores * tau                     # broadcasts τ→[...,1]→[...,P]\n",
    "\n",
    "        lse    = self.fused_lse_hulls(scaled)  # (..., 1)\n",
    "\n",
    "        # divide by τ and squeeze\n",
    "        return (lse / tau).squeeze(-1)             # (...,)\n",
    "\n",
    "class VectorHull(nn.Module):\n",
    "    def __init__(self, dim: int, petals: int, out_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.in_dim = dim\n",
    "        self.out_dim = out_dim if out_dim is not None else dim\n",
    "        self.register_buffer('noise_scale', torch.tensor(1e-5))\n",
    "        self.petals = KCN(self.in_dim, petals)\n",
    "        self.gate   = ConvexGate(dim)\n",
    "        self.register_buffer(\"creative\", torch.tensor(True))\n",
    "        self.register_buffer('eps', torch.tensor(1e-6))\n",
    "        self.fused_lse_hulls = FusedLogSumExp(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (..., in_dim)\n",
    "        g = self.gate(x)  # (..., 1)\n",
    "\n",
    "        # apply creativity scaling\n",
    "        if self.creative:\n",
    "            xg = (x + torch.randn_like(x) * self.noise_scale) * g  # (..., in_dim)\n",
    "        else:\n",
    "            xg = x * g\n",
    "\n",
    "        # compute τ\n",
    "        r = torch.sqrt(xg.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        tau = torch.exp(0.30343 * r + 0.22159)  # (..., 1)\n",
    "\n",
    "        # batched ICNN output (..., P, out_dim)\n",
    "        out_all = self.petals(xg)  # (..., P, out_dim)\n",
    "\n",
    "        # scale each vector per petal\n",
    "        scaled = out_all * tau.unsqueeze(-1)  # (..., P, out_dim)\n",
    "\n",
    "        # transpose petals to last dim for LSE\n",
    "        scaled = scaled.transpose(-2, -1)     # (..., out_dim, P)\n",
    "\n",
    "        # fused LSE over petals\n",
    "        lse = self.fused_lse_hulls(scaled).squeeze(-1)  # (..., out_dim)\n",
    "\n",
    "        # divide out τ\n",
    "        return lse / tau  # (..., out_dim)\n",
    "        \n",
    "class ConvexMixer(nn.Module):\n",
    "    def __init__(self, d_k: int, petals: int, r: int):\n",
    "        super().__init__()\n",
    "        self.register_buffer('eps', torch.tensor(1e-6))\n",
    "        self.register_buffer('noise_scale', torch.tensor(1e-5))\n",
    "\n",
    "        self.score_q = ScalarHull(d_k, petals)\n",
    "        self.score_k = ScalarHull(d_k, petals)\n",
    "        self.gate = nn.Softplus()\n",
    "        self.lin_h_q = nn.Linear(d_k, r, bias=False)\n",
    "        self.lin_h_k = nn.Linear(d_k, r, bias=False)\n",
    "        self.register_buffer('creative', torch.tensor(True))\n",
    "        self.fused = FusedLogSumExp(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        B, H, S, D = q.shape\n",
    "        device = q.device\n",
    "\n",
    "        # ——— 1) tau ———\n",
    "        gate_q = self.gate(q)                          # (B,H,S,d_k)\n",
    "        q = q * gate_q\n",
    "        r = torch.sqrt(q.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        tau = torch.exp(0.30343 * r + 0.22159)  # or + tau_min\n",
    "\n",
    "        # ——— 2) scalar hull scores ———\n",
    "        fq = self.score_q(q)  # (B,H,S)\n",
    "        gk = self.score_k(k)  # (B,H,S)\n",
    "        if self.creative:\n",
    "            qn = (torch.rand_like(q) - 0.5) * self.noise_scale\n",
    "            kn = (torch.rand_like(k) - 0.5) * self.noise_scale\n",
    "            fq_ = self.score_q(q + qn)\n",
    "            gk_ = self.score_k(k + kn)\n",
    "            delta_fq = (fq_ - fq).detach()\n",
    "            delta_gk = (gk_ - gk).detach()\n",
    "            fq = fq - 0.1 * delta_fq\n",
    "            gk = gk - 0.1 * delta_gk\n",
    "\n",
    "        # ——— 3) random-feature kernel ———\n",
    "        phi_q = self.gate(self.lin_h_q(q).clamp(max=20.0))\n",
    "        phi_k = self.gate(self.lin_h_k(k).clamp(max=20.0))\n",
    "        log_phi_q = torch.log(phi_q + self.eps)\n",
    "        log_phi_k = torch.log(phi_k + self.eps)\n",
    "        sum_ab = log_phi_q.unsqueeze(-2) + log_phi_k.unsqueeze(-3)  # (B,H,S,S,r)\n",
    "        logK = self.fused(sum_ab).squeeze(-1)                        # (B,H,S,S)\n",
    "\n",
    "        # 5) Assemble logits with mask and temperature\n",
    "        log_mask = torch.log(mask.clamp_min(self.eps))  # convert to log-domain\n",
    "        scores = fq.unsqueeze(-1) + gk.unsqueeze(-2) + logK + log_mask  # additive\n",
    "        \n",
    "        logits = scores * tau.squeeze(-1).unsqueeze(-1)\n",
    "        log_weights = logits - torch.logsumexp(logits, dim=-1, keepdim=True)\n",
    "        weights = torch.exp(log_weights)  # (B,H,S,S)\n",
    "\n",
    "        # 6) Weighted sum for attention output\n",
    "        out = weights.reshape(B * H, S, S).bmm(v.reshape(B * H, S, D))\n",
    "        out = out.reshape(B, H, S, D)\n",
    "\n",
    "        # Optional: compute aggregated attn_score\n",
    "        attn_score = weights.sum(dim=-3)\n",
    "        attn_score = torch.softmax(attn_score, dim=-1).mean(dim=1)\n",
    "        min_vals = attn_score.min(dim=-1, keepdim=True).values\n",
    "        max_vals = attn_score.max(dim=-1, keepdim=True).values\n",
    "        attn_score = (attn_score - min_vals) / (max_vals - min_vals + self.eps)\n",
    "\n",
    "        return out, attn_score\n",
    "\n",
    "\n",
    "        \n",
    "class InterleavedPhaseChannelizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding shape: (B, T, 2*M) == [c0, ϕ0, c1, ϕ1, ..., c_{M-1}, ϕ_{M-1}].\n",
    "    Now uses a convex bump kernel instead of sine.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, init_gate_bias: float = -3.0, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 2 == 0, \"embed_dim must be even\"\n",
    "        self.M = embed_dim // 2\n",
    "        self.gate_raw = nn.Parameter(torch.full((self.M,), init_gate_bias))\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.eps = eps\n",
    "\n",
    "    def bump_kernel(self, T: int, device: torch.device):\n",
    "        \"\"\"\n",
    "        Returns a (T, T) kernel K[i,j] ∈ (0,1], convex in |j-i|,\n",
    "        with K[i,i]=1, and smoothly → eps as j→end.\n",
    "        \"\"\"\n",
    "        i = torch.arange(T, device=device).unsqueeze(1).float()      # (T,1)\n",
    "        j = torch.arange(T, device=device).unsqueeze(0).float()      # (1,T)\n",
    "\n",
    "        # future offset u = (j - i) / (T - i)\n",
    "        diff    = (j - i).clamp(min=0.0)                            # (T,T)\n",
    "        horizon = (T - i).clamp(min=1.0)                            # (T,1)\n",
    "        u       = (diff / horizon).clamp(max=1.0 - self.eps)        # [0,1)\n",
    "\n",
    "        # bump exponent: convex, =1 at u=0, → -∞ as u→1\n",
    "        expnt = 1.0 - 1.0 / (1.0 - u*u + self.eps)\n",
    "        K      = torch.exp(expnt)                                   # (T,T)\n",
    "\n",
    "        # enforce exact eps at u≈1\n",
    "        K = torch.where(u >= 1.0 - self.eps, torch.full_like(K, self.eps), K)\n",
    "        return K  # (T,T)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, 2*M)\n",
    "        B, T, _ = x.shape\n",
    "        M       = self.M\n",
    "        device  = x.device\n",
    "        dtype   = x.dtype\n",
    "\n",
    "        # 1) content channels\n",
    "        x_c = x[..., 0::2]  # (B, T, M)\n",
    "\n",
    "        # 2) build convex bump kernel and mask it\n",
    "        K = self.bump_kernel(T, device).to(dtype)   # (T,T)\n",
    "        causal2d = mask.view(T, T).to(dtype)        # (T,T)\n",
    "        K = K * causal2d                            # zero out masked-out positions\n",
    "\n",
    "        # 3) normalize rows to sum=1\n",
    "        K = K / (K.sum(-1, keepdim=True).clamp(min=self.eps))\n",
    "\n",
    "        # 4) accumulate phase φ[b,i,m] = ∑₍ⱼ₌₀…T₋₁₎ K[i,j] · x_c[b,j,m]\n",
    "        φ = torch.einsum('ij,bjm->bim', K, x_c)     # (B, T, M)\n",
    "\n",
    "        # 5) gate & write back into odd slots\n",
    "        gate = self.softplus(self.gate_raw).view(1,1,M)  # (1,1,M)\n",
    "        φg   = φ * gate                                 # (B, T, M)\n",
    "        out  = x.clone()\n",
    "        out[..., 1::2] = φg\n",
    "        return out\n",
    "\n",
    "class ConvexRoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    Convex RoPE substitute with dynamic sequence length.\n",
    "    Generates a monotonic, convex angle for each position-pair subspace.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_k: int):\n",
    "        super().__init__()\n",
    "        assert d_k % 2 == 0, \"d_k must be even for pairing\"\n",
    "        self.d_pair = d_k // 2\n",
    "        # Linear mapping from scalar pos to angle per pair\n",
    "        self.lin = nn.Linear(1, self.d_pair)\n",
    "\n",
    "    def forward(self, S: int, device: torch.device) -> torch.Tensor:\n",
    "        # positions normalized to [0,1]\n",
    "        pos = torch.arange(S, device=device, dtype=torch.float32).unsqueeze(1) / max(S - 1, 1)\n",
    "        θ = F.softplus(self.lin(pos))  # (S, d_pair), convex & ≥0\n",
    "        return θ  # shape (S, d_pair)\n",
    "\n",
    "from typing import Tuple\n",
    "def apply_convex_rope(q: torch.Tensor, k: torch.Tensor, θ: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:    \n",
    "    \"\"\"\n",
    "    Apply ConvexRoPE rotation to q and k.\n",
    "      q, k: (B, H, S, d_k)\n",
    "      θ:    (S, d_k//2)\n",
    "    Returns rotated q, k of same shape.\n",
    "    \"\"\"\n",
    "    B, H, S, d_k = q.shape\n",
    "    d2 = d_k // 2\n",
    "\n",
    "    # reshape into (paired) shape\n",
    "    q_ = q.view(B, H, S, d2, 2)\n",
    "    k_ = k.view(B, H, S, d2, 2)\n",
    "\n",
    "    # split into even (x) and odd (y) parts\n",
    "    x_q, y_q = q_.unbind(-1)  # each (B, H, S, d2)\n",
    "    x_k, y_k = k_.unbind(-1)\n",
    "\n",
    "    # build cos/sin with correct shape (1,1,S,d2) to broadcast\n",
    "    cosθ = torch.cos(θ).unsqueeze(0).unsqueeze(0)  # (1,1,S,d2)\n",
    "    sinθ = torch.sin(θ).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # rotate x/y pairs\n",
    "    x_q2 = x_q * cosθ - y_q * sinθ\n",
    "    y_q2 = x_q * sinθ + y_q * cosθ\n",
    "    x_k2 = x_k * cosθ - y_k * sinθ\n",
    "    y_k2 = x_k * sinθ + y_k * cosθ\n",
    "\n",
    "    # stack back into pairs and reshape to original\n",
    "    q_rot = torch.stack([x_q2, y_q2], dim=-1).reshape(B, H, S, d_k)\n",
    "    k_rot = torch.stack([x_k2, y_k2], dim=-1).reshape(B, H, S, d_k)\n",
    "\n",
    "    return q_rot, k_rot\n",
    "    \n",
    "# ----------------------------------------------------------------------\n",
    "#   Pairwise Hull Attention (mask‑aware)\n",
    "# ----------------------------------------------------------------------\n",
    "class PairwiseHullAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, heads,moe_petals, use):\n",
    "        super().__init__()\n",
    "        assert embed_dim % heads == 0, \"embed_dim must be divisible by heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.d_k = embed_dim // heads\n",
    "        if use==0:\n",
    "            self.pre = S4PreMix(embed_dim, heads)\n",
    "        else:\n",
    "            self.pre = LinearPreMix(embed_dim, heads)\n",
    "        self.mixer = ConvexMixer(self.d_k, moe_petals, self.d_k*2)#dont need many for scoring\n",
    "        self.W_O = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.phase = InterleavedPhaseChannelizer(embed_dim)\n",
    "        self.register_buffer('noise_scale', torch.tensor(1e-5))\n",
    "        self.register_buffer(\"creative\", torch.tensor(True))\n",
    "        self.rope = ConvexRoPE(self.d_k)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        self.phase(x,mask) #apply in-place positional phasing\n",
    "        B, S, E = x.shape\n",
    "        Q, K, V= self.pre(x)\n",
    "        offset = self.rope(S, x.device)                  # (S, d_k//2)\n",
    "        Q, K = apply_convex_rope(Q, K, offset) \n",
    "\n",
    "        mean = 0.5 * (Q.mean() + K.mean())\n",
    "        std  = 0.5 * (Q.std()  + K.std())\n",
    "        Q = (Q - mean) / std\n",
    "        K = (K - mean) / std        \n",
    "        y,attn_scores = self.mixer(Q, K, V, mask=mask)\n",
    "        \n",
    "        y = y.transpose(1, 2).reshape(B, S, self.embed_dim)\n",
    "        return self.W_O(y), attn_scores\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#   OmniHull Block\n",
    "# ----------------------------------------------------------------------\n",
    "class OmniHullBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, moe_petals, use=0):\n",
    "        super().__init__()\n",
    "        self.attn = PairwiseHullAttention(dim, heads, moe_petals, use=use)\n",
    "        self.hff  = VectorHull(dim, moe_petals)\n",
    "\n",
    "        self.ln1, self.ln2,self.ln3 = nn.LayerNorm(dim), nn.LayerNorm(dim), nn.LayerNorm(dim)\n",
    "        self.a1, self.a2   = nn.Parameter(torch.zeros(())), nn.Parameter(torch.zeros(()))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        x1 ,attn_scores = self.attn(self.ln1(x), mask)\n",
    "        x = x + x1\n",
    "        x1 = self.hff(self.ln2(x))\n",
    "        x = x + x1\n",
    "        return x,attn_scores\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#   GPT Wrapper with Causal Mask\n",
    "# ----------------------------------------------------------------------\n",
    "class ConvexGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        depth: int,\n",
    "        heads: int,\n",
    "        moe_petals: int,\n",
    "        creativity: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert embed_dim >= 1, \"embed_channels must be ≥1\"\n",
    "        self.embed_channels = embed_dim\n",
    "        self.embed_dim = 2 * embed_dim\n",
    "\n",
    "        # Embeddings only for even channels [0,2,4,...]\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Blocks operate on full embed_dim\n",
    "        self.blocks = nn.ModuleList([\n",
    "        OmniHullBlock(\n",
    "            self.embed_dim,\n",
    "            heads,\n",
    "            moe_petals,\n",
    "            use=1 if i == 0 or i == depth - 1 else 0\n",
    "                  )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim)\n",
    "        self.head = nn.Linear(self.embed_dim, vocab_size, bias=False)\n",
    "        self.set_creativity(creativity)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_mask(S: int, eps: float = 1e-17) -> torch.Tensor:\n",
    "        i = torch.arange(S).unsqueeze(1).float()  # (S,1)\n",
    "        j = torch.arange(S).unsqueeze(0).float()  # (1,S)\n",
    "    \n",
    "        # Compute normalized future offset u = (j - i) / (S - i)\n",
    "        diff = (j - i).clamp(min=0.0)\n",
    "        horizon = (S - i).clamp(min=1.0)\n",
    "        u = (diff / horizon).clamp(max=1.0)  # ∈ [0,1]\n",
    "    \n",
    "        # Decay from 1 to eps as u ∈ (0, 1]\n",
    "        decay = torch.exp(1.0 - 1.0 / (1.0 - u ** 2 + 1e-6))  # smooth bump\n",
    "        decay = decay.clamp_max(1.0)\n",
    "    \n",
    "        # Build final mask: full for j ≤ i, soft-decay for i < j\n",
    "        mask = torch.where(j <= i, torch.ones_like(decay), decay)\n",
    "        mask = mask.clamp(min=eps)\n",
    "        return mask  # shape (S, S)\n",
    "\n",
    "    def set_creativity(self, value: bool):\n",
    "        val = torch.tensor(value)\n",
    "        def recurse(m):\n",
    "            if hasattr(m, \"creative\"):\n",
    "                m.creative.copy_(val)\n",
    "            for child in m.children():\n",
    "                recurse(child)\n",
    "        recurse(self)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor):\n",
    "        \"\"\"\n",
    "        idx: (B, S) token indices\n",
    "        returns logits: (B, S, vocab_size)\n",
    "        \"\"\"\n",
    "        B, S = idx.shape\n",
    "        device = idx.device\n",
    "        #stack 0::2 as embeddings, 1::2 as zeros for positional embeddings\n",
    "        E = self.token_emb.weight                             # (V, D)\n",
    "        E_proc = (torch.tanh(E))                    # log-domain safe\n",
    "        embeddings = F.embedding(idx, E_proc)                 # (B, S, D)\n",
    "        x = torch.stack([embeddings, torch.zeros_like(self.token_emb(idx))], dim=-1).reshape(idx.shape[0], idx.shape[1], 2 * self.token_emb.embedding_dim)\n",
    "\n",
    "        # 3) build causal mask\n",
    "        mask = self.logistic_mask(S)          # (1, 1, S, S)\n",
    "        attn_scores = []\n",
    "        # 4) apply each block (which will write φ into odd slots)\n",
    "        for blk in self.blocks:\n",
    "            x,attn_temp = blk(x, mask)\n",
    "            attn_scores.append(attn_temp)\n",
    "\n",
    "        attn_scores =  torch.stack(attn_scores).mean(dim=0)#divide by heads\n",
    "        # 5) final layernorm + head\n",
    "        x = self.ln_f(x)                             # (B, S, embed_dim)\n",
    "        logits = self.head(x)                        # (B, S, vocab_size)\n",
    "        return logits,attn_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q47er8Ni84xt"
   },
   "source": [
    "#if you use my ideas, please credit me, dont just steal\n",
    "joshuah.rainstar@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLs7qQUOxRPQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  2231370\n",
      "4.286777019500732\n",
      "3.743297576904297\n",
      "3.467034339904785\n",
      "3.296698808670044\n",
      "3.208189010620117\n",
      "3.135937452316284\n",
      "3.0678083896636963\n",
      "3.004629373550415\n",
      "2.9622044563293457\n",
      "2.8823277950286865\n",
      "2.8752825260162354\n",
      "2.771150588989258\n",
      "2.7917585372924805\n",
      "2.8289716243743896\n",
      "2.772632122039795\n",
      "2.712769031524658\n",
      "2.687892436981201\n",
      "2.7274067401885986\n",
      "2.656449317932129\n",
      "2.707737445831299\n",
      "2.66801118850708\n",
      "2.61427903175354\n",
      "2.614622116088867\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "def wolf_update(p: torch.Tensor,\n",
    "                g: torch.Tensor,\n",
    "                state_p: torch.Tensor,\n",
    "                lr: float):\n",
    "    # define your constants here instead of capturing them\n",
    "    etcerta: float = 0.367879441\n",
    "    et:      float = 1.0 - etcerta\n",
    "\n",
    "    # same logic as before\n",
    "    update    = state_p * et + g * etcerta\n",
    "    new_state = state_p * et + update * etcerta\n",
    "    sign_agree = torch.sign(update) * torch.sign(g)\n",
    "    update    = update + (torch.rand_like(update)*2 - 1) * etcerta * update\n",
    "    p_new     = torch.where(sign_agree > 0, p - lr * update, p)\n",
    "    return p_new, new_state\n",
    "\n",
    "class Wolf(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p]['p'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = closure() if closure is not None else None\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state_p = self.state[p]['p']\n",
    "                p_new, new_state = wolf_update(p.data, p.grad, state_p, lr)\n",
    "                p.data.copy_(p_new)\n",
    "                state_p.copy_(new_state)\n",
    "        return loss\n",
    "\n",
    "# 1) Load data and meta as before\n",
    "data_dir  = os.path.dirname(base_dir)\n",
    "train_ids = np.fromfile(os.path.join(data_dir, 'train.bin'), dtype=np.uint16)\n",
    "val_ids   = np.fromfile(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16)\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# 2) Compute data‐marginal q[v]\n",
    "counts = np.bincount(train_ids, minlength=vocab_size).astype(float)\n",
    "q = torch.tensor(counts / counts.sum(), dtype=torch.float32, device=device)  # [V]\n",
    "\n",
    "# 3) Dataset + DataLoader\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = torch.from_numpy(data).long()\n",
    "        self.block_size = block_size\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "block_size = 256\n",
    "train_loader = DataLoader(CharDataset(train_ids, block_size),\n",
    "                          batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(CharDataset(val_ids,   block_size),\n",
    "                          batch_size=16, shuffle=False, drop_last=True)\n",
    "virgin = ConvexGPT(vocab_size = vocab_size,embed_dim  = 64,depth  = 2,heads = 4,moe_petals = 16,creativity=True)\n",
    "\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in virgin.parameters()))\n",
    "model = torch.jit.script(virgin)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)#or adam, but i prefer the WOLF.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 6) Train / eval functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits ,attn_weights = model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        per_token_loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            yb.view(-1),\n",
    "            reduction='none'  # This gives the raw loss per token\n",
    "        ).reshape(B,T) # Shape: (B*S,)\n",
    "        \n",
    "        loss = per_token_loss.mean()\n",
    "        loss_cpu = per_token_loss.cpu().detach().numpy()\n",
    "        tokens = [[itos[idx] for idx in seq.tolist()] for seq in yb]\n",
    "        attn_cpu = attn_weights.cpu().detach().numpy()\n",
    "        update_framebuffer(attn_cpu, loss_cpu, loss.item(), tokens)\n",
    "        update_display()\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "    \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 7) Run training\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_epoch() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff956811ccd46878a47d6467a25fd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "CHAR_WIDTH = 8  # Font size 8 for token rendering\n",
    "CHAR_HEIGHT = 11\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "LOSS_BAR_HEIGHT = 32\n",
    "EWMA_HEIGHT = 32  # Increased to accommodate large text (previously 32)\n",
    "\n",
    "# Full-resolution framebuffer dimensions\n",
    "container_width = CHAR_WIDTH * SEQ_LEN  # 1024 pixels\n",
    "container_height = CHAR_HEIGHT * BATCH_SIZE  # 176 pixels\n",
    "total_height = container_height + LOSS_BAR_HEIGHT + EWMA_HEIGHT  # Adjusted for larger EWMA\n",
    "\n",
    "# Final scaled-down dimensions\n",
    "scaled_width = container_width   # 512 pixels\n",
    "scaled_height = total_height  # 170 pixels\n",
    "\n",
    "# Initialize framebuffer\n",
    "framebuffer = np.zeros((total_height, container_width, 3), dtype=np.uint8)\n",
    "\n",
    "# EWMA storage\n",
    "ticker_history = np.zeros(SEQ_LEN, dtype=np.float32)  # Stock ticker moving buffer\n",
    "loss_memory = 0.0\n",
    "# Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(\"DejaVuSansMono.ttf\", 8)  # Monospaced font\n",
    "    font_large = ImageFont.truetype(\"DejaVuSansMono.ttf\", 64)  # Large EWMA display\n",
    "except:\n",
    "    font = ImageFont.load_default()\n",
    "    font_large = font\n",
    "\n",
    "# --- Color Mapping Functions ---\n",
    "def get_flame_color(val):\n",
    "    \"\"\"Map a normalized value to a flame-like color.\"\"\"\n",
    "    return np.array([int(val * 255), int(val * 0.5 * 255), 0], dtype=np.uint8)\n",
    "\n",
    "# --- IPython Display Setup ---\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "def get_dynamic_color(attn_val, loss_val):\n",
    "    \"\"\"\n",
    "    Compute a dynamic color transition between flame orange (uncertain) and phosphor green (confident).\n",
    "\n",
    "    attn_val: Normalized attention value (0 to 1)\n",
    "    loss_val: Normalized loss value (0 to 1, inverted as 1 - loss)\n",
    "\n",
    "    Returns an RGB color as a NumPy array.\n",
    "    colors late in training will often be red. this is suggested to swap out for get_flame_color\n",
    "    but only on fine tuning on new data.\n",
    "    \"\"\"\n",
    "    certainty = 1 - loss_val  # High certainty = low loss\n",
    "\n",
    "    # Define RGB endpoints\n",
    "    orange = np.array([attn_val * 255, attn_val * 0.5 * 255, 0], dtype=np.uint8)   # Uncertain (High Loss)\n",
    "    green = np.array([attn_val * 0.5 * 255, attn_val * 255, attn_val * 0.25 * 255], dtype=np.uint8)  # Confident (Low Loss)\n",
    "\n",
    "    # Interpolate based on certainty (0 = uncertain/orange, 1 = confident/green)\n",
    "    color = (certainty * green) + ((1 - certainty) * orange)\n",
    "\n",
    "    return color.astype(np.uint8)\n",
    "def normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    min_val = np.min(x, axis=1, keepdims=True)\n",
    "    max_val = np.max(x, axis=1, keepdims=True)\n",
    "    scale = max_val - min_val\n",
    "    return (x - min_val) / (scale + 1e-16)\n",
    "    \n",
    "# --- Framebuffer Update Function ---\n",
    "def update_framebuffer(attn_weights, token_losses, current_loss, tokens):\n",
    "    token_losses = normalize_rows(token_losses)\n",
    "    attn_weights = normalize_rows(attn_weights)\n",
    "    \"\"\"Render the text grid with coloration based on attn * inverse loss.\"\"\"\n",
    "    global framebuffer, loss_history, ticker_history, loss_memory\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "\n",
    "    # Create image buffer\n",
    "    img = Image.new(\"RGB\", (container_width, total_height), (0, 0, 0))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Render text with colored intensity\n",
    "    char_positions = [\n",
    "        (col * CHAR_WIDTH, row * CHAR_HEIGHT + EWMA_HEIGHT + LOSS_BAR_HEIGHT, tokens[row][col])\n",
    "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
    "    ]\n",
    "    colors = [\n",
    "        tuple(get_dynamic_color(attn_weights[row, col], token_losses[row, col]))\n",
    "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
    "    ]\n",
    "    for (x, y, char), color in zip(char_positions, colors):\n",
    "        draw.text((x, y), char, font=font, fill=color)\n",
    "\n",
    "\n",
    "    etcerta = 0.367879441  # Constant used in update rule\n",
    "    et = 1 - etcerta\n",
    "    update = loss_memory * et + np.minimum(12, np.maximum(current_loss , 0)) * etcerta\n",
    "    loss_memory = loss_memory * et + update * etcerta\n",
    "    # --- EWMA Display (LARGE FONT) ---\n",
    "    ewma = loss_memory\n",
    "    ewma_text = f\"{ewma:.4f}\"\n",
    "    draw.text((container_width-128, 0), ewma_text, font_size=32, fill=(65,255, 125))\n",
    "\n",
    "    # --- Moving Loss Ticker Graph ---\n",
    "    ticker_history = np.roll(ticker_history, -1)  # Shift left\n",
    "    ticker_history[-1] = current_loss  # Insert new loss on the right\n",
    "\n",
    "    # Rescale ticker dynamically like a stock ticker (normalize to min-max range)\n",
    "    min_loss = np.min(ticker_history)\n",
    "    max_loss = np.max(ticker_history)\n",
    "    range_loss = max_loss - min_loss if max_loss != min_loss else 1\n",
    "    normalized_ticker = (ticker_history - min_loss) / range_loss\n",
    "\n",
    "    # Draw ticker graph line\n",
    "    # Optimized drawing loop (fewer function calls)\n",
    "    y_vals = EWMA_HEIGHT + (1 - normalized_ticker) * LOSS_BAR_HEIGHT\n",
    "    x_vals = np.arange(SEQ_LEN) * CHAR_WIDTH\n",
    "    for i in range(SEQ_LEN - 1):\n",
    "        draw.line([(x_vals[i], y_vals[i]), (x_vals[i + 1], y_vals[i + 1])], fill=(0, 255, 255), width=2)\n",
    "\n",
    "    framebuffer = np.array(img)\n",
    "\n",
    "# --- IPython Display Update Function ---\n",
    "def update_display():\n",
    "    \"\"\"Show the framebuffer, scaled down by half using ipywidgets.\"\"\"\n",
    "    img = Image.fromarray(framebuffer)\n",
    "    img_resized = img.resize((scaled_width, scaled_height), Image.LANCZOS)\n",
    "\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        display(img_resized)\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'shakespeare.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W508 07:16:24.495442000 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::bmm        17.25%     211.708ms        18.50%     227.064ms       2.703ms       1.36 Gb       1.36 Gb            84  \n",
      "                                         aten::softplus        11.14%     136.647ms        11.14%     136.647ms       1.980ms       2.69 Mb       2.69 Mb            69  \n",
      "                                              aten::mul         9.68%     118.760ms         9.73%     119.381ms     348.051us       2.25 Gb       2.25 Gb           343  \n",
      "                                              aten::exp         9.27%     113.739ms         9.27%     113.739ms       1.404ms       1.38 Gb       1.38 Gb            81  \n",
      "                                              aten::sum         9.05%     111.015ms         9.34%     114.596ms     830.409us     111.75 Mb     111.75 Mb           138  \n",
      "                                              aten::add         6.71%      82.367ms         6.73%      82.546ms     491.346us       1.75 Gb       1.75 Gb           168  \n",
      "                                              aten::div         5.64%      69.163ms         5.65%      69.342ms     343.278us     805.99 Mb     805.99 Mb           202  \n",
      "                                              aten::max         5.03%      61.766ms         5.03%      61.766ms       5.147ms     103.50 Mb     103.50 Mb            12  \n",
      "                                              aten::sub         4.51%      55.320ms         4.52%      55.506ms     668.745us       1.48 Gb       1.48 Gb            83  \n",
      "                                          aten::normal_         2.21%      27.083ms         2.21%      27.083ms       2.708ms           0 b           0 b            10  \n",
      "                                            aten::where         2.01%      24.635ms         2.01%      24.681ms     587.653us    -289.30 Mb    -289.30 Mb            42  \n",
      "                                        _FusedLogSumExp         1.81%      22.261ms        19.71%     241.848ms      20.154ms       1.11 Gb      -1.17 Gb            12  \n",
      "                                          <backward op>         1.69%      20.744ms        30.88%     378.914ms       6.422ms    -123.48 Mb      -1.24 Gb            59  \n",
      "                                                forward         1.67%      20.502ms        54.80%     672.472ms     672.472ms       1.80 Gb      -1.21 Gb             1  \n",
      "                                _FusedLogSumExpBackward         1.59%      19.557ms         8.14%      99.848ms      12.481ms       1.04 Gb      -1.04 Gb             8  \n",
      "autograd::engine::evaluate_function: _FusedLogSumExp...         1.25%      15.355ms         9.39%     115.203ms      14.400ms     -68.50 Mb      -1.10 Gb             8  \n",
      "                                           aten::select         1.17%      14.332ms         1.25%      15.286ms       2.299us           0 b           0 b          6650  \n",
      "autograd::engine::evaluate_function: torch::jit::(an...         1.13%      13.807ms        32.09%     393.750ms       6.674ms    -886.27 Mb    -768.79 Mb            59  \n",
      "                                               aten::gt         0.99%      12.116ms         1.00%      12.331ms     293.601us    -211.51 Mb    -211.51 Mb            42  \n",
      "autograd::engine::evaluate_function: ExpandBackward0...         0.83%      10.245ms         2.32%      28.473ms       2.373ms    -900.00 Mb    -960.00 Mb            12  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.227s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.profiler\n",
    "device=\"cpu\"\n",
    "def profile_one_iter():\n",
    "    model.train()\n",
    "    xb, yb = next(iter(train_loader))\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logdir'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=20))\n",
    "profile_one_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9GiSvYAnJnv",
    "outputId": "d77d34f7-55ad-4f2b-cac8-90dd2159d95f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  To be, or not to be,\n",
      "Yf n adso ay thed ie by but nyo nca bed'susext f nagofouk,\n",
      "T:\n",
      "\n",
      "T:\n",
      "ANENCENRYOF?\n",
      "AS:\n",
      "GLERICARINCENTENGous ad cow,\n",
      "W:\n",
      "OTENI n ard hil,\n",
      "apelmffrot wal lay anbad\n",
      "Froucorit tu mes dll T:\n",
      "OMPSTHENTRDULOLLADUDUSTENDUCINENCENCIARIRI arow urm y!\n",
      "STHENENALO!\n",
      "Asorsherccorestouy, bus e obof he ORERI\n",
      "ONCENTUTro o, dinda uchalllomsme himerou ne nlse brof omo tounde, asutaceseshus TERI:\n",
      "OFatll,antuistuto uapyow'sble haroledlly ay botheleshet t sknd nin t me m fowokst Has ing TRUERI fo omay to hatrot t;ss ICKENTENTee ay,\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "def fenchel_decode(logits, tau=1.0, iters=3):\n",
    "    \"\"\"Fenchel‑dual KL‑regularised projection of -logits (energy).\"\"\"\n",
    "    energy = -logits                        # (B,V)\n",
    "    p = torch.full_like(energy, 1.0 / energy.size(-1))  # uniform start\n",
    "    for _ in range(iters):\n",
    "        p = torch.softmax((-energy / tau) + p.log(), dim=-1)\n",
    "    return p\n",
    "    \n",
    "# --- generation ------------------------------------------------------\n",
    "use_fenchel   = False          # flip to compare\n",
    "tau           = 1.0           # λ  (temperature analogue)\n",
    "max_new_tokens = 512\n",
    "top_k          = 25\n",
    "block_size     = 256\n",
    "temperature    = 1.0\n",
    "\n",
    "bcontext_str = \"To be, or not to be,\"\n",
    "context_ids = torch.tensor([[ stoi[c] for c in bcontext_str ]],\n",
    "                           dtype=torch.long)\n",
    "context_ids = context_ids.to(device)\n",
    "\n",
    "\n",
    "generated = context_ids.clone()  # (1,T0)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for _ in range(max_new_tokens):\n",
    "    input_ids = generated[:, -block_size:]        # casual block\n",
    "    logits = model(input_ids)[:, -1, :] / temperature\n",
    "    # top‑k mask\n",
    "    if top_k is not None:\n",
    "        v, _ = torch.topk(logits, top_k)\n",
    "        logits[logits < v[:, [-1]]] = -1e10\n",
    "\n",
    "    if use_fenchel:\n",
    "        probs = fenchel_decode(logits, tau=tau, iters=3)\n",
    "    else:\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    next_id = torch.multinomial(probs, num_samples=1)   # (1,1)\n",
    "    generated = torch.cat([generated, next_id], dim=1)\n",
    "\n",
    "print('> ', ''.join(itos[i] for i in generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x33262b0b0>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
