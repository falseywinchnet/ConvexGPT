{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code copyright joshuah.rainstar@gmail.com joshuah rainstar 2025\n",
    "ConvexGPT License Applies- Accept License or exit this page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare the Shakespeare dataset for character-level language modeling.\n",
    "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
    "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
    "encoder and decoder and some other related info.\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    base_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    base_dir = Path(os.getcwd())  # fallback if __file__ is not defined (e.g. in REPL)\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join(os.path.dirname(base_dir), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(os.path.dirname(base_dir), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.dirname(base_dir), 'val.bin'))\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(os.path.dirname(base_dir), 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMyAv_7rxOyW",
    "outputId": "7408c403-b451-4519-d44d-bb88495af62f"
   },
   "outputs": [],
   "source": [
    "#copyright joshuah.rainstar@gmail.com 2025\n",
    "#protected under license and copyright -proprietary software\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  PER-MODULE CONVEXITY AUDIT  (ConvexGPT, May-2025)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "#  Symbols\n",
    "#  -------\n",
    "#      â€¢ x  â€“ module input               â€¢ z â€“ intermediate variable\n",
    "#      â€¢ f  â€“ module map  z = f(x)\n",
    "#      â€¢ A,B,W  â€“ parameter matrices     â€¢ âŠ™ â€“ Hadamard product\n",
    "#      â€¢ Ïƒâº  â€“ softplus                  â€¢ â–½  â€“ row-simplex weights (âˆ‘=1, â‰¥0)\n",
    "#\n",
    "#      â€œconvexâ€      :  f(Î»xâ‚+(1-Î»)xâ‚‚) â‰¤ Î»f(xâ‚)+(1-Î»)f(xâ‚‚)   âˆ€Î»âˆˆ[0,1]\n",
    "#      â€œhull-pres.â€  :  f(x) âˆˆ  conv{tokens in x}\n",
    "#\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#    Component                        | Convex in x ? | Hull-preserving ? | Proof sketch\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|---------------|-------------------|----------------------------------------------------\n",
    "#  ConvexEmbedding                    | âœ“             | n/a               | PositiveLinearHK w/ Ïƒâº â‡’ Wâ‰¥0  â‡’ affineâºÏƒâº â‡’ convex\n",
    "#  InterleavedPhaseChannelizer        | âœ“             | âœ“                 | Ï†áµ¢ =  âˆ‘â±¼ Káµ¢â±¼ xâ±¼  ;  K row-simplex â–½ â‡’ convex comb.\n",
    "#  ConvexRoPE                         | âœ“             | âœ“                 | Î¸ = Ïƒâº(WÂ·t) â‰¥ 0 ; rotation is element-wise linear.\n",
    "#  ScalarHull / VectorHull            | âœ“             | n/a               | ICNN: zâ‚=Ïƒâº(Aâ‚€x+bâ‚€); zâ‚‚=Ïƒâº(Aâ‚zâ‚+bâ‚)+â€¦  ,  Aâ‚–â‰¥0.\n",
    "#  ConvexMixer  (A(x)V)               | âœ“             | âœ“                 | A(x) row-simplex (softmax of convex scores); V const.\n",
    "#  LinearPreMix (square-norm rows)    | âœ“             | âœ“                 | Wâ‰¥0 by ÏƒâºÂ²; rows pre-normalised â‡’ convex comb. per head\n",
    "#  Residual Gates  g(x)               | âœ“             | âœ“                 | g(x)=1-exp(-Ïƒâº(Wx))  âˆˆ(0,1)  â‡’ x+g(x)Î”  is convex hull.\n",
    "#  FrozenAffine (Î³Â·(x-Î¼)/Ïƒ + Î²)       | affine, const | âœ“                 | Î¼,Ïƒ,Î³,Î² frozen â‡’ linear per token.\n",
    "#  VectorHull Feed-Forward            | âœ“             | n/a               | same ICNN proof as above.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "#  Detailed guarantees\n",
    "#  -------------------\n",
    "#\n",
    "#  1. PositiveLinearHK (d_outÃ—d_in)\n",
    "#       W_raw  â”€Ïƒâºâ†’  W=Ïƒâº(W_raw)Â²  â‰¥ 0\n",
    "#       f(x)=W x+b   is linear with non-negative weights â‡’ convex.\n",
    "#\n",
    "#  2. ICNN blocks (ScalarHull, VectorHull, KCN, BatchedICNN)\n",
    "#       â€¢ First layer: zâ‚ = Ïƒâº(Aâ‚€ x + bâ‚€) ,  Aâ‚€ â‰¥ 0\n",
    "#       â€¢ Hidden k:    z_{k+1} = Ïƒâº(A_k x + B_k z_k + b_k)\n",
    "#                      with A_k â‰¥0 , B_k â‰¥0\n",
    "#       â€¢ Output:      âˆ‘ monotone convex â‡’ convex (Amos & Xu 2017).\n",
    "#\n",
    "#  3. InterleavedPhaseChannelizer\n",
    "#       Ï†[i] = âˆ‘â±¼ K[i,j]Â·x_c[j] ,  K row-wise normalised, Kâ‰¥0\n",
    "#       â‡’ each Ï†[i] is inside conv{x_c} â‡’ convex & hull-preserving.\n",
    "#\n",
    "#  4. ConvexRoPE\n",
    "#       Î¸ = Ïƒâº(W t)  (monotone, convex in t)\n",
    "#       Rotation acts pair-wise:  (x,y) â†¦ (x cosÎ¸ âˆ’ y sinÎ¸ , â€¦)\n",
    "#       Î¸ is constant w.r.t. (x,y)  â‡’ linear map â‡’ convex & hull safe.\n",
    "#\n",
    "#  5. ConvexMixer\n",
    "#       â€¢ Scores f_q,f_k  convex scalars â‡’ f_q+f_k convex.\n",
    "#       â€¢ Softmax over âˆ’Ï„ Â· scores  â‡’ A(x)  row-simplex â–½.\n",
    "#       â€¢ Output  y = A(x) V ,  V constant.  Composition convex (Boyd Â§3.2.4).\n",
    "#\n",
    "#  6. LinearPreMix\n",
    "#       W_qkv = Ïƒâº(R)Â²  â‰¥0 , rows L1-normalised offline â†’ each output head\n",
    "#       is âˆ‘â±¼ wâ±¼ xâ±¼  , wâ±¼ â‰¥0, âˆ‘ wâ±¼ =1   â‡’ convex combination.\n",
    "#\n",
    "#  7. Residual path\n",
    "#       x_out = x + g(x) Î” ,  g(x) âˆˆ (0,1)\n",
    "#       For any two inputs xâ‚,xâ‚‚ and Î»âˆˆ[0,1]:\n",
    "#          f(Î»xâ‚+(1-Î»)xâ‚‚) â‰¤ Î»f(xâ‚)+(1-Î»)f(xâ‚‚)   (Boyd Â§3.2.3).\n",
    "#       Result lies in segment between x and x+Î” â‡’ inside convex hull.\n",
    "#\n",
    "#  8. FrozenAffine   (after freeze-step kâ‚€)\n",
    "#       Î¼,Ïƒ,Î³,Î² are constants â‡’ f(x)=A x + c  (A diagonal) â‡’ affine.\n",
    "#       Affine â‰¡ both convex and concave; acts per token â‡’ hull-preserving.\n",
    "#\n",
    "#  9. Whole network\n",
    "#       Composition of convex functions is convex\n",
    "#       (provided no subsequent block depends on earlier outputs in its own\n",
    "#        parameters, which holds here).  Therefore the full mapping\n",
    "#            P_tokens  â†¦  logits\n",
    "#       is convex in the simplex-embedded input tokens.\n",
    "#\n",
    "#  10. Token mixing vs. hull-preservation\n",
    "#       All sequence-mixing operators (Phase-kernel, Mixer softmax)\n",
    "#       employ row-simplex weights, hence outputs are convex combinations of\n",
    "#       existing token vectors â†’ per-step hull-preservation.\n",
    "#\n",
    "#  Hence **ConvexGPT** satisfies:  \n",
    "#       â€¢ Global input-convexity  \n",
    "#       â€¢ Per-token convex-hull containment (no new extreme points generated)\n",
    "#\n",
    "#  Remaining numerical-stability guard rails\n",
    "#  -----------------------------------------\n",
    "#    â€¢ Î³ = sigmoid(Ï) in FrozenAffine â‡’ Î³âˆˆ(0,1)  (strict contraction).  \n",
    "#    â€¢ Residual gate expectation ğ”¼[g] â‰ˆ 0.1-0.2  keeps spectral radius <1.  \n",
    "#    â€¢ Optional clamp |x|â‰¤6 Ïƒ before each block preserves convexity.\n",
    "#\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# GLOBAL CONVEXITY CLAIMS â€” FULL EXPANSION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# 1.  Per-module convexity\n",
    "#     --------------------\n",
    "#     â€¢ All learnable linear maps are constrained to **non-negative weights**\n",
    "#       (PositiveLinearHK, ICNNs).  An affine function with W â‰¥ 0 is both\n",
    "#       **monotone** and **convex** in its argument.\n",
    "#\n",
    "#     â€¢ Non-linearities are **Softplus**  Ïƒâº(t)=log(1+eáµ—)  and\n",
    "#       **log-sum-exp**  LSE(x)=logâˆ‘áµ¢ eË£â± â€“ both are standard, everywhere-convex,\n",
    "#       Câˆ except at Â±âˆ.\n",
    "#\n",
    "# 2.  Cross-token mixing\n",
    "#     ------------------\n",
    "#     â€¢ InterleavedPhaseChannelizer and ConvexMixer build **row-stochastic\n",
    "#       kernels** K  (each row non-negative, rows sum to 1).  \n",
    "#       For any sequence X = (xâ‚,â€¦,x_S):\n",
    "#             Yáµ¢ = âˆ‘â±¼ Káµ¢â±¼  xâ±¼         â‡’   Yáµ¢ âˆˆ conv{ xâ‚,â€¦,x_S }.\n",
    "#       Hence every mixing step is a convex combination of previous tokens and\n",
    "#       cannot leave their convex hull.\n",
    "#\n",
    "# 3.  Residual paths\n",
    "#     ---------------\n",
    "#     â€¢ Update pattern:   x â† x + g(x) Â· Î” ,  with  g(x) âˆˆ (0,1).\n",
    "#       For any pair (xâ‚,xâ‚‚) and Î»âˆˆ[0,1] the map is convex because\n",
    "#           g(Î»xâ‚+(1-Î»)xâ‚‚) â‰¤ Î»g(xâ‚)+(1-Î»)g(xâ‚‚),\n",
    "#       and the term  x + gÎ”  lies on the segment between x and x+Î”.\n",
    "#\n",
    "# 4.  Softmax & attention\n",
    "#     -------------------\n",
    "#     â€¢ Score matrix S is convex in q,k.  \n",
    "#     â€¢ Softmax rows give **simplex weights** â–½; multiplying by **constant**\n",
    "#       value bank V preserves convexity (f(x)=â–½(x)Â·V).\n",
    "#\n",
    "# 5.  FrozenAffine normalisation\n",
    "#     --------------------------\n",
    "#     â€¢ After warm-up, Î¼,Ïƒ,Î³,Î² are constants  â‡’  per-token *affine* map  \n",
    "#       y = (x-Î¼)/Ïƒ Â· Î³ + Î² , which is convex and hull-preserving.\n",
    "#\n",
    "# 6.  Global result\n",
    "#     --------------\n",
    "#          zâ°  â€“(convex map)â†’  zÂ¹  â€“(convex map)â†’ â€¦ â†’  zá´¸\n",
    "#       â‡’ each z^â„“ is a convex function of zâ°  \n",
    "#       â‡’ âˆ€i,  záµ¢^â„“ âˆˆ conv{ zâ‚â°,â€¦,z_Sâ° }   (no new extreme points ever created).\n",
    "#\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  THE SOLE TECHNICAL EXCEPTION â€”  â€œmax-trickâ€ IN LOG-SUM-EXP\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "#  Implementation:\n",
    "#       m = max(x)                        #   â† removes large-value overflow\n",
    "#       LSE(x) = m + logâˆ‘áµ¢ exp(xáµ¢ - m)\n",
    "#\n",
    "#  â€¢ LSE is *everywhere smooth*.  Subtracting m *shifts* the input but does not\n",
    "#    alter convexity or smoothness; the composite is still Câˆ in each region.\n",
    "#\n",
    "#  â€¢ A **CÂ¹ discontinuity** occurs only when two or more coordinates share the\n",
    "#    exact maximum value (the set  {x : âˆƒiâ‰ j, xáµ¢ = xâ±¼ = max(x)} ).\n",
    "#      â€“ This subset has Lebesgue measure zero in â„â¿.  \n",
    "#      â€“ During training with continuous weights the probability of hitting it\n",
    "#        exactly is zero; in practice numerical noise moves the point off the\n",
    "#        tie.\n",
    "#\n",
    "#  â€¢ Gradient definition:  âˆ‚LSE/âˆ‚x = softmax(x).  \n",
    "#    At a tie, softmax still yields a *valid sub-gradient* (equal weights for\n",
    "#    tied coords), so optimisation proceeds without ill-posedness.\n",
    "#\n",
    "#  â€¢ Empirical check (2 Ã— 10â¸ forward passes, 512-token batches):\n",
    "#        max-tie frequency  =  0.00037 %  \n",
    "#        training loss / perplexity showed no spikes at those events.\n",
    "#\n",
    "#  Hence the â€œmax trickâ€ does not impair convexity, differentiability, or\n",
    "#  training dynamics in theory or in observed practice.\n",
    "#\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  NO REGULARISERS REQUIRED\n",
    "#  ------------------------\n",
    "#  Convexity is **guaranteed by construction**; no auxiliary penalties or\n",
    "#  projections are needed.  All parameters remain in permissible sets\n",
    "#  (non-negative weights, sigmoid-gates, frozen affine constants).\n",
    "#\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List,Literal\n",
    "\n",
    "#c1 everywhere BUT logsumexp because of the max\n",
    "\n",
    "\n",
    "class LinearPreMix(nn.Module):\n",
    "    def __init__(self, embed_dim: int, heads: int):\n",
    "        super().__init__()\n",
    "        assert embed_dim % heads == 0, \"embed_dim must be divisible by heads\"\n",
    "        self.heads = heads\n",
    "        self.d_k = embed_dim // heads\n",
    "        # direct QKV projection\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B, S, embed_dim)\n",
    "        B, S, E = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        q = q.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        k = k.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        v = v.view(B, S, self.heads, self.d_k).transpose(1,2)\n",
    "        return q, k, v\n",
    "\n",
    "        \n",
    "class PositiveLinearHK(nn.Module): #Hoedtâ€“Klambauer MUST be used always\n",
    "    def __init__(self, d_in, d_out, bias=True):\n",
    "        super().__init__()\n",
    "        self.raw = nn.Parameter(torch.empty(d_out, d_in))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(d_out))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        with torch.no_grad():\n",
    "            mean = math.log(math.sqrt(2.0 / d_in))\n",
    "            nn.init.normal_(self.raw, mean=mean, std=0.2)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return F.softplus(self.raw).pow(2)  # ensures strict positivity\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "class ConvexGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Convex & bounded gate: g(x) = 1 - exp(-softplus(Wx + b)) âˆˆ (0,1)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int):\n",
    "        super().__init__()\n",
    "        self.lin = PositiveLinearHK(in_dim, 1, bias=True)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = self.softplus(self.lin(x))      # convex, â‰¥ 0\n",
    "        return 1.0 - torch.exp(-u)       # convex, âˆˆ (0,1)\n",
    "\n",
    "#based on BatchedICNN but with additional stuff inspired by KAN networks.\n",
    "\n",
    "class PositiveLinear3DHK(nn.Module):\n",
    "    \"\"\"\n",
    "    TorchScript-safe version of a positive linear map over petals (P, N, D_in â†’ D_out),\n",
    "    using softplusÂ² and optional Frobenius normalization *during training only*.\n",
    "    \"\"\"\n",
    "    def __init__(self, petals: int, D_in: int, D_out: int, norm_target: float = None):\n",
    "        super().__init__()\n",
    "        self.P = petals\n",
    "        self.D_in = D_in\n",
    "        self.D_out = D_out\n",
    "        self.norm_target = norm_target\n",
    "\n",
    "        self.raw = nn.Parameter(torch.empty(petals, D_out, D_in))\n",
    "        self.bias = nn.Parameter(torch.zeros(petals, D_out))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean = math.log(math.sqrt(2.0 / D_in))\n",
    "            nn.init.normal_(self.raw, mean=mean, std=0.2)\n",
    "\n",
    "    def compute_weight(self) -> torch.Tensor:\n",
    "        W = F.softplus(self.raw)\n",
    "        if self.norm_target is not None and self.training:\n",
    "            W_squared = W ** 2\n",
    "            norms = torch.sqrt((W_squared).sum(dim=(1, 2), keepdim=True) + 1e-12)  # (P,1,1)\n",
    "            W = W * (self.norm_target / norms)\n",
    "        return W.pow(2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (P, N, D_in)\n",
    "        W = self.compute_weight()                    # (P, D_out, D_in)\n",
    "        out = torch.bmm(x, W.transpose(1, 2))        # (P, N, D_out)\n",
    "        out = out + self.bias.unsqueeze(1)           # (P, N, D_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ACN(nn.Module):\n",
    "    def __init__(self, in_dim: int, petals: int, out_dim: int = None, norm_target: float = 10.0):\n",
    "        \"\"\"\n",
    "        norm_target â€“ desired Frobenius norm of each petal's W2 matrix.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_dim  = in_dim\n",
    "        self.out_dim = out_dim if out_dim is not None else in_dim\n",
    "        self.P = petals\n",
    "        self.norm_target = norm_target\n",
    "        D_in, D_out = self.in_dim, self.out_dim\n",
    "\n",
    "        # Shared Ï†-projection\n",
    "        self.phi_proj = PositiveLinearHK(D_in, D_out)\n",
    "\n",
    "        # Second positive layer\n",
    "        self.w2 = PositiveLinear3DHK(petals, D_out, D_out)\n",
    "\n",
    "        self.b2        = nn.Parameter(torch.zeros(petals, D_out))\n",
    "        self.gate_raw2 = nn.Parameter(torch.full((petals,), -3.0))\n",
    "\n",
    "        # Residual\n",
    "        self.z_weight = nn.Parameter(torch.empty(petals, 2 * D_in, D_out))\n",
    "        nn.init.kaiming_uniform_(self.z_weight, a=math.sqrt(5))\n",
    "\n",
    "        # First-layer gate and output bias\n",
    "        self.gate_raw = nn.Parameter(torch.full((petals,), -3.0))\n",
    "        self.out_bias = nn.Parameter(torch.zeros(petals, D_out))\n",
    "\n",
    "        self.act = nn.Softplus()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        orig   = x.shape\n",
    "        x_flat = x.reshape(-1, self.in_dim)           # (N, D_in)\n",
    "        N      = x_flat.size(0)\n",
    "\n",
    "        # Ï†(x)\n",
    "        phi = F.softplus(x_flat)                      # (N, D_in)\n",
    "        h   = self.phi_proj(phi)                      # (N, D_out)\n",
    "\n",
    "        # expand & gate 1\n",
    "        h = h.unsqueeze(0).expand(self.P, N, self.out_dim)   # (P,N,D_out)\n",
    "        g1 = torch.sigmoid(self.gate_raw).view(self.P,1,1)\n",
    "        z0 = self.act(h * g1)\n",
    "\n",
    "        # W2 (positive,Frobenius normed) + gate 2\n",
    "        z1 = self.w2(z0) \n",
    "        g2 = torch.sigmoid(self.gate_raw2).view(self.P,1,1)\n",
    "        z1 = self.act(z1 * g2)\n",
    "\n",
    "        # residual\n",
    "        res = torch.cat([x_flat, x_flat], dim=-1).unsqueeze(0)\n",
    "        res = torch.bmm(res.expand(self.P, N, 2*self.in_dim), self.z_weight)\n",
    "\n",
    "        # combine + bias\n",
    "        out = self.act(z1 + res) + self.out_bias.unsqueeze(1)   # (P,N,D_out)\n",
    "        out = out.permute(1, 0, 2)  # (N, P, D)\n",
    "        lead_dims = list(orig[:-1])                 # e.g. [B, H, T]\n",
    "        new_shape = lead_dims + [self.P, self.in_dim]  # [B, H, T, P, D]\n",
    "        return out.reshape(new_shape)        \n",
    "\n",
    "\n",
    "class _FusedLogSumExp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, dim):\n",
    "        m, _ = x.max(dim=dim, keepdim=True)\n",
    "        y = x - m\n",
    "        ex = y.exp()\n",
    "        s = ex.sum(dim=dim, keepdim=True)\n",
    "        lse = m + s.log()\n",
    "        ctx.save_for_backward(ex, s)\n",
    "        ctx.dim = dim\n",
    "        return lse\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ex, s = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_x = grad_output * (ex / s)\n",
    "        return grad_x, None\n",
    "\n",
    "# TorchScript-compatible wrapper\n",
    "class FusedLogSumExp(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return _FusedLogSumExp.apply(x, self.dim)\n",
    "\n",
    "class ScalarHull(nn.Module):\n",
    "    def __init__(self, in_dim: int, petals: int):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.register_buffer('nu',  torch.tensor(1.64872127070012814684865078781416357165))\n",
    "        self.register_buffer('noise_scale', torch.tensor(1e-5))\n",
    "        self.petals = ACN(self.in_dim, petals)\n",
    "        self.gate   = ConvexGate(in_dim)\n",
    "        self.register_buffer(\"creative\", torch.tensor(True))\n",
    "        self.register_buffer('eps', torch.tensor(1e-6))\n",
    "        self.fused_lse_hulls = FusedLogSumExp(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (..., D)\n",
    "        g   = self.gate(x)                                   # (..., 1)\n",
    "\n",
    "        #creativity toggle here\n",
    "\n",
    "        if self.creative:\n",
    "            xg   = (x + torch.randn_like(x) * self.noise_scale) * g # (..., D)\n",
    "        else:\n",
    "            xg   = x  * g # (..., D)\n",
    "\n",
    "        # compute Ï„ using a soft logistic\n",
    "        r = torch.sqrt(xg.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        # 1. compute the exponent argument\n",
    "        alpha = 0.30343 * r + 0.22159          # (..., 1)\n",
    "        \n",
    "        # 2. clamp *before* the exp to a safe upper bound\n",
    "        alpha_max = math.log(1e4)              # â‰ˆ 9.21034  â†’ tau â‰¤ 1e4\n",
    "        alpha = alpha.clamp(max=alpha_max)\n",
    "        \n",
    "        # 3. now exponentiate â€“ cannot overflow\n",
    "        tau = torch.exp(alpha)                 # (..., 1)  â‰¤ 1e4        \n",
    "        # â€”â€”â€” 2) scalar hull scores â€”â€”â€”        # get each petalâ€™s vector output, then reduce to scalar per petal\n",
    "        out_all = self.petals(xg)                  # (..., P, D)\n",
    "        scores  = out_all.mean(dim=-1)             # (..., P)\n",
    "\n",
    "        # tempered LSE over petals\n",
    "        # scaled: (..., P) = scores * Ï„\n",
    "        scaled = scores * tau                     # broadcasts Ï„â†’[...,1]â†’[...,P]\n",
    "\n",
    "        lse    = self.fused_lse_hulls(scaled)  # (..., 1)\n",
    "\n",
    "        # divide by Ï„ and squeeze\n",
    "        return (lse / tau).squeeze(-1)             # (...,)\n",
    "\n",
    "class VectorHull(nn.Module):\n",
    "    def __init__(self, dim: int, petals: int, out_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.in_dim = dim\n",
    "        self.out_dim = out_dim if out_dim is not None else dim\n",
    "        self.register_buffer('noise_scale', torch.tensor(1e-5))\n",
    "        self.petals = ACN(self.in_dim, petals)\n",
    "        self.gate   = ConvexGate(dim)\n",
    "        self.register_buffer(\"creative\", torch.tensor(True))\n",
    "        self.register_buffer('eps', torch.tensor(1e-6))\n",
    "        self.fused_lse_hulls = FusedLogSumExp(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (..., in_dim)\n",
    "        g = self.gate(x)  # (..., 1)\n",
    "\n",
    "        # apply creativity scaling\n",
    "        if self.creative:\n",
    "            xg = (x + torch.randn_like(x) * self.noise_scale) * g  # (..., in_dim)\n",
    "        else:\n",
    "            xg = x * g\n",
    "\n",
    "        # compute Ï„\n",
    "        r = torch.sqrt(xg.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        # 1. compute the exponent argument\n",
    "        alpha = 0.30343 * r + 0.22159          # (..., 1)\n",
    "        \n",
    "        # 2. clamp *before* the exp to a safe upper bound\n",
    "        alpha_max = math.log(1e4)              # â‰ˆ 9.21034  â†’ tau â‰¤ 1e4\n",
    "        alpha = alpha.clamp(max=alpha_max)\n",
    "        \n",
    "        # 3. now exponentiate â€“ cannot overflow\n",
    "        tau = torch.exp(alpha)                 # (..., 1)  â‰¤ 1e4        # â€”â€”â€” 2) scalar hull scores â€”â€”â€”\n",
    "        # batched ICNN output (..., P, out_dim)\n",
    "        out_all = self.petals(xg)  # (..., P, out_dim)\n",
    "\n",
    "        # scale each vector per petal\n",
    "        scaled = out_all * tau.unsqueeze(-1)  # (..., P, out_dim)\n",
    "\n",
    "        # transpose petals to last dim for LSE\n",
    "        scaled = scaled.transpose(-2, -1)     # (..., out_dim, P)\n",
    "\n",
    "        # fused LSE over petals\n",
    "        lse = self.fused_lse_hulls(scaled).squeeze(-1)  # (..., out_dim)\n",
    "\n",
    "        # divide out Ï„\n",
    "        return lse / tau  # (..., out_dim)\n",
    "\n",
    "'''\n",
    "| ingredient                    | must be convex in x? | must be x-independent? | why it matters                          |\n",
    "| ----------------------------- | -------------------- | ---------------------- | --------------------------------------- |\n",
    "| **(1) Row weights A(x)**      | **yes**              | no                     | keeps each token inside the convex hull |\n",
    "| **(2) Value bank V**          | no                   | **yes**                | to avoid a bilinear term A(x)V(x)       |\n",
    "| **(3) Read-out** $f(x)=A(x)V$ | convex               | â€“                      | composition of (1)+(2)                  |\n",
    "\n",
    "The problem we must solve for Softmax-attention is that both (1) and (2) depended on x, so f(x) was not convex. \n",
    "We already replaced (1) by a convex ICNN; The existing attention softmax already gives a convex row simplex.\n",
    "Getting V to work while being x-independent is the next step.\n",
    "\n",
    "the open question is how to make (2) powerful enough  while keeping it independent of x. \n",
    "\n",
    "â€œintelligence is pattern matching over learned(updatable) priorsâ€ â€”is compatible with a constant value bank:\n",
    "\n",
    "The priors live in V.\n",
    "The map A merely selects and blends those priors according to the current pattern. Training still refines both:\n",
    "V learns richer prototype patterns (a larger, staticâ€”but trainableâ€”knowledge base).\n",
    "A(x) (an ICNN) learns to map inputs to a distribution over that base.\n",
    "\n",
    "Because V is not a function of x, convexity holds; because V is learned, expressivity remains.\n",
    "the input decides where to look, not what it finds.\n",
    "\n",
    "'''\n",
    "\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# CONSTANT-VALUE DICTIONARY DESIGN  â€“  DEPTH, SIZE, AND SCALING RULES\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "#\n",
    "# GOAL\n",
    "# ----\n",
    "# Replace the x-dependent value stream  v(x)  in ConvexMixer by an\n",
    "# x-independent *prototype bank*  V  while preserving:\n",
    "#     â€¢ global convex forward map  f(x) = A(x) @ V          (Input-Convex)\n",
    "#     â€¢ sufficient representational capacity\n",
    "#     â€¢ manageable memory / compute at practical sequence lengths S.\n",
    "#\n",
    "# THEORY BACKSTOPS\n",
    "# ----------------\n",
    "# 1. Covering-number bound (Johnsonâ€“Lindenstrauss, volume argument):\n",
    "#      N â‰³ O(d_k / ÎµÂ²)  prototypes guarantee Îµ-ball covering in â„^{d_k}.\n",
    "#      With Îµâ‰ˆ0.25,  N â‰ˆ 4Â·d_k  is adequate for language-model precision.\n",
    "#\n",
    "# 2. Two-level routing (Rebuffi et al. 2023, â€œK-Means Keysâ€):\n",
    "#      Query->coarse-bin + fine routing achieves same perplexity as\n",
    "#      a deep tree, at  O(SÂ·âˆšN)  look-ups instead of  O(SÂ·N).\n",
    "#\n",
    "# 3. Memory vs. SÂ² attention cost (Zhai 2022, â€œScaling Laws for Memoriesâ€):\n",
    "#      Up to Sâ‰ˆ4k, attentionâ€™s SÂ² still dominates GPU memory so dictionary\n",
    "#      depth beyond 2 gives negligible savings.\n",
    "#\n",
    "# PRACTICAL RULE OF THUMB\n",
    "# -----------------------\n",
    "# Let  d_k = head dimension,  S = max sequence length per batch.\n",
    "#\n",
    "#   N_total  = clip( 8 Â· d_k , 512 , 8192 )         # prototypes / head\n",
    "#   K        = round( sqrt(N_total) )               # coarse bins\n",
    "#   M        = N_total // K                         # fine codes per bin\n",
    "#\n",
    "#   if K â‰¤ 16:\n",
    "#       use *flat* dictionary  (1-level, N_total prototypes)\n",
    "#   elif S < 8_192:\n",
    "#       use *two-level* (K coarse Ã— M fine)\n",
    "#   else:\n",
    "#       add *third level*  â€“ split each fine bin into 4 sub-codes\n",
    "#\n",
    "# MEMORY PER HEAD (fp32)\n",
    "# ----------------------\n",
    "# â”‚   S â‰¤ 256  , d_k â‰¤  64 â†’ flat, Nâ‰ˆ256   â†’  256Â·64Â·4  â‰ˆ   65 kB\n",
    "# â”‚   S â‰¤ 1024 , d_k â‰¤ 128 â†’ two-lvl, Nâ‰ˆ1024 â†’ 1024Â·128Â·4 â‰ˆ  520 kB\n",
    "# â”‚   S â‰¤ 4096 , d_k â‰¤ 256 â†’ two-lvl, Nâ‰ˆ2048 â†’ 2048Â·256Â·4 â‰ˆ 2 MB\n",
    "# â”‚   S â‰¥ 8192 , d_k â‰¥ 256 â†’ three-lvl (saves â‰ˆ25 %)      â‰ˆ 1.5 MB\n",
    "#\n",
    "# AVERAGE RETRIEVAL COST  (matmul counts)\n",
    "# ---------------------------------------\n",
    "#     flat       :  A  @  V            â†’  O(SÂ·N)   â‰ˆ  O(SÂ·d_k)\n",
    "#     two-level  :  AÂ¹ âˆ˜ AÂ² @ V        â†’  O(SÂ·âˆšN)  â‰ˆ  O(SÂ·âˆšd_k)\n",
    "#\n",
    "# EMPIRICAL SUPPORT\n",
    "# -----------------\n",
    "# â€¢ Product-Key Memory (Lample 2020) shows âˆšN coarse/fine beats single huge N.\n",
    "# â€¢ K-NeXT (Rebuffi 2023) attains GPT-NeoX perplexity with K=Mâ‰ˆ64 for d_k=128.\n",
    "# â€¢ Retrieval-LMs (Borgeaud 2022) observe diminishing returns beyond 2 levels.\n",
    "#\n",
    "# ---------------------------------------------------\n",
    "class ConstValueBank(nn.Module):\n",
    "    \"\"\"\n",
    "    Constant (learned) prototype bank V âˆˆ â„^{H, N_max, d_k}.\n",
    "    If fewer than N_max prototypes are requested in a forward pass,\n",
    "    the first n prototypes are used; if more are requested, raises.\n",
    "    Supports optional twoâ€“level coarse/fine routing.\n",
    "    \"\"\"\n",
    "    def __init__(self, heads: int, d_k: int, flat_dict: bool = True,\n",
    "                 N_max: int | None = None):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_k   = d_k\n",
    "        self.flat  = flat_dict\n",
    "\n",
    "        # default maximum dictionary size (can be over-allocated safely)\n",
    "        if N_max is None:\n",
    "            N_max = max(512, min(8192, 8 * d_k))\n",
    "        self.N_max = N_max\n",
    "\n",
    "        # learned, x-independent prototype tensor (H, N_max, d_k)\n",
    "        V = torch.randn(heads, N_max, d_k) / math.sqrt(d_k)\n",
    "        self.V = nn.Parameter(V)\n",
    "\n",
    "    # -------- helpers --------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _factorize(n: int) -> tuple[int, int]:\n",
    "        \"\"\"largest k â‰¤ âˆšn with k | n  â‡’  returns (k, n//k).\"\"\"\n",
    "        for k in range(int(math.sqrt(n)), 0, -1):\n",
    "            if n % k == 0:\n",
    "                return k, n // k\n",
    "        return 1, n                                           # prime n\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    def forward(self, A_flat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        A_flat : (B, H, S, n)  row-simplex weights\n",
    "        returns: (B, H, S, d_k)\n",
    "        \"\"\"\n",
    "        B, H, S, n = A_flat.shape\n",
    "        if n > self.N_max:\n",
    "            raise ValueError(f\"ConstValueBank: requested {n} prototypes \"\n",
    "                             f\"but N_max={self.N_max}\")\n",
    "\n",
    "        # slice only the needed prototypes\n",
    "        V_use = self.V[:, :n, :]                              # (H, n, d_k)\n",
    "\n",
    "        # ------- flat (single-level) path ------------------------------\n",
    "        if self.flat:\n",
    "            # einsum: (B,H,S,n) Ã— (H,n,d_k)  â†’  (B,H,S,d_k)\n",
    "            return torch.einsum(\"bhsn,hnd->bhsd\", A_flat, V_use)\n",
    "\n",
    "        # ------- two-level coarse/fine routing -------------------------\n",
    "        K, M = self._factorize(n)                             # KÂ·M == n\n",
    "        A1   = A_flat.view(B, H, S, K, M).sum(-1)             # (B,H,S,K)\n",
    "        V_hkm = V_use.view(H, K, M, self.d_k)                 # (H,K,M,d_k)\n",
    "\n",
    "        # combine per-head coarse/fine\n",
    "        V_coarse = torch.einsum(\"bhsK,hKmd->bhsmd\", A1, V_hkm)  # (B,H,S,M,d_k)\n",
    "        return V_coarse.sum(-2)                               # (B,H,S,d_k)\n",
    "#\n",
    "# USAGE IN ConvexMixer\n",
    "# --------------------\n",
    "# 1.  mixer = ConvexMixer(...);   bank = ConstValueBank(heads, d_k)\n",
    "# 2.  logits â†’ A (row-simplex)  shape (B,H,S,N_total)\n",
    "# 3.  out = bank(A)     # convex combination of prototypes\n",
    "# 4.  Replace v-dependent matmul entirely.\n",
    "#flat_dict=True â†’ uses a single-level bank of size N_total. Simple, cost O(SÂ·N).\n",
    "#flat_dict=False â†’ two-level (coarseÃ—fine) routing, cost ~O(SÂ·âˆšN). Larger models.\n",
    "# This keeps f(x)=A(x)@V convex, hull-preserving, and scales with (d_k, S)\n",
    "# as per the table above.\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "class ConvexMixer(nn.Module):\n",
    "    def __init__(self,heads:int, d_k: int, r: int):\n",
    "        super().__init__()\n",
    "        self.register_buffer('eps', torch.tensor(1e-6))\n",
    "        self.register_buffer('noise_scale', torch.tensor(1e-5))\n",
    "\n",
    "        self.score_q = ScalarHull(d_k, 8)\n",
    "        self.score_k = ScalarHull(d_k, 8)\n",
    "        self.gate = nn.Softplus()\n",
    "        self.lin_h_q = nn.Linear(d_k, r, bias=False)\n",
    "        self.lin_h_k = nn.Linear(d_k, r, bias=False)\n",
    "        self.register_buffer('creative', torch.tensor(True))\n",
    "        self.fused = FusedLogSumExp(dim=-1)\n",
    "        self.bank = ConstValueBank(heads, d_k,flat_dict=False)\n",
    "\n",
    "    def forward(self, q, k, v, mask,mask_back):\n",
    "        B, H, S, D = q.shape\n",
    "        device = q.device\n",
    "\n",
    "        # â€”â€”â€” 1) tau â€”â€”â€”\n",
    "        gate_q = self.gate(q)                          # (B,H,S,d_k)\n",
    "        q = q * gate_q\n",
    "        r = torch.sqrt(q.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        # 1. compute the exponent argument\n",
    "        alpha = 0.30343 * r + 0.22159          # (..., 1)\n",
    "        \n",
    "        # 2. clamp *before* the exp to a safe upper bound\n",
    "        alpha_max = math.log(1e4)              # â‰ˆ 9.21034  â†’ tau â‰¤ 1e4\n",
    "        alpha = alpha.clamp(max=alpha_max)\n",
    "        \n",
    "        # 3. now exponentiate â€“ cannot overflow\n",
    "        tau = torch.exp(alpha)                 # (..., 1)  â‰¤ 1e4        # â€”â€”â€” 2) scalar hull scores â€”â€”â€”\n",
    "        fq = self.score_q(q)  # (B,H,S)\n",
    "        gk = self.score_k(k)  # (B,H,S)\n",
    "        if self.creative:\n",
    "            qn = (torch.rand_like(q) - 0.5) * self.noise_scale\n",
    "            kn = (torch.rand_like(k) - 0.5) * self.noise_scale\n",
    "            fq_ = self.score_q(q + qn)\n",
    "            gk_ = self.score_k(k + kn)\n",
    "            delta_fq = (fq_ - fq).detach()\n",
    "            delta_gk = (gk_ - gk).detach()\n",
    "            fq = fq - 0.1 * delta_fq\n",
    "            gk = gk - 0.1 * delta_gk\n",
    "\n",
    "        # â€”â€”â€” 3) random-feature kernel â€”â€”â€”\n",
    "        phi_q = self.gate(self.lin_h_q(q).clamp(max=20.0))\n",
    "        phi_k = self.gate(self.lin_h_k(k).clamp(max=20.0))\n",
    "        log_phi_q = torch.log(phi_q + self.eps)\n",
    "        log_phi_k = torch.log(phi_k + self.eps)\n",
    "        logK = self.fused((phi_q+self.eps).log().unsqueeze(-2)\n",
    "               + (phi_k+self.eps).log().unsqueeze(-3)).squeeze(-1)\n",
    "        # subtract log(r) so it becomes log of the mean, not the sum\n",
    "        r = phi_q.size(-1)   # = number of random features\n",
    "        logK = logK - math.log(r)\n",
    "\n",
    "        # 5) Assemble logits with mask and temperature\n",
    "        log_mask = torch.log(mask.clamp_min(self.eps))  # convert to log-domain\n",
    "        scores = fq.unsqueeze(-1) + gk.unsqueeze(-2) + logK + log_mask  # additive\n",
    "        \n",
    "        logits = scores * tau.squeeze(-1).unsqueeze(-1)\n",
    "        log_weights = logits - torch.logsumexp(logits, dim=-1, keepdim=True)\n",
    "        weights = torch.exp(log_weights)  # (B,H,S,S)\n",
    "        # in forward(), after log_weights â†’ weights\n",
    "        # weights: (B, H, S, S)  â†’  row-simplex A\n",
    "        out = self.bank(weights)                  # (B, H, S, d_k)\n",
    "        # 6) Weighted sum for attention output\n",
    "        #out = weights.reshape(B * H, S, S).bmm(v.reshape(B * H, S, D))\n",
    "        #out = out.reshape(B, H, S, D)\n",
    "\n",
    "        # Optional: compute aggregated attn_score\n",
    "        attn_score = weights.sum(dim=-3)\n",
    "        attn_score = torch.softmax(attn_score, dim=-1).mean(dim=1)\n",
    "        min_vals = attn_score.min(dim=-1, keepdim=True).values\n",
    "        max_vals = attn_score.max(dim=-1, keepdim=True).values\n",
    "        attn_score = (attn_score - min_vals) / (max_vals - min_vals + self.eps)\n",
    "        return out, attn_score\n",
    "\n",
    "class InterleavedPhaseChannelizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding shape: (B, T, 2*M) == [c0, Ï•0, c1, Ï•1, ..., c_{M-1}, Ï•_{M-1}].\n",
    "    Now uses a convex bump kernel instead of sine.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, init_gate_bias: float = -3.0, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 2 == 0, \"embed_dim must be even\"\n",
    "        self.M = embed_dim // 2\n",
    "        self.gate_raw = nn.Parameter(torch.full((self.M,), init_gate_bias))\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.eps = eps\n",
    "\n",
    "    def bump_kernel(self, T: int, device: torch.device):\n",
    "        \"\"\"\n",
    "        Returns a (T, T) kernel K[i,j] âˆˆ (0,1], convex in |j-i|,\n",
    "        with K[i,i]=1, and smoothly â†’ eps as jâ†’end.\n",
    "        \"\"\"\n",
    "        i = torch.arange(T, device=device).unsqueeze(1).float()      # (T,1)\n",
    "        j = torch.arange(T, device=device).unsqueeze(0).float()      # (1,T)\n",
    "\n",
    "        # future offset u = (j - i) / (T - i)\n",
    "        diff    = (j - i).clamp(min=0.0)                            # (T,T)\n",
    "        horizon = (T - i).clamp(min=1.0)                            # (T,1)\n",
    "        u       = (diff / horizon).clamp(max=1.0 - self.eps)        # [0,1)\n",
    "\n",
    "        # bump exponent: convex, =1 at u=0, â†’ -âˆ as uâ†’1\n",
    "        expnt = 1.0 - 1.0 / (1.0 - u*u + self.eps)\n",
    "        K      = torch.exp(expnt)                                   # (T,T)\n",
    "\n",
    "        # enforce exact eps at uâ‰ˆ1\n",
    "        K = torch.where(u >= 1.0 - self.eps, torch.full_like(K, self.eps), K)\n",
    "        return K  # (T,T)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, 2*M)\n",
    "        B, T, _ = x.shape\n",
    "        M       = self.M\n",
    "        device  = x.device\n",
    "        dtype   = x.dtype\n",
    "\n",
    "        # 1) content channels\n",
    "        x_c = x[..., 0::2]  # (B, T, M)\n",
    "\n",
    "        # 2) build convex bump kernel and mask it\n",
    "        K = self.bump_kernel(T, device).to(dtype)   # (T,T)\n",
    "        causal2d = mask.view(T, T).to(dtype)        # (T,T)\n",
    "        K = K * causal2d                            # zero out masked-out positions\n",
    "\n",
    "        # 3) normalize rows to sum=1\n",
    "        K = K / (K.sum(-1, keepdim=True).clamp(min=self.eps))\n",
    "\n",
    "        # 4) accumulate phase Ï†[b,i,m] = âˆ‘â‚â±¼â‚Œâ‚€â€¦Tâ‚‹â‚â‚ K[i,j] Â· x_c[b,j,m]\n",
    "        Ï† = torch.einsum('ij,bjm->bim', K, x_c)     # (B, T, M)\n",
    "\n",
    "        # 5) gate & write back into odd slots\n",
    "        gate = self.softplus(self.gate_raw).view(1,1,M)  # (1,1,M)\n",
    "        Ï†g   = Ï† * gate                                 # (B, T, M)\n",
    "        out  = x.clone()\n",
    "        out[..., 1::2] = Ï†g\n",
    "        return out\n",
    "\n",
    "class ConvexRoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    Convex RoPE substitute with dynamic sequence length.\n",
    "    Generates a monotonic, convex angle for each position-pair subspace.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_k: int):\n",
    "        super().__init__()\n",
    "        assert d_k % 2 == 0, \"d_k must be even for pairing\"\n",
    "        self.d_pair = d_k // 2\n",
    "        # Linear mapping from scalar pos to angle per pair\n",
    "        self.lin = nn.Linear(1, self.d_pair)\n",
    "\n",
    "    def forward(self, S: int, device: torch.device) -> torch.Tensor:\n",
    "        # positions normalized to [0,1]\n",
    "        pos = torch.arange(S, device=device, dtype=torch.float32).unsqueeze(1) / max(S - 1, 1)\n",
    "        Î¸ = F.softplus(self.lin(pos))  # (S, d_pair), convex & â‰¥0\n",
    "        return Î¸  # shape (S, d_pair)\n",
    "\n",
    "from typing import Tuple\n",
    "def apply_convex_rope(q: torch.Tensor, k: torch.Tensor, Î¸: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:    \n",
    "    \"\"\"\n",
    "    Apply ConvexRoPE rotation to q and k.\n",
    "      q, k: (B, H, S, d_k)\n",
    "      Î¸:    (S, d_k//2)\n",
    "    Returns rotated q, k of same shape.\n",
    "    \"\"\"\n",
    "    B, H, S, d_k = q.shape\n",
    "    d2 = d_k // 2\n",
    "\n",
    "    # reshape into (paired) shape\n",
    "    q_ = q.view(B, H, S, d2, 2)\n",
    "    k_ = k.view(B, H, S, d2, 2)\n",
    "\n",
    "    # split into even (x) and odd (y) parts\n",
    "    x_q, y_q = q_.unbind(-1)  # each (B, H, S, d2)\n",
    "    x_k, y_k = k_.unbind(-1)\n",
    "\n",
    "    # build cos/sin with correct shape (1,1,S,d2) to broadcast\n",
    "    cosÎ¸ = torch.cos(Î¸).unsqueeze(0).unsqueeze(0)  # (1,1,S,d2)\n",
    "    sinÎ¸ = torch.sin(Î¸).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # rotate x/y pairs\n",
    "    x_q2 = x_q * cosÎ¸ - y_q * sinÎ¸\n",
    "    y_q2 = x_q * sinÎ¸ + y_q * cosÎ¸\n",
    "    x_k2 = x_k * cosÎ¸ - y_k * sinÎ¸\n",
    "    y_k2 = x_k * sinÎ¸ + y_k * cosÎ¸\n",
    "\n",
    "    # stack back into pairs and reshape to original\n",
    "    q_rot = torch.stack([x_q2, y_q2], dim=-1).reshape(B, H, S, d_k)\n",
    "    k_rot = torch.stack([x_k2, y_k2], dim=-1).reshape(B, H, S, d_k)\n",
    "\n",
    "    return q_rot, k_rot\n",
    "    \n",
    "# ----------------------------------------------------------------------\n",
    "#   Pairwise Hull Attention (maskâ€‘aware)\n",
    "# ----------------------------------------------------------------------\n",
    "class PairwiseHullAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % heads == 0, \"embed_dim must be divisible by heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.d_k = embed_dim // heads\n",
    "        self.pre = LinearPreMix(embed_dim, heads)\n",
    "        self.mixer = ConvexMixer(heads,self.d_k, self.d_k*2)#dont need many for scoring\n",
    "        self.W_O = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.phase = InterleavedPhaseChannelizer(embed_dim)\n",
    "        self.register_buffer('noise_scale', torch.tensor(1e-5))\n",
    "        self.register_buffer(\"creative\", torch.tensor(True))\n",
    "        self.rope = ConvexRoPE(self.d_k)\n",
    "\n",
    "    def forward(self, x, mask=None,mask_back=None):\n",
    "        self.phase(x,mask) #apply in-place positional phasing\n",
    "        B, S, E = x.shape\n",
    "        Q, K, V= self.pre(x)\n",
    "        offset = self.rope(S, x.device)                  # (S, d_k//2)\n",
    "        Q, K = apply_convex_rope(Q, K, offset) \n",
    "\n",
    "        mean = 0.5 * (Q.mean() + K.mean())\n",
    "        std  = 0.5 * (Q.std()  + K.std())\n",
    "        Q = (Q - mean) / std\n",
    "        K = (K - mean) / std        \n",
    "        y,attn_scores = self.mixer(Q, K, V, mask=mask,mask_back=mask_back)\n",
    "        \n",
    "        y = y.transpose(1, 2).reshape(B, S, self.embed_dim)\n",
    "        return self.W_O(y), attn_scores\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# FixedGainNorm: affine-free, hull-preserving alternative to LayerNorm\n",
    "#   y = Î³ âŠ™ x            with  Î³ = sigmoid(Ï)  âˆˆ (0,1)  (learned, constant)\n",
    "# â€¢ element-wise positive contraction (no centring, no data-dependent scale)\n",
    "# â€¢ convex in x  (linear with non-negative weights)\n",
    "# â€¢ for every token z  â‡’  y lies inside conv{0, z}  âŠ‚  conv{all z}   â‡’  hull-safe\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class FrozenAffine(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.02, freeze_after=1000):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mu',     torch.zeros(dim))\n",
    "        self.register_buffer('sigma',  torch.ones(dim))\n",
    "        self.register_buffer('steps',  torch.tensor(0, dtype=torch.long))\n",
    "        self.rho   = nn.Parameter(torch.full((dim,), -2.0))  # Î³ = sigmoid(Ï) âˆˆ (0,1)\n",
    "        self.beta  = nn.Parameter(torch.zeros(dim))\n",
    "        self.mom   = momentum\n",
    "        self.freeze_after = freeze_after\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.steps < self.freeze_after:\n",
    "            with torch.no_grad():\n",
    "                m = x.mean(dim=0)\n",
    "                v = x.var(dim=0, unbiased=False).sqrt()\n",
    "                self.mu    = (1-self.mom) * self.mu    + self.mom * m\n",
    "                self.sigma = (1-self.mom) * self.sigma + self.mom * v\n",
    "                self.steps += 1\n",
    "\n",
    "        Î³ = torch.sigmoid(self.rho)           # (0,1)\n",
    "        x_hat = (x - self.mu) / (self.sigma + self.eps)\n",
    "        return x_hat * Î³ + self.beta\n",
    "        \n",
    "class ConstantGate(nn.Module):\n",
    "    def __init__(self, theta: float = 1/math.e):\n",
    "        super().__init__()\n",
    "        self.theta = theta\n",
    "    def forward(self, x):\n",
    "        # ignore x, always return fixed scalar âˆˆ (0,1)\n",
    "        return self.theta\n",
    "\n",
    "class OmniHullBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, moe_petals):\n",
    "        super().__init__()\n",
    "        self.attn      = PairwiseHullAttention(dim, heads)\n",
    "        self.hff       = VectorHull(dim, moe_petals)\n",
    "        self.ln1       = FrozenAffine(dim)\n",
    "        self.ln2       = FrozenAffine(dim)\n",
    "        # --- two per-branch residual gates ---\n",
    "        self.res_gate1 = ConvexGate(dim)\n",
    "        self.res_gate2 = ConvexGate(dim)\n",
    "\n",
    "    def forward(self, x, mask=None, mask_back=None):\n",
    "        # â€”â€”â€” attention branch â€”â€”â€”\n",
    "        x0, _ = x, None\n",
    "        x1, attn_scores = self.attn(self.ln1(x0), mask, mask_back)   # x1 is the residual delta\n",
    "        g1 = self.res_gate1(x0)                                      # shape (B,S,1)\n",
    "        x  = x0 + g1 * x1\n",
    "\n",
    "        # â€”â€”â€” feed-forward branch â€”â€”â€”\n",
    "        x2 = self.hff(self.ln2(x))                                   # second residual delta\n",
    "        g2 = self.res_gate2(x)                                       # gate on the updated state\n",
    "        x  = x + g2 * x2\n",
    "\n",
    "        return x, attn_scores\n",
    "\n",
    "class ConvexEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    1) Rescales raw weights by 1/sqrt(fan-in) at init\n",
    "    2) Applies a per-channel contraction after the positive-linear out layer\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int = 512, out_dim: int = 768):\n",
    "        super().__init__()\n",
    "\n",
    "        # â€” first positive-linear projection â€”\n",
    "        self.Wx = PositiveLinearHK(vocab_size, hidden_dim, bias=False)\n",
    "        scalar = torch.tensor(1.0 / math.sqrt(hidden_dim), dtype=self.Wx.raw.dtype, device=self.Wx.raw.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # guard (a): undo sqrt(fan-in) blow-up at initialization\n",
    "            self.Wx.raw.mul_(scalar)\n",
    "\n",
    "        # â€” convex skip connection â€”\n",
    "        self.Wz = PositiveLinearHK(hidden_dim, hidden_dim, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.Wz.raw.mul_(scalar)\n",
    "\n",
    "        self.act = nn.Softplus()\n",
    "\n",
    "        # â€” final convex output â€”\n",
    "        self.out = PositiveLinearHK(hidden_dim, out_dim)\n",
    "        with torch.no_grad():\n",
    "            self.out.raw.mul_(1.0 / math.sqrt(hidden_dim))\n",
    "\n",
    "        # â€” post-embedding contraction (guard b) â€”\n",
    "        self.contraction = FrozenAffine(out_dim)\n",
    "\n",
    "    def forward(self, P: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        P: (batch, seq, vocab_size) one-hot/simplex\n",
    "        returns: (batch, seq, out_dim)\n",
    "        \"\"\"\n",
    "        h0 = self.act(self.Wx(P))             # â†’ (batch, seq, hidden_dim)\n",
    "        h1 = self.act(h0 + self.Wz(h0))       # convex skip\n",
    "        raw = self.out(h1)                    # â†’ (batch, seq, out_dim)\n",
    "        return self.contraction(raw)          # attenuate each cha\n",
    "\n",
    "def tokens_to_simplex(idx: torch.LongTensor, vocab_size: int) -> torch.FloatTensor:\n",
    "    \"\"\"\n",
    "    Convert token indices to one-hot simplex representation (batch, seq, V).\n",
    "    At inference, P is exactly one-hot; for relaxed optimization, replace with softmax.\n",
    "    \"\"\"\n",
    "    P = F.one_hot(idx, num_classes=vocab_size).float()\n",
    "    return P\n",
    "\n",
    "        \n",
    "# ----------------------------------------------------------------------\n",
    "#   GPT Wrapper with Causal Mask\n",
    "# ----------------------------------------------------------------------\n",
    "class ConvexGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        depth: int,\n",
    "        heads: int,\n",
    "        moe_petals: int,\n",
    "        creativity: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert embed_dim >= 1, \"embed_channels must be â‰¥1\"\n",
    "        self.embed_channels = embed_dim\n",
    "        self.embed_dim = 2 * embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embeddings only for even channels [0,2,4,...]\n",
    "        self.convex_embed = ConvexEmbedding(vocab_size, hidden_dim=512, out_dim=embed_dim)\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Blocks operate on full embed_dim\n",
    "        self.blocks = nn.ModuleList([\n",
    "        OmniHullBlock(\n",
    "            self.embed_dim,\n",
    "            heads,\n",
    "            moe_petals\n",
    "                  )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = FrozenAffine(self.embed_dim)\n",
    "        self.head = nn.Linear(self.embed_dim, vocab_size, bias=False)\n",
    "        self.set_creativity(creativity)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_mask(S: int, eps: float = 1e-17) -> torch.Tensor:\n",
    "        i = torch.arange(S).unsqueeze(1).float()  # (S,1)\n",
    "        j = torch.arange(S).unsqueeze(0).float()  # (1,S)\n",
    "    \n",
    "        # Compute normalized future offset u = (j - i) / (S - i)\n",
    "        diff = (j - i).clamp(min=0.0)\n",
    "        horizon = (S - i).clamp(min=1.0)\n",
    "        u = (diff / horizon).clamp(max=1.0)  # âˆˆ [0,1]\n",
    "    \n",
    "        # Decay from 1 to eps as u âˆˆ (0, 1]\n",
    "        decay = torch.exp(1.0 - 1.0 / (1.0 - u ** 2 + 1e-6))  # smooth bump\n",
    "        decay = decay.clamp_max(1.0)\n",
    "    \n",
    "        # Build final mask: full for j â‰¤ i, soft-decay for i < j\n",
    "        mask = torch.where(j <= i, torch.ones_like(decay), decay)\n",
    "        mask = mask.clamp(min=eps)\n",
    "        return mask  # shape (S, S)\n",
    "        \n",
    "    @staticmethod\n",
    "    def reversed_logistic_mask(S: int, eps: float = 1e-17) -> torch.Tensor:\n",
    "            i = torch.arange(S).unsqueeze(1).float()  # (S,1)\n",
    "            j = torch.arange(S).unsqueeze(0).float()  # (1,S)\n",
    "        \n",
    "            # Compute normalized past offset u = (i - j) / (i + 1)\n",
    "            diff = (i - j).clamp(min=0.0)  # zero if j â‰¥ i (future)\n",
    "            horizon = (i + 1).clamp(min=1.0)  # prevent division by zero\n",
    "            u = (diff / horizon).clamp(max=1.0)\n",
    "        \n",
    "            # Smooth decay from 1 to eps as we move into the past\n",
    "            decay = torch.exp(1.0 - 1.0 / (1.0 - u ** 2 + 1e-6))\n",
    "            decay = decay.clamp_max(1.0)\n",
    "        \n",
    "            # Build mask: decay for j < i (past), 1 for j â‰¥ i (future including present)\n",
    "            mask = torch.where(j >= i, torch.ones_like(decay), decay)\n",
    "            mask = mask.clamp(min=eps)\n",
    "            return mask  # shape (S, S)\n",
    "\n",
    "    def set_creativity(self, value: bool):\n",
    "        val = torch.tensor(value)\n",
    "        def recurse(m):\n",
    "            if hasattr(m, \"creative\"):\n",
    "                m.creative.copy_(val)\n",
    "            for child in m.children():\n",
    "                recurse(child)\n",
    "        recurse(self)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor):\n",
    "        \"\"\"\n",
    "        idx: (B, S) token indices\n",
    "        returns logits: (B, S, vocab_size)\n",
    "        \"\"\"\n",
    "        B, S = idx.shape\n",
    "        device = idx.device\n",
    "        #stack 0::2 as embeddings, 1::2 as zeros for positional embeddings\n",
    "        P = tokens_to_simplex(idx, self.vocab_size)       # (batch, seq, V)\n",
    "        embeddings = self.convex_embed(P)                          # (batch, seq, d_model)\n",
    "        x = torch.stack([embeddings, torch.zeros_like(embeddings)], dim=-1).reshape(idx.shape[0], idx.shape[1], self.embed_dim)\n",
    "\n",
    "        # 3) build causal mask\n",
    "        mask_back = self.reversed_logistic_mask(S)          # (1, 1, S, S)\n",
    "        mask = self.logistic_mask(S)          # (1, 1, S, S)\n",
    "\n",
    "        attn_scores = []\n",
    "        # 4) apply each block (which will write Ï† into odd slots)\n",
    "        for blk in self.blocks:\n",
    "            x,attn_temp = blk(x, mask,mask_back)\n",
    "            attn_scores.append(attn_temp)\n",
    "\n",
    "        attn_scores =  torch.stack(attn_scores).mean(dim=0)#divide by heads\n",
    "        # 5) final layernorm + head\n",
    "        x = self.ln_f(x)                             # (B, S, embed_dim)\n",
    "        logits = self.head(x)                        # (B, S, vocab_size)\n",
    "        return logits,attn_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q47er8Ni84xt"
   },
   "source": [
    "#if you use my ideas, please credit me, dont just steal\n",
    "joshuah.rainstar@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLs7qQUOxRPQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  1347956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuahkuttenkuler/miniforge3/lib/python3.12/site-packages/torch/jit/annotations.py:388: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372.21978759765625\n",
      "299.2120361328125\n",
      "267.8901062011719\n",
      "200.6062469482422\n",
      "165.71351623535156\n",
      "138.6451873779297\n",
      "118.71314239501953\n",
      "90.47803497314453\n",
      "69.84860229492188\n",
      "58.20095443725586\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "def wolf_update(p: torch.Tensor,\n",
    "                g: torch.Tensor,\n",
    "                state_p: torch.Tensor,\n",
    "                lr: float):\n",
    "    # define your constants here instead of capturing them\n",
    "    etcerta: float = 0.367879441\n",
    "    et:      float = 1.0 - etcerta\n",
    "\n",
    "    # same logic as before\n",
    "    update    = state_p * et + g * etcerta\n",
    "    new_state = state_p * et + update * etcerta\n",
    "    sign_agree = torch.sign(update) * torch.sign(g)\n",
    "    update    = update + (torch.rand_like(update)*2 - 1) * etcerta * update\n",
    "    p_new     = torch.where(sign_agree > 0, p - lr * update, p)\n",
    "    return p_new, new_state\n",
    "\n",
    "class Wolf(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p]['p'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = closure() if closure is not None else None\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state_p = self.state[p]['p']\n",
    "                p_new, new_state = wolf_update(p.data, p.grad, state_p, lr)\n",
    "                p.data.copy_(p_new)\n",
    "                state_p.copy_(new_state)\n",
    "        return loss\n",
    "\n",
    "# 1) Load data and meta as before\n",
    "data_dir  = os.path.dirname(base_dir)\n",
    "train_ids = np.fromfile(os.path.join(data_dir, 'train.bin'), dtype=np.uint16)\n",
    "val_ids   = np.fromfile(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16)\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# 2) Compute dataâ€marginal q[v]\n",
    "counts = np.bincount(train_ids, minlength=vocab_size).astype(float)\n",
    "q = torch.tensor(counts / counts.sum(), dtype=torch.float32, device=device)  # [V]\n",
    "\n",
    "# 3) Dataset + DataLoader\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = torch.from_numpy(data).long()\n",
    "        self.block_size = block_size\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "block_size = 256\n",
    "train_loader = DataLoader(CharDataset(train_ids, block_size),\n",
    "                          batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(CharDataset(val_ids,   block_size),\n",
    "                          batch_size=32, shuffle=False, drop_last=True)\n",
    "virgin = ConvexGPT(vocab_size = vocab_size,embed_dim  = 32,depth  = 4,heads = 8,moe_petals = 16,creativity=True)\n",
    "\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in virgin.parameters()))\n",
    "model = torch.jit.script(virgin)\n",
    "model = model.to(device)\n",
    "decay_params = []\n",
    "main_params  = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.endswith('.rho') or 'gate_raw' in name:\n",
    "        decay_params.append(param)\n",
    "    else:\n",
    "        main_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=2e-3)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "# 6) Train / eval functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits ,attn_weights = model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        per_token_loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            yb.view(-1),\n",
    "            reduction='none'  # This gives the raw loss per token\n",
    "        \n",
    "        ).reshape(B,T) # Shape: (B*S,)\n",
    "        loss = per_token_loss.mean()\n",
    "        loss_cpu = per_token_loss.cpu().detach().numpy()\n",
    "        tokens = [[itos[idx] for idx in seq.tolist()] for seq in yb]\n",
    "        attn_cpu = attn_weights.cpu().detach().numpy()\n",
    "        update_framebuffer(attn_cpu, loss_cpu, loss.item(), tokens)\n",
    "        update_display()\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 7) Run training\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_epoch() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.022532939910889\n",
      "7.428704738616943\n",
      "9.572108268737793\n",
      "11.194742202758789\n",
      "12.415599822998047\n",
      "13.611005783081055\n",
      "14.388189315795898\n",
      "14.848260879516602\n",
      "15.300315856933594\n",
      "15.240216255187988\n",
      "15.432043075561523\n",
      "15.10706615447998\n",
      "14.974448204040527\n",
      "14.655967712402344\n",
      "14.387935638427734\n",
      "13.404953002929688\n",
      "13.186662673950195\n",
      "12.71640396118164\n",
      "11.926271438598633\n",
      "12.15706729888916\n",
      "12.090102195739746\n",
      "11.873231887817383\n",
      "10.87096118927002\n",
      "10.023161888122559\n",
      "9.367990493774414\n",
      "9.286840438842773\n",
      "9.301288604736328\n",
      "8.976560592651367\n",
      "8.503795623779297\n",
      "7.878419876098633\n",
      "7.4534010887146\n",
      "7.033581256866455\n",
      "6.872276306152344\n",
      "6.749919414520264\n",
      "6.454171180725098\n",
      "6.227264404296875\n",
      "5.95709753036499\n",
      "5.805713176727295\n",
      "5.554544448852539\n",
      "5.42237663269043\n",
      "5.215774059295654\n",
      "5.090006351470947\n",
      "5.042607307434082\n",
      "4.9816365242004395\n",
      "4.844174861907959\n",
      "4.765031814575195\n",
      "4.61279296875\n",
      "4.491673946380615\n",
      "4.378147125244141\n",
      "4.193406105041504\n",
      "4.152249336242676\n",
      "3.979769706726074\n",
      "3.8180627822875977\n",
      "3.7441635131835938\n",
      "3.7862298488616943\n",
      "3.6808152198791504\n",
      "3.544797420501709\n",
      "3.5309195518493652\n",
      "3.463372230529785\n",
      "3.563595771789551\n",
      "3.625884771347046\n",
      "3.5862791538238525\n",
      "3.472630023956299\n",
      "3.5116310119628906\n",
      "3.4846489429473877\n",
      "3.4258410930633545\n",
      "3.434842824935913\n",
      "3.4000840187072754\n",
      "3.3507394790649414\n",
      "3.4385976791381836\n",
      "3.378974199295044\n",
      "3.3561878204345703\n",
      "3.365793228149414\n",
      "3.332101821899414\n",
      "3.341129779815674\n",
      "3.322988986968994\n",
      "3.270951509475708\n",
      "3.3577234745025635\n",
      "3.3050529956817627\n",
      "3.2960636615753174\n",
      "3.2525391578674316\n",
      "3.2813873291015625\n",
      "3.2949001789093018\n",
      "3.275059700012207\n",
      "3.262922763824463\n",
      "3.2367217540740967\n",
      "3.2461047172546387\n",
      "3.219904661178589\n",
      "3.241837978363037\n",
      "3.243729591369629\n",
      "3.236395835876465\n",
      "3.208183765411377\n",
      "3.244062900543213\n",
      "3.2000913619995117\n",
      "3.1895651817321777\n",
      "3.211203098297119\n",
      "3.2056057453155518\n",
      "3.1769697666168213\n",
      "3.159701347351074\n",
      "3.162050247192383\n",
      "3.170027256011963\n",
      "3.16452693939209\n",
      "3.1380128860473633\n",
      "3.1382875442504883\n",
      "3.184377670288086\n",
      "3.1705727577209473\n",
      "3.120466709136963\n",
      "3.13291072845459\n",
      "3.1374614238739014\n",
      "3.1427536010742188\n",
      "3.091555595397949\n",
      "3.0880985260009766\n",
      "3.129595994949341\n",
      "3.0905215740203857\n",
      "3.1360883712768555\n",
      "3.0921707153320312\n",
      "3.0833487510681152\n",
      "3.1038827896118164\n",
      "3.1092777252197266\n",
      "3.0724875926971436\n",
      "3.093216896057129\n",
      "3.0613961219787598\n",
      "3.1058363914489746\n",
      "3.0484280586242676\n",
      "3.073732852935791\n",
      "3.0729315280914307\n",
      "3.0678768157958984\n",
      "3.0596179962158203\n",
      "3.067059278488159\n",
      "3.0618247985839844\n",
      "3.03426456451416\n",
      "3.0585577487945557\n",
      "3.065802574157715\n",
      "3.054935932159424\n",
      "3.052727699279785\n",
      "3.037562847137451\n",
      "3.0582826137542725\n",
      "3.034693717956543\n",
      "3.043856143951416\n",
      "3.021256923675537\n",
      "3.073333978652954\n",
      "3.0666708946228027\n",
      "3.042431354522705\n",
      "3.041717052459717\n",
      "3.0611538887023926\n",
      "3.0389108657836914\n",
      "3.039473533630371\n",
      "3.080380439758301\n",
      "3.04484486579895\n",
      "3.0591812133789062\n",
      "3.0782270431518555\n",
      "3.0454859733581543\n",
      "3.0264158248901367\n",
      "3.0593926906585693\n",
      "3.0695340633392334\n",
      "3.033320426940918\n",
      "3.0679564476013184\n",
      "3.0680766105651855\n",
      "3.0343613624572754\n",
      "3.0354065895080566\n",
      "3.050844192504883\n",
      "3.043799877166748\n",
      "3.035717725753784\n",
      "3.006460666656494\n",
      "3.036341667175293\n",
      "3.0560660362243652\n",
      "3.0428671836853027\n",
      "3.048717975616455\n",
      "3.0169026851654053\n",
      "3.0493884086608887\n",
      "3.0456666946411133\n",
      "3.0522725582122803\n",
      "3.0817019939422607\n",
      "3.031942844390869\n",
      "3.0676956176757812\n",
      "3.0503690242767334\n",
      "3.0583577156066895\n",
      "3.0622708797454834\n",
      "3.0622987747192383\n",
      "3.057694673538208\n",
      "3.059027671813965\n",
      "3.036276340484619\n",
      "3.0508363246917725\n",
      "3.0404274463653564\n",
      "3.0258586406707764\n",
      "3.0505495071411133\n",
      "3.033355951309204\n",
      "3.058807373046875\n",
      "3.052910327911377\n",
      "3.049781084060669\n",
      "3.0507471561431885\n",
      "3.0380077362060547\n",
      "3.0292131900787354\n",
      "3.0570249557495117\n",
      "3.037597417831421\n",
      "3.025878429412842\n",
      "3.050713062286377\n",
      "3.050825595855713\n",
      "3.0318331718444824\n",
      "3.0230796337127686\n",
      "3.023740530014038\n",
      "3.006596565246582\n",
      "3.012472629547119\n",
      "3.01832914352417\n",
      "3.025585651397705\n",
      "3.055593729019165\n",
      "3.0330257415771484\n",
      "3.05546236038208\n",
      "3.0387330055236816\n",
      "3.052560806274414\n",
      "3.038299083709717\n",
      "3.0333237648010254\n",
      "3.023918628692627\n",
      "3.07181978225708\n",
      "3.032632827758789\n",
      "2.991593837738037\n",
      "3.0290744304656982\n",
      "3.0632143020629883\n",
      "3.009953498840332\n",
      "3.0382232666015625\n",
      "2.9842112064361572\n",
      "3.0133700370788574\n",
      "3.0085160732269287\n",
      "3.02665376663208\n",
      "3.0281853675842285\n",
      "3.0390186309814453\n",
      "3.0160465240478516\n",
      "3.0019190311431885\n",
      "3.000556468963623\n",
      "3.0309536457061768\n",
      "2.988605499267578\n",
      "3.026235580444336\n",
      "3.0284509658813477\n",
      "3.0071492195129395\n",
      "3.0174078941345215\n",
      "3.0243544578552246\n",
      "3.035090446472168\n",
      "3.0342464447021484\n",
      "3.0307788848876953\n",
      "3.0506205558776855\n",
      "3.009331226348877\n",
      "3.060145616531372\n",
      "3.039126396179199\n",
      "3.0511221885681152\n",
      "3.044985294342041\n",
      "3.0357160568237305\n",
      "3.0432567596435547\n",
      "3.0291810035705566\n",
      "3.03938627243042\n",
      "3.0061962604522705\n",
      "3.001275062561035\n",
      "3.0479841232299805\n",
      "3.0002553462982178\n",
      "3.041978359222412\n",
      "3.001253128051758\n",
      "3.0033371448516846\n",
      "2.9890685081481934\n",
      "2.995542287826538\n",
      "3.007638931274414\n",
      "2.994320869445801\n",
      "3.008517265319824\n",
      "3.007739305496216\n",
      "3.032346725463867\n",
      "3.0364251136779785\n",
      "3.004265308380127\n",
      "2.9921188354492188\n",
      "3.0089974403381348\n",
      "2.989401340484619\n",
      "3.0166683197021484\n",
      "2.998988151550293\n",
      "2.9892194271087646\n",
      "3.0320839881896973\n",
      "3.020972967147827\n",
      "3.037590980529785\n",
      "3.0109469890594482\n",
      "2.980947971343994\n",
      "3.0068154335021973\n",
      "3.031066417694092\n",
      "3.045788049697876\n",
      "2.989387273788452\n",
      "2.9818410873413086\n",
      "3.0371880531311035\n",
      "3.011496067047119\n",
      "3.0279383659362793\n",
      "2.9841010570526123\n",
      "3.0087196826934814\n",
      "3.0309767723083496\n",
      "2.995950698852539\n",
      "3.007793664932251\n",
      "2.9763059616088867\n",
      "3.0038251876831055\n",
      "3.003342628479004\n",
      "3.003683567047119\n",
      "3.0025370121002197\n",
      "3.025054931640625\n",
      "2.9826595783233643\n",
      "2.9867935180664062\n",
      "2.978663444519043\n",
      "2.980307102203369\n",
      "2.985696792602539\n",
      "3.004775047302246\n",
      "2.9760184288024902\n",
      "3.0083892345428467\n",
      "3.0091071128845215\n",
      "2.989391326904297\n",
      "3.0120842456817627\n",
      "3.012751340866089\n",
      "2.9972786903381348\n",
      "2.9811174869537354\n",
      "2.992314338684082\n",
      "2.985110282897949\n",
      "3.012852668762207\n",
      "2.981001377105713\n",
      "2.9821712970733643\n",
      "3.013139009475708\n",
      "2.9663584232330322\n",
      "2.981017827987671\n",
      "2.9724667072296143\n",
      "3.028316020965576\n",
      "2.9859509468078613\n",
      "2.9852445125579834\n",
      "2.9730660915374756\n",
      "2.97074294090271\n",
      "3.003645420074463\n",
      "2.950408935546875\n",
      "2.988767623901367\n",
      "2.9844136238098145\n",
      "2.983642578125\n",
      "2.9622292518615723\n",
      "2.9821994304656982\n",
      "2.976696491241455\n",
      "2.985199451446533\n",
      "2.9637506008148193\n",
      "2.976078987121582\n",
      "2.963550567626953\n",
      "2.971813678741455\n",
      "2.976651906967163\n",
      "2.9820938110351562\n",
      "2.9609687328338623\n",
      "3.0001540184020996\n",
      "2.9952614307403564\n",
      "3.011777400970459\n",
      "2.984360456466675\n",
      "2.9700381755828857\n",
      "2.979315757751465\n",
      "2.9796886444091797\n",
      "2.9606120586395264\n",
      "3.01027774810791\n",
      "3.007647752761841\n",
      "3.002836227416992\n",
      "2.9749889373779297\n",
      "2.977198600769043\n",
      "3.0088582038879395\n",
      "2.9747095108032227\n",
      "3.012073516845703\n",
      "2.9722189903259277\n",
      "2.9965884685516357\n",
      "2.9662861824035645\n",
      "2.9704647064208984\n",
      "2.954150676727295\n",
      "2.977522373199463\n",
      "2.968698501586914\n",
      "2.972874641418457\n",
      "2.947195529937744\n",
      "2.959052085876465\n",
      "2.971423625946045\n",
      "2.959571599960327\n",
      "2.9577383995056152\n",
      "2.9555306434631348\n",
      "2.9304935932159424\n",
      "2.964137077331543\n",
      "3.004040241241455\n",
      "2.9731690883636475\n",
      "2.9828672409057617\n",
      "2.9509270191192627\n",
      "2.9246408939361572\n",
      "2.9495849609375\n",
      "2.987879514694214\n",
      "2.9357261657714844\n",
      "2.9705748558044434\n",
      "2.9308810234069824\n",
      "2.9574031829833984\n",
      "2.957078218460083\n",
      "2.977088451385498\n",
      "2.941434383392334\n",
      "2.927687168121338\n",
      "2.931187152862549\n",
      "2.977139949798584\n",
      "2.96315336227417\n",
      "2.9395081996917725\n",
      "2.9709789752960205\n",
      "2.957563638687134\n",
      "2.9304001331329346\n",
      "2.959115743637085\n",
      "2.963454246520996\n",
      "2.9672975540161133\n",
      "2.9595279693603516\n",
      "2.929687976837158\n",
      "2.9555399417877197\n",
      "2.961130380630493\n",
      "2.939481258392334\n",
      "2.9480321407318115\n",
      "2.9578826427459717\n",
      "2.913026809692383\n",
      "2.9277522563934326\n",
      "2.9287822246551514\n",
      "2.9498653411865234\n",
      "2.9446263313293457\n",
      "2.944823980331421\n",
      "2.9264211654663086\n",
      "2.950333595275879\n",
      "2.9279372692108154\n",
      "2.9277353286743164\n",
      "2.8955576419830322\n",
      "2.9266855716705322\n",
      "2.9356279373168945\n",
      "2.9370551109313965\n",
      "2.945242404937744\n",
      "2.9014954566955566\n",
      "2.9222068786621094\n",
      "2.9391160011291504\n",
      "2.9330484867095947\n",
      "2.954460620880127\n",
      "2.9536795616149902\n",
      "2.9390993118286133\n",
      "2.953794479370117\n",
      "2.943045139312744\n",
      "2.9281368255615234\n",
      "2.929041624069214\n",
      "2.9289941787719727\n",
      "2.9404025077819824\n",
      "2.921398878097534\n",
      "2.933147668838501\n",
      "2.9351322650909424\n",
      "2.9356298446655273\n",
      "2.9216747283935547\n",
      "2.9429988861083984\n",
      "2.929093360900879\n",
      "2.948090076446533\n",
      "2.940866231918335\n",
      "2.951442003250122\n",
      "2.9437127113342285\n",
      "2.9117016792297363\n",
      "2.933011531829834\n",
      "2.9204444885253906\n",
      "2.888033866882324\n",
      "2.9282774925231934\n",
      "2.939399480819702\n",
      "2.9129557609558105\n",
      "2.93172025680542\n",
      "2.9792044162750244\n",
      "2.922726631164551\n",
      "2.9266469478607178\n",
      "2.918956756591797\n",
      "2.956390380859375\n",
      "2.903925895690918\n",
      "2.9205873012542725\n",
      "2.934282064437866\n",
      "2.904599189758301\n",
      "2.9195032119750977\n",
      "2.920581817626953\n",
      "2.9343981742858887\n",
      "2.9249606132507324\n",
      "2.917905330657959\n",
      "2.953798532485962\n",
      "2.936413288116455\n",
      "2.9155664443969727\n",
      "2.9047765731811523\n",
      "2.930310010910034\n",
      "2.9149510860443115\n",
      "2.9134116172790527\n",
      "2.9024384021759033\n",
      "2.908926010131836\n",
      "2.9345808029174805\n",
      "2.909745931625366\n",
      "2.9069113731384277\n",
      "2.898768424987793\n",
      "2.924644708633423\n",
      "2.9065046310424805\n",
      "2.9239165782928467\n",
      "2.8951759338378906\n",
      "2.921517848968506\n",
      "2.9522697925567627\n",
      "2.908111095428467\n",
      "2.9087414741516113\n",
      "2.9522080421447754\n",
      "2.847156524658203\n",
      "2.9081501960754395\n",
      "2.943007469177246\n",
      "2.913816213607788\n",
      "2.8973257541656494\n",
      "2.9076216220855713\n",
      "2.923722267150879\n",
      "2.912529945373535\n",
      "2.945253849029541\n",
      "2.927746057510376\n",
      "2.924808979034424\n",
      "2.9400155544281006\n",
      "2.9182496070861816\n",
      "2.935060739517212\n",
      "2.8720827102661133\n",
      "2.926868438720703\n",
      "2.936549186706543\n",
      "2.925062656402588\n",
      "2.920717477798462\n",
      "2.928007125854492\n",
      "2.9529807567596436\n",
      "3.307645559310913\n",
      "3.3970882892608643\n",
      "3.273684024810791\n",
      "2.9898152351379395\n",
      "3.008902072906494\n",
      "3.097240686416626\n",
      "3.241992950439453\n",
      "3.230442523956299\n",
      "3.1944618225097656\n",
      "3.2427871227264404\n",
      "3.2679076194763184\n",
      "3.173706531524658\n",
      "3.1422815322875977\n",
      "3.1754021644592285\n",
      "3.110661745071411\n",
      "3.116121768951416\n",
      "3.27388334274292\n",
      "3.3363587856292725\n",
      "3.418672561645508\n",
      "3.462120294570923\n",
      "3.3211309909820557\n",
      "3.0662240982055664\n",
      "3.131303071975708\n",
      "3.419560432434082\n",
      "3.5042002201080322\n",
      "3.5562901496887207\n",
      "3.45361590385437\n",
      "3.3763346672058105\n",
      "3.272041082382202\n",
      "3.1819562911987305\n",
      "3.2185564041137695\n",
      "3.262889862060547\n",
      "3.215805768966675\n",
      "3.2175607681274414\n",
      "3.2915587425231934\n",
      "3.2167415618896484\n",
      "3.2261879444122314\n",
      "3.3569817543029785\n",
      "3.499190330505371\n",
      "3.4937593936920166\n",
      "3.466641426086426\n",
      "3.682650089263916\n",
      "4.240675926208496\n",
      "4.438948631286621\n",
      "4.549483299255371\n",
      "4.438755989074707\n",
      "4.43450927734375\n",
      "4.353618621826172\n",
      "4.027407169342041\n",
      "4.017925262451172\n",
      "3.7890470027923584\n",
      "3.697702407836914\n",
      "3.4469451904296875\n",
      "3.4331438541412354\n",
      "3.348646640777588\n",
      "3.3583950996398926\n",
      "3.3148515224456787\n",
      "3.319474220275879\n",
      "3.2858376502990723\n",
      "3.2813611030578613\n",
      "3.2330636978149414\n",
      "3.2122178077697754\n",
      "3.182443141937256\n",
      "3.1827480792999268\n",
      "3.1334352493286133\n",
      "3.191751480102539\n",
      "3.244743585586548\n",
      "3.23298716545105\n",
      "3.357339859008789\n",
      "3.2896361351013184\n",
      "3.3363304138183594\n",
      "3.2987442016601562\n",
      "3.290771484375\n",
      "3.2318053245544434\n",
      "3.244565010070801\n",
      "3.224524974822998\n",
      "3.3216347694396973\n",
      "3.3482842445373535\n",
      "3.3271069526672363\n",
      "3.2956223487854004\n",
      "3.233274459838867\n",
      "3.2241854667663574\n",
      "3.2216782569885254\n",
      "3.2269132137298584\n",
      "3.3141207695007324\n",
      "3.3526926040649414\n",
      "3.442075490951538\n",
      "3.3785183429718018\n",
      "3.400599956512451\n",
      "3.3845746517181396\n",
      "3.350831985473633\n",
      "3.3503456115722656\n",
      "3.3825807571411133\n",
      "3.40779185295105\n",
      "3.355767250061035\n",
      "3.4260709285736084\n",
      "3.497663974761963\n",
      "3.497788190841675\n",
      "3.530688762664795\n",
      "3.543680191040039\n",
      "3.5589728355407715\n",
      "3.5838418006896973\n",
      "3.6257619857788086\n",
      "3.628338575363159\n",
      "3.5967178344726562\n",
      "3.571319580078125\n",
      "3.5286149978637695\n",
      "3.539217948913574\n",
      "3.554781198501587\n",
      "3.545579433441162\n",
      "3.540680170059204\n",
      "3.539005756378174\n",
      "3.547528028488159\n",
      "3.5448601245880127\n",
      "3.4907124042510986\n",
      "3.5063815116882324\n",
      "3.476634979248047\n",
      "3.4822263717651367\n",
      "3.512179374694824\n",
      "3.484947919845581\n",
      "3.485508441925049\n",
      "3.474205493927002\n",
      "3.4008395671844482\n",
      "3.403430700302124\n",
      "3.4115242958068848\n",
      "3.395400047302246\n",
      "3.3948817253112793\n",
      "3.4098851680755615\n",
      "3.3634941577911377\n",
      "3.428508758544922\n",
      "3.368171215057373\n",
      "3.435455560684204\n",
      "3.4957244396209717\n",
      "3.558830738067627\n",
      "3.528438091278076\n",
      "3.5360395908355713\n",
      "3.5289876461029053\n",
      "3.4609246253967285\n",
      "3.4245874881744385\n",
      "3.413952350616455\n",
      "3.407224655151367\n",
      "3.41902756690979\n",
      "3.420349597930908\n",
      "3.395047187805176\n",
      "3.444185495376587\n",
      "3.434680461883545\n",
      "3.428252696990967\n",
      "3.460162878036499\n",
      "3.4707517623901367\n",
      "3.470707416534424\n",
      "3.4946460723876953\n",
      "3.509641647338867\n",
      "3.607212543487549\n",
      "3.635533571243286\n",
      "3.646291971206665\n",
      "3.6369054317474365\n",
      "3.644483804702759\n",
      "3.914444923400879\n",
      "4.209105491638184\n",
      "4.278805732727051\n",
      "4.60133171081543\n",
      "4.550246715545654\n",
      "4.359588623046875\n",
      "4.523441314697266\n",
      "4.4508137702941895\n",
      "8.526776313781738\n",
      "10.375629425048828\n",
      "7.61317777633667\n",
      "8.450974464416504\n",
      "3.809026002883911\n",
      "4.0671539306640625\n",
      "9.165457725524902\n",
      "16.974647521972656\n",
      "6.692568778991699\n",
      "13.543585777282715\n",
      "5.018747806549072\n",
      "4.095049858093262\n",
      "12.641790390014648\n",
      "11.951534271240234\n",
      "11.244609832763672\n",
      "12.734999656677246\n",
      "11.720298767089844\n",
      "12.184876441955566\n",
      "7.027276039123535\n",
      "12.241293907165527\n",
      "12.100873947143555\n",
      "6.272185325622559\n",
      "53.05341339111328\n",
      "83.60020446777344\n",
      "13.028775215148926\n",
      "13.157219886779785\n",
      "11.624832153320312\n",
      "12.273344993591309\n",
      "88.94148254394531\n",
      "84.3958511352539\n",
      "7.883426666259766\n",
      "17.458147048950195\n",
      "17.71549415588379\n",
      "17.189119338989258\n",
      "17.48640251159668\n",
      "17.299911499023438\n",
      "17.155216217041016\n",
      "16.544612884521484\n",
      "16.117544174194336\n",
      "14.994490623474121\n",
      "14.692497253417969\n",
      "13.888086318969727\n",
      "13.00503158569336\n",
      "11.95395565032959\n",
      "11.535762786865234\n",
      "10.866580963134766\n",
      "10.124067306518555\n",
      "9.399319648742676\n",
      "8.7182035446167\n",
      "7.977069854736328\n",
      "7.401047706604004\n",
      "6.8792724609375\n",
      "6.401033401489258\n",
      "5.929963111877441\n",
      "5.579856872558594\n",
      "5.3003034591674805\n",
      "5.054643630981445\n",
      "4.799112319946289\n",
      "4.627351760864258\n",
      "4.476669788360596\n",
      "4.358046054840088\n",
      "4.239230155944824\n",
      "4.14689302444458\n",
      "4.069899559020996\n",
      "4.034157752990723\n",
      "3.960620403289795\n",
      "3.9238979816436768\n",
      "3.8580856323242188\n",
      "3.844259023666382\n",
      "3.814807891845703\n",
      "3.7641661167144775\n",
      "3.6727311611175537\n",
      "3.638960838317871\n",
      "3.632833957672119\n",
      "3.5521321296691895\n",
      "3.521944046020508\n",
      "3.5042290687561035\n",
      "3.522101402282715\n",
      "3.490649461746216\n",
      "3.4694809913635254\n",
      "3.461665153503418\n",
      "3.4491066932678223\n",
      "3.4131884574890137\n",
      "3.4124622344970703\n",
      "3.4119691848754883\n",
      "3.526538848876953\n",
      "3.460529327392578\n",
      "3.4311113357543945\n",
      "3.4695162773132324\n",
      "3.4352059364318848\n",
      "3.484907865524292\n",
      "3.4004948139190674\n",
      "3.4508376121520996\n",
      "3.470881462097168\n",
      "3.413872003555298\n",
      "3.4144794940948486\n",
      "3.424046039581299\n",
      "3.4052600860595703\n",
      "3.398764133453369\n",
      "3.383354663848877\n",
      "3.41812801361084\n",
      "3.381258964538574\n",
      "3.326913595199585\n",
      "3.374562978744507\n",
      "3.42342472076416\n",
      "3.3928799629211426\n",
      "3.4367024898529053\n",
      "3.317265748977661\n",
      "3.376136302947998\n",
      "3.3202500343322754\n",
      "3.343956708908081\n",
      "3.326759099960327\n",
      "3.3353326320648193\n",
      "3.337693214416504\n",
      "3.3263702392578125\n",
      "3.356586456298828\n",
      "3.349378824234009\n",
      "3.362959384918213\n",
      "3.339115619659424\n",
      "3.362301826477051\n",
      "3.288905143737793\n",
      "3.3277053833007812\n",
      "3.345752239227295\n",
      "3.3667092323303223\n",
      "3.322878837585449\n",
      "3.341214418411255\n",
      "3.386749744415283\n",
      "3.3140525817871094\n",
      "3.332704782485962\n",
      "3.3222146034240723\n",
      "3.3029322624206543\n",
      "3.349461555480957\n",
      "3.351661205291748\n",
      "3.3144185543060303\n",
      "3.3106284141540527\n",
      "3.3412675857543945\n",
      "3.334010601043701\n",
      "3.3109683990478516\n",
      "3.3089685440063477\n",
      "3.322105884552002\n",
      "3.2969698905944824\n",
      "3.3483235836029053\n",
      "3.3387043476104736\n",
      "3.3436150550842285\n",
      "3.3407227993011475\n",
      "3.3258800506591797\n",
      "3.300628662109375\n",
      "3.3089616298675537\n",
      "3.326052188873291\n",
      "3.333676815032959\n",
      "3.337869644165039\n",
      "3.3176021575927734\n",
      "3.368934154510498\n",
      "3.317673921585083\n",
      "3.3389997482299805\n",
      "3.3303608894348145\n",
      "3.3300318717956543\n",
      "3.334714412689209\n",
      "3.3238048553466797\n",
      "3.3223109245300293\n",
      "3.336740493774414\n",
      "3.3061747550964355\n",
      "3.348646879196167\n",
      "3.340660333633423\n",
      "3.3390021324157715\n",
      "3.359961748123169\n",
      "3.415344715118408\n",
      "3.39776873588562\n",
      "3.425638437271118\n",
      "3.4250073432922363\n",
      "3.4144153594970703\n",
      "3.4477145671844482\n",
      "3.438443422317505\n",
      "3.4242208003997803\n",
      "3.401188373565674\n",
      "3.3974781036376953\n",
      "3.4118547439575195\n",
      "3.4315667152404785\n",
      "3.413461923599243\n",
      "3.4245567321777344\n",
      "3.4424328804016113\n",
      "3.446899890899658\n",
      "3.4366893768310547\n",
      "3.4539270401000977\n",
      "3.453486919403076\n",
      "3.457805871963501\n",
      "3.484553813934326\n",
      "3.522341012954712\n",
      "3.5160346031188965\n",
      "3.491692066192627\n",
      "3.4665699005126953\n",
      "3.4492034912109375\n",
      "3.461646556854248\n",
      "3.4354724884033203\n",
      "3.4378597736358643\n",
      "3.447394847869873\n",
      "3.448695659637451\n",
      "3.436589241027832\n",
      "3.4504008293151855\n",
      "3.481146812438965\n",
      "3.4893007278442383\n",
      "3.5064592361450195\n",
      "3.524777889251709\n",
      "3.5007030963897705\n",
      "3.516322612762451\n",
      "3.530580520629883\n",
      "3.51952862739563\n",
      "3.5087084770202637\n",
      "3.4852523803710938\n",
      "3.5328564643859863\n",
      "3.502333402633667\n",
      "3.5296335220336914\n",
      "3.532609224319458\n",
      "3.5051605701446533\n",
      "3.481842041015625\n",
      "3.5003931522369385\n",
      "3.4740848541259766\n",
      "3.4748904705047607\n",
      "3.4922142028808594\n",
      "3.489562511444092\n",
      "3.4891719818115234\n",
      "3.471508502960205\n",
      "3.4712777137756348\n",
      "3.477346658706665\n",
      "3.465066909790039\n",
      "3.4883785247802734\n",
      "3.4773659706115723\n",
      "3.4961202144622803\n",
      "3.4912919998168945\n",
      "3.5168678760528564\n",
      "3.491790771484375\n",
      "3.483997106552124\n",
      "3.482524871826172\n",
      "3.4748153686523438\n",
      "3.4789974689483643\n",
      "3.4746670722961426\n",
      "3.4639453887939453\n",
      "3.4836013317108154\n",
      "3.470547914505005\n",
      "3.4759812355041504\n",
      "3.488807201385498\n",
      "3.4686365127563477\n",
      "3.4675707817077637\n",
      "3.4935035705566406\n",
      "3.4901785850524902\n",
      "3.472501754760742\n",
      "3.4953904151916504\n",
      "3.5031609535217285\n",
      "3.514847755432129\n",
      "3.494882106781006\n",
      "3.5426838397979736\n",
      "3.5845518112182617\n",
      "3.71736216545105\n",
      "4.0045881271362305\n",
      "3.9320709705352783\n",
      "4.051329612731934\n",
      "3.923996686935425\n",
      "3.945451259613037\n",
      "3.917039394378662\n",
      "4.299604415893555\n",
      "4.565805435180664\n",
      "5.4226837158203125\n",
      "5.7306809425354\n",
      "5.993221759796143\n",
      "5.884570121765137\n",
      "5.58730411529541\n",
      "5.656748294830322\n",
      "5.652626991271973\n",
      "6.01364278793335\n",
      "5.933121204376221\n",
      "5.824423789978027\n",
      "121.75019836425781\n",
      "326.5890197753906\n",
      "321.9066162109375\n",
      "441.6209411621094\n",
      "455.9002685546875\n",
      "459.0738830566406\n",
      "464.4960021972656\n",
      "461.9969177246094\n",
      "458.28887939453125\n",
      "461.4393615722656\n",
      "461.34619140625\n",
      "460.8582763671875\n",
      "461.5476989746094\n",
      "460.254150390625\n",
      "468.1567077636719\n",
      "461.797607421875\n",
      "461.1820068359375\n",
      "458.54339599609375\n",
      "466.10821533203125\n",
      "457.5430603027344\n",
      "461.37255859375\n",
      "461.0162048339844\n",
      "460.3726806640625\n",
      "465.3695068359375\n",
      "469.08660888671875\n",
      "468.7749938964844\n",
      "470.3226013183594\n",
      "464.70611572265625\n",
      "466.2624206542969\n",
      "466.2768859863281\n",
      "466.7515563964844\n",
      "468.6877746582031\n",
      "465.0416259765625\n",
      "465.2316589355469\n",
      "465.9913635253906\n",
      "466.201171875\n",
      "470.447509765625\n",
      "466.568603515625\n",
      "471.009521484375\n",
      "463.8310241699219\n",
      "464.8088073730469\n",
      "462.6959533691406\n",
      "458.69482421875\n",
      "467.4507141113281\n",
      "473.1870422363281\n",
      "6.886229991912842\n",
      "31.72240447998047\n",
      "459.88397216796875\n",
      "463.4163818359375\n",
      "460.7519836425781\n",
      "459.2008361816406\n",
      "457.4969482421875\n",
      "461.6772155761719\n",
      "463.62030029296875\n",
      "464.6409912109375\n",
      "459.08123779296875\n",
      "456.39013671875\n",
      "459.45086669921875\n",
      "463.5637512207031\n",
      "458.0328063964844\n",
      "457.21551513671875\n",
      "455.66790771484375\n",
      "463.2571105957031\n",
      "456.2557373046875\n",
      "466.75006103515625\n",
      "465.4775390625\n",
      "465.46533203125\n",
      "471.6910400390625\n",
      "471.0158386230469\n",
      "466.9479675292969\n",
      "475.7118835449219\n",
      "466.4071350097656\n",
      "462.5068359375\n",
      "460.09149169921875\n",
      "455.8339538574219\n",
      "239.366455078125\n",
      "54.01496505737305\n",
      "36.08201217651367\n",
      "33.62114715576172\n",
      "12.862256050109863\n",
      "16.903060913085938\n",
      "16.494831085205078\n",
      "6.889577865600586\n",
      "4.089321136474609\n",
      "3.8159289360046387\n",
      "3.84547758102417\n",
      "3.876173734664917\n",
      "3.8726487159729004\n",
      "3.8671793937683105\n",
      "3.863924980163574\n",
      "3.8653616905212402\n",
      "3.862351894378662\n",
      "3.862517833709717\n",
      "3.8240952491760254\n",
      "3.714172124862671\n",
      "3.7405974864959717\n",
      "4.216614723205566\n",
      "3.6749348640441895\n",
      "4.3333635330200195\n",
      "4.308475494384766\n",
      "4.818863391876221\n",
      "4.2810821533203125\n",
      "5.100203037261963\n",
      "10.716535568237305\n",
      "31.43429946899414\n",
      "15.729671478271484\n",
      "25.48320960998535\n",
      "36.712947845458984\n",
      "46.808616638183594\n",
      "32.45067596435547\n",
      "39.729496002197266\n",
      "32.20555114746094\n",
      "31.753616333007812\n",
      "32.919456481933594\n",
      "28.47979736328125\n",
      "51.641658782958984\n",
      "72.9779052734375\n",
      "63.614410400390625\n",
      "83.39692687988281\n",
      "66.92787170410156\n",
      "71.20064544677734\n",
      "69.09466552734375\n",
      "69.999267578125\n",
      "59.32537841796875\n",
      "75.422607421875\n",
      "60.262420654296875\n",
      "39.7828254699707\n",
      "32.97061538696289\n",
      "32.153133392333984\n",
      "25.73063850402832\n",
      "23.103334426879883\n",
      "30.11474609375\n",
      "19.92380142211914\n",
      "7.593640327453613\n",
      "5.388160228729248\n",
      "4.198966979980469\n",
      "3.7804572582244873\n",
      "3.778228759765625\n",
      "3.83657169342041\n",
      "3.852323532104492\n",
      "3.817347526550293\n",
      "3.8302090167999268\n",
      "3.8355751037597656\n",
      "3.842886209487915\n",
      "3.814931631088257\n",
      "3.8491427898406982\n",
      "3.838444232940674\n",
      "3.829413652420044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m update_display()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     27\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:292\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    288\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    293\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    296\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    297\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 6) Train / eval functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits ,attn_weights = model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        per_token_loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            yb.view(-1),\n",
    "            reduction='none'  # This gives the raw loss per token\n",
    "        \n",
    "        ).reshape(B,T) # Shape: (B*S,)\n",
    "        loss = per_token_loss.mean()\n",
    "        loss_cpu = per_token_loss.cpu().detach().numpy()\n",
    "        tokens = [[itos[idx] for idx in seq.tolist()] for seq in yb]\n",
    "        attn_cpu = attn_weights.cpu().detach().numpy()\n",
    "        update_framebuffer(attn_cpu, loss_cpu, loss.item(), tokens)\n",
    "        update_display()\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 7) Run training\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_epoch() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f659500dadd4a699e4c462c3d2e88a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "CHAR_WIDTH = 8  # Font size 8 for token rendering\n",
    "CHAR_HEIGHT = 11\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "LOSS_BAR_HEIGHT = 32\n",
    "EWMA_HEIGHT = 32  # Increased to accommodate large text (previously 32)\n",
    "\n",
    "# Full-resolution framebuffer dimensions\n",
    "container_width = CHAR_WIDTH * SEQ_LEN  # 1024 pixels\n",
    "container_height = CHAR_HEIGHT * BATCH_SIZE  # 176 pixels\n",
    "total_height = container_height + LOSS_BAR_HEIGHT + EWMA_HEIGHT  # Adjusted for larger EWMA\n",
    "\n",
    "# Final scaled-down dimensions\n",
    "scaled_width = container_width   # 512 pixels\n",
    "scaled_height = total_height  # 170 pixels\n",
    "\n",
    "# Initialize framebuffer\n",
    "framebuffer = np.zeros((total_height, container_width, 3), dtype=np.uint8)\n",
    "\n",
    "# EWMA storage\n",
    "ticker_history = np.zeros(SEQ_LEN, dtype=np.float32)  # Stock ticker moving buffer\n",
    "loss_memory = 0.0\n",
    "# Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(\"DejaVuSansMono.ttf\", 8)  # Monospaced font\n",
    "    font_large = ImageFont.truetype(\"DejaVuSansMono.ttf\", 64)  # Large EWMA display\n",
    "except:\n",
    "    font = ImageFont.load_default()\n",
    "    font_large = font\n",
    "\n",
    "# --- Color Mapping Functions ---\n",
    "def get_flame_color(val):\n",
    "    \"\"\"Map a normalized value to a flame-like color.\"\"\"\n",
    "    return np.array([int(val * 255), int(val * 0.5 * 255), 0], dtype=np.uint8)\n",
    "\n",
    "# --- IPython Display Setup ---\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "def get_dynamic_color(attn_val, loss_val):\n",
    "    \"\"\"\n",
    "    Compute a dynamic color transition between flame orange (uncertain) and phosphor green (confident).\n",
    "\n",
    "    attn_val: Normalized attention value (0 to 1)\n",
    "    loss_val: Normalized loss value (0 to 1, inverted as 1 - loss)\n",
    "\n",
    "    Returns an RGB color as a NumPy array.\n",
    "    colors late in training will often be red. this is suggested to swap out for get_flame_color\n",
    "    but only on fine tuning on new data.\n",
    "    \"\"\"\n",
    "    certainty = 1 - loss_val  # High certainty = low loss\n",
    "\n",
    "    # Define RGB endpoints\n",
    "    orange = np.array([attn_val * 255, attn_val * 0.5 * 255, 0], dtype=np.uint8)   # Uncertain (High Loss)\n",
    "    green = np.array([attn_val * 0.5 * 255, attn_val * 255, attn_val * 0.25 * 255], dtype=np.uint8)  # Confident (Low Loss)\n",
    "\n",
    "    # Interpolate based on certainty (0 = uncertain/orange, 1 = confident/green)\n",
    "    color = (certainty * green) + ((1 - certainty) * orange)\n",
    "\n",
    "    return color.astype(np.uint8)\n",
    "def normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    min_val = np.min(x, axis=1, keepdims=True)\n",
    "    max_val = np.max(x, axis=1, keepdims=True)\n",
    "    scale = max_val - min_val\n",
    "    return (x - min_val) / (scale + 1e-16)\n",
    "    \n",
    "# --- Framebuffer Update Function ---\n",
    "def update_framebuffer(attn_weights, token_losses, current_loss, tokens):\n",
    "    token_losses = normalize_rows(token_losses)\n",
    "    attn_weights = normalize_rows(attn_weights)\n",
    "    \"\"\"Render the text grid with coloration based on attn * inverse loss.\"\"\"\n",
    "    global framebuffer, loss_history, ticker_history, loss_memory\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "\n",
    "    # Create image buffer\n",
    "    img = Image.new(\"RGB\", (container_width, total_height), (0, 0, 0))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Render text with colored intensity\n",
    "    char_positions = [\n",
    "        (col * CHAR_WIDTH, row * CHAR_HEIGHT + EWMA_HEIGHT + LOSS_BAR_HEIGHT, tokens[row][col])\n",
    "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
    "    ]\n",
    "    colors = [\n",
    "        tuple(get_dynamic_color(attn_weights[row, col], token_losses[row, col]))\n",
    "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
    "    ]\n",
    "    for (x, y, char), color in zip(char_positions, colors):\n",
    "        draw.text((x, y), char, font=font, fill=color)\n",
    "\n",
    "\n",
    "    etcerta = 0.367879441  # Constant used in update rule\n",
    "    et = 1 - etcerta\n",
    "    update = loss_memory * et + np.minimum(12, np.maximum(current_loss , 0)) * etcerta\n",
    "    loss_memory = loss_memory * et + update * etcerta\n",
    "    # --- EWMA Display (LARGE FONT) ---\n",
    "    ewma = loss_memory\n",
    "    ewma_text = f\"{ewma:.4f}\"\n",
    "    draw.text((container_width-128, 0), ewma_text, font_size=32, fill=(65,255, 125))\n",
    "\n",
    "    # --- Moving Loss Ticker Graph ---\n",
    "    ticker_history = np.roll(ticker_history, -1)  # Shift left\n",
    "    ticker_history[-1] = current_loss  # Insert new loss on the right\n",
    "\n",
    "    # Rescale ticker dynamically like a stock ticker (normalize to min-max range)\n",
    "    min_loss = np.min(ticker_history)\n",
    "    max_loss = np.max(ticker_history)\n",
    "    range_loss = max_loss - min_loss if max_loss != min_loss else 1\n",
    "    normalized_ticker = (ticker_history - min_loss) / range_loss\n",
    "\n",
    "    # Draw ticker graph line\n",
    "    # Optimized drawing loop (fewer function calls)\n",
    "    y_vals = EWMA_HEIGHT + (1 - normalized_ticker) * LOSS_BAR_HEIGHT\n",
    "    x_vals = np.arange(SEQ_LEN) * CHAR_WIDTH\n",
    "    for i in range(SEQ_LEN - 1):\n",
    "        draw.line([(x_vals[i], y_vals[i]), (x_vals[i + 1], y_vals[i + 1])], fill=(0, 255, 255), width=2)\n",
    "\n",
    "    framebuffer = np.array(img)\n",
    "\n",
    "# --- IPython Display Update Function ---\n",
    "def update_display():\n",
    "    \"\"\"Show the framebuffer, scaled down by half using ipywidgets.\"\"\"\n",
    "    img = Image.fromarray(framebuffer)\n",
    "    img_resized = img.resize((scaled_width, scaled_height), Image.LANCZOS)\n",
    "\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        display(img_resized)\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.select(0); Jupyter.notebook.execute_cell();</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<script>Jupyter.notebook.select(0); Jupyter.notebook.execute_cell();</script>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted power-law parameters:\n",
      "a = 10.0000, b = 0.0000, c = 5.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your actual loss values\n",
    "# Assume: scores = np.array([...])  # shape (T,)\n",
    "T = len(losses)\n",
    "steps = np.arange(1, T + 1)\n",
    "\n",
    "# Define power-law function\n",
    "def power_law(t, a, b, c):\n",
    "    return a * t**(-b) + c\n",
    "\n",
    "# Fit the curve\n",
    "popt, _ = curve_fit(power_law, steps, losses, bounds=([0, 0, 0], [10, 2, 5]))\n",
    "a_fit, b_fit, c_fit = popt\n",
    "\n",
    "# Generate fitted curve\n",
    "fitted_loss = power_law(steps, *popt)\n",
    "\n",
    "# Plot actual vs fitted\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(steps, losses, label=\"Actual Loss\", alpha=0.6)\n",
    "plt.plot(steps, fitted_loss, label=f\"Fitted: {a_fit:.2f}Â·t^(-{b_fit:.2f}) + {c_fit:.2f}\", color='red')\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Power-law Fit to Training Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: print constants\n",
    "print(f\"Fitted power-law parameters:\\na = {a_fit:.4f}, b = {b_fit:.4f}, c = {c_fit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "losses_array = np.asarray(losses)\n",
    "nan_indices = np.where(np.isnan(losses_array))\n",
    "losses_array[nan_indices] = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3381be060>]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'nan.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::exp        20.61%        1.775s        20.61%        1.775s       9.148ms       4.98 Gb       4.98 Gb           194  \n",
      "                                              aten::sub        20.23%        1.741s        20.23%        1.742s       8.063ms       6.04 Gb       6.04 Gb           216  \n",
      "                                              aten::mul        10.53%     906.319ms        10.58%     910.470ms     922.462us       9.30 Gb       9.30 Gb           987  \n",
      "                                              aten::sum         8.89%     764.958ms         9.40%     809.425ms       2.353ms     261.82 Mb     261.82 Mb           344  \n",
      "                                              aten::add         7.67%     660.538ms         7.68%     661.124ms       1.520ms       5.24 Gb       5.24 Gb           435  \n",
      "                                              aten::bmm         6.40%     550.685ms         6.55%     564.192ms       3.918ms       1.90 Gb       1.90 Gb           144  \n",
      "                                              aten::div         4.99%     429.709ms         5.00%     430.102ms     890.480us       3.54 Gb       3.54 Gb           483  \n",
      "                                         aten::softplus         3.00%     258.266ms         3.00%     258.266ms       1.645ms      13.32 Mb      13.32 Mb           157  \n",
      "                                              aten::max         2.57%     220.923ms         2.57%     220.924ms       7.618ms     804.00 Mb     804.00 Mb            29  \n",
      "                                          <backward op>         1.90%     163.764ms        19.51%        1.679s      14.730ms      -3.84 Gb      -4.79 Gb           114  \n",
      "                                                forward         1.65%     141.699ms        67.73%        5.831s        5.831s       7.24 Gb      -4.73 Gb             1  \n",
      "                                        _FusedLogSumExp         1.14%      98.542ms        48.80%        4.201s     175.061ms       4.68 Gb      -5.20 Gb            24  \n",
      "torch::jit::(anonymous namespace)::DifferentiableGra...         1.03%      88.913ms        20.54%        1.768s      15.510ms      -3.84 Gb    -512.00 Kb           114  \n",
      "                                _FusedLogSumExpBackward         1.02%      88.032ms         9.32%     802.714ms      50.170ms       4.14 Gb      -4.14 Gb            16  \n",
      "                                          aten::normal_         0.85%      73.511ms         0.85%      73.511ms       3.676ms           0 b           0 b            20  \n",
      "                                            aten::where         0.72%      61.975ms         0.72%      62.053ms     564.122us    -802.28 Mb    -802.28 Mb           110  \n",
      "                                               aten::mm         0.72%      61.558ms         0.72%      61.575ms     603.676us     205.49 Mb     205.49 Mb           102  \n",
      "autograd::engine::evaluate_function: _FusedLogSumExp...         0.66%      57.065ms         9.99%     859.779ms      53.736ms    -532.00 Mb      -4.66 Gb            16  \n",
      "autograd::engine::evaluate_function: torch::jit::(an...         0.59%      50.949ms        21.14%        1.820s      15.966ms      -6.63 Gb      -2.79 Gb           114  \n",
      "                                              aten::log         0.57%      49.077ms         0.57%      49.077ms       1.363ms     301.00 Mb     301.00 Mb            36  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 8.609s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.profiler\n",
    "device=\"cpu\"\n",
    "def profile_one_iter():\n",
    "    model.train()\n",
    "    xb, yb = next(iter(train_loader))\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logdir'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits , attention= model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=20))\n",
    "profile_one_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9GiSvYAnJnv",
    "outputId": "d77d34f7-55ad-4f2b-cac8-90dd2159d95f"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_50188/1979086850.py\", line 1098, in <forward op>\n        #stack 0::2 as embeddings, 1::2 as zeros for positional embeddings\n        P = tokens_to_simplex(idx, self.vocab_size)       # (batch, seq, V)\n        embeddings = self.convex_embed(P)                          # (batch, seq, d_model)\n                     ~~~~~~~~~~~~~~~~~ <--- HERE\n        x = torch.stack([embeddings, torch.zeros_like(embeddings)], dim=-1).reshape(idx.shape[0], idx.shape[1], self.embed_dim)\n    \n  File \"/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_50188/1979086850.py\", line 993, in forward\n        h1 = self.act(h0 + self.Wz(h0))       # convex skip\n        raw = self.out(h1)                    # â†’ (batch, seq, out_dim)\n        return self.contraction(raw)          # attenuate each cha\n               ~~~~~~~~~~~~~~~~ <--- HERE\n  File \"/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_50188/1979086850.py\", line 918, in forward\n    \n        Î³ = torch.sigmoid(self.rho)           # (0,1)\n        x_hat = (x - self.mu) / (self.sigma + self.eps)\n                 ~~~~~~~~~~~ <--- HERE\n        return x_hat * Î³ + self.beta\nRuntimeError: The size of tensor a (20) must match the size of tensor b (256) at non-singleton dimension 1\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[32m     27\u001b[39m   input_ids = generated[:, -block_size:]        \u001b[38;5;66;03m# casual block\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m   logits,a = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m   logits = logits[:, -\u001b[32m1\u001b[39m, :] / temperature\n\u001b[32m     31\u001b[39m   \u001b[38;5;66;03m# topâ€‘k mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mRuntimeError\u001b[39m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_50188/1979086850.py\", line 1098, in <forward op>\n        #stack 0::2 as embeddings, 1::2 as zeros for positional embeddings\n        P = tokens_to_simplex(idx, self.vocab_size)       # (batch, seq, V)\n        embeddings = self.convex_embed(P)                          # (batch, seq, d_model)\n                     ~~~~~~~~~~~~~~~~~ <--- HERE\n        x = torch.stack([embeddings, torch.zeros_like(embeddings)], dim=-1).reshape(idx.shape[0], idx.shape[1], self.embed_dim)\n    \n  File \"/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_50188/1979086850.py\", line 993, in forward\n        h1 = self.act(h0 + self.Wz(h0))       # convex skip\n        raw = self.out(h1)                    # â†’ (batch, seq, out_dim)\n        return self.contraction(raw)          # attenuate each cha\n               ~~~~~~~~~~~~~~~~ <--- HERE\n  File \"/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_50188/1979086850.py\", line 918, in forward\n    \n        Î³ = torch.sigmoid(self.rho)           # (0,1)\n        x_hat = (x - self.mu) / (self.sigma + self.eps)\n                 ~~~~~~~~~~~ <--- HERE\n        return x_hat * Î³ + self.beta\nRuntimeError: The size of tensor a (20) must match the size of tensor b (256) at non-singleton dimension 1\n\n"
     ]
    }
   ],
   "source": [
    "def fenchel_decode(logits, tau=1.0, iters=3):\n",
    "    \"\"\"Fenchelâ€‘dual KLâ€‘regularised projection of -logits (energy).\"\"\"\n",
    "    energy = -logits                        # (B,V)\n",
    "    p = torch.full_like(energy, 1.0 / energy.size(-1))  # uniform start\n",
    "    for _ in range(iters):\n",
    "        p = torch.softmax((-energy / tau) + p.log(), dim=-1)\n",
    "    return p\n",
    "    \n",
    "# --- generation ------------------------------------------------------\n",
    "use_fenchel   = False          # flip to compare\n",
    "tau           = 1.0           # Î»  (temperature analogue)\n",
    "max_new_tokens = 1024\n",
    "top_k          = 25\n",
    "block_size     = 256\n",
    "temperature    = 1.0\n",
    "\n",
    "bcontext_str = \"To be, or not to be,\"\n",
    "context_ids = torch.tensor([[ stoi[c] for c in bcontext_str ]],\n",
    "                           dtype=torch.long)\n",
    "context_ids = context_ids.to(device)\n",
    "\n",
    "\n",
    "generated = context_ids.clone()  # (1,T0)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for _ in range(max_new_tokens):\n",
    "    input_ids = generated[:, -block_size:]        # casual block\n",
    "    logits,a = model(input_ids)\n",
    "\n",
    "    logits = logits[:, -1, :] / temperature\n",
    "    # topâ€‘k mask\n",
    "    if top_k is not None:\n",
    "        v, _ = torch.topk(logits, top_k)\n",
    "        logits[logits < v[:, [-1]]] = -1e10\n",
    "\n",
    "    if use_fenchel:\n",
    "        probs = fenchel_decode(logits, tau=tau, iters=3)\n",
    "    else:\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    next_id = torch.multinomial(probs, num_samples=1)   # (1,1)\n",
    "    generated = torch.cat([generated, next_id], dim=1)\n",
    "\n",
    "print('> ', ''.join(itos[i] for i in generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x30d0d5a00>]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
