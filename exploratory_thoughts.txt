vibe coded in a week. Are we AGI yet?
problem of epistemic stability under parameter updates â€” a consolidation-versus-redistribution dilemma.  a desired inductive bias: new information should localize to new regions of the representation space, without overwriting or diffusing prior stable representations ("truth stays in one place").

This can be reframed as:

Preserve representational locality: learned features should not shift arbitrarily with continued training.
Avoid representational drift: old knowledge shouldn't be constantly re-encoded elsewhere.
Encourage compositional expansion: new knowledge gets appended into unused parts of the model, not grafted onto or blended into existing parts.

This is a necessity.

routing sparsity, sadly. is only so achievable.
But what I could do is set a weight history, and make weights resist retraining.
this could be done in an inverse manner- the more number of times a weight is adjusted without the gradient sign changing, the more we accumulate a "resistant" value.
this "resistant" value is stored alongside the data.
the very first time the model tries to flip with a gradient thats the opposite, it severely dampens the shift, dividing it by resistant. the more times the model moves the "other" way- in other words, doesn't flip again, but continues to try to move away from the previous minima-
we deplete this resistant but store it in another resistant.
In other words, we track two directions- because a 1d gradient moves in two dimensions-
And accumulate or deplete resistance on successive moves.

as the model moves away from the minima, it will then resist being moved back.
this is sort of like a viscosity thing? we could apply optimal transport theory to this.
This allows weights to freeze into forms that can readily deform when new patterns emerge.
there's also elasticity involved. elasticity would take the form of a third and fourth variable.
ie, when the weight is being pushed around a lot, over iterations, its a hot spot. the hotter a weight is, the more elasticity it should have- it should be hot. fluid. This fluidity would not adjust the viscosity of the first mechanism, rather, it sets the initial flexibility and then the transport viscosity parameters restrict it further. so going into depth- this deeper fluidity set by updates over time- weights cool. in other words, they have a variable with an initial temp, and the temp is ablated over time if the weight isnt moved. as the weight gets colder, it gets harder to move. Thus we have both viscosity that resists being moved back and forth and viscosity that impedes attempts to update weights that have consolidated without strong effort. Once the model's petals have been fully trained, we could apply a per-petal temp divider constant that acts like a cryogenic freezer, causing those petals to be very, very, very hard to shift, without actually clamping anything, thus preserving our convex and c1 properties- models are frozen via chrysalis. 
We can automatically set this temp divider when a certain percentage of the weights in a petal are established, and automatically adjust the model to add more petals when a certain percentage of petals are frozen. there's probably statistical theory here. student-t with 1/e governs the ideal partitioning of noise to signal, but i forget what the actual partition there is in terms of statistical distribution.  but we could say, when the petals param elasticity has dropped below the level set as "crystalization"- here drawing on pressure physics and water as steam -> liquid -> crystal - in other words inferring a temp to mean "crystalized" - how many of the atoms in a body of water as a percentage must be below a certain temp for it to form ice, in a vacuum? actually i guess ice doesnt form in a vacuum, not a true one. So we'd have a relationship of pressure to temp. pressure could be said to be the relative force on the weights to change, vs their resistance to change. so we could estimate the learning rate is equal to the pressure, so the pressure is the learning rate, and the temp variable would then be considered against it, and we ask how much force is needed to shift the param at the current temp and learning rate, where that force is considered energy. That would tell us if the atom- or weight in this case- is in a frozen state. Then, if, say, 1-1/e or some other statistical percentage of the atoms- remember earlier i ask how many must be below a temp? if that percentage is reached, we consider the petal frozen and mark it as such. petals can be unfrozen manually by unsetting this, but still must be "warmed". 

Then, we use statistical theory involving 1/(n-1) and the petals to determine if we have enough petals unfrozen, and if not, we expand the model by adding more petals in place to the hull.


 increasing the number of heads does affect the coordinate system of representation in a way that can invalidate prior petal specializations, unless constraints are enforced.
what about making it so only certain embedding channels are always handled by certain heads, and each head and the corresponding hull has a fixed embedding capacity? partitioning in embedding space? that will work.


Let:

embed_dim = E

num_heads = H

Fix d_k = E / H

Each head h âˆˆ [0, H-1] is responsible for x[..., h*d_k : (h+1)*d_k]

Consequences:
All operations within head h (QKV projection, scoring, petal activation, etc.) operate only on its own channel slice.

No mixing of representation across heads in x.

No redistribution of learned features unless explicitly done.

This defines a static, spatial routing policy: channels c âˆˆ [h*d_k, (h+1)*d_k] â†” head h â†” petals in ScalarHull_h.

Advantages
Petal Consolidation is Preserved

Because ScalarHull_h only ever sees input from channels h*d_k:(h+1)*d_k, its semantic region is fixed.

Expansion does not affect prior heads.

what this means is we set a fixed embed per head capacity and the number of heads is actually decided by embed dimension. So you can adjust the model's embed dimension to add more capacity.
but this also means that the embedding layer has to be relearned, as do the vectorhulls, and thus the savings are not great- all you've done is to preserve channel information.

in other words, attention is a projection into a latent space. its actually transformative. 
Representational capacity is secondary to alignment and separability of the signal.
learning attention patterns is foundational to a model, fine tuning output is an MLP problem.

in other words, we can assign embed dim, train some petals until fully trained, then lock in and add more petal capacity until we hit the loss we want, and then we can expand embed-dim and add more data to the distribution the model sees- the existing attention *will* see the patterns it thinks it sees, but it will be up to soft routing to deform this into a shape downstream modules can utilize. its ground truth is locked in.

and, we can swap out vectorhulls on different representation problems, or combine their outputs.
a vectorhull trained for the first 32, and another one trained for the second 32, are then each going to project a respective truth. you can have multiple attention mixers in parallel.
in fact, this abstraction isnt even needed.
what you can do is make omnihullblocks only handle a fixed partition of the embed space, and add additional blocks that operate in *parallel* over more embedding space, itself allocated to expanded vocabulary, or a projection. ie, the first layer is a block of 1x1 embedding,
and that creates a latent representation, and then we use a projector to move this up to a wider bottleneck of twice the size, and split it into two heads, and train one on part of the problem domain, and another on more of the problem domain, to fully utilize capacity. then, in the third layer,
we need a wider attention module, because we need to combine all scores, and we score all the parallel block outputs which have been transformed in the third layer system in parallel, and thus the third layer learns to elucidate truth from the second. 
in other words, to grow the model, we recursively create a tree that is also a pyramid:
we double the second stage, then in the third stage consolidate.
 if we want to go further, we can double the second stage again, but now, the 3 and 4 block
go to a second block in the third stage,
and *those* get combined in the fourth stage, using a block like the one in the third stage.
this becomes a "block of blocks" which outputs something the same shape as its input but recursively formulated in such a way that linear capacity growth is possible.
each BoB can be restructured to add capacity by factor of 2 growth, to allow it to adapt to expanded domains.

block of blocks in turn can be stacked in convexgpt to form the base model.



TODO:

short term, medium term, long term memory *outside* of core modules.
Integrate more science from my cognitive theory metaphor - mind is a manifold, like the ripples in a body of water when a rock is dropped into them.
At the center is the input but also the output. memory is stored in the ripples as geometry, used as refraction.

Because train()/eval() are inherited by children, you just need to call .eval() on the submodules you want locked down after any .train() on the full model. The gradient mask (requires_grad) ensures no updates ever flow into those weights.

In other words, I can design a model that continually grows with new knowledge, building integrated logic to freeze petals when information acquisition is optimal for them.
This wont work in the sense of more layers, but it can mean : added multimodal capacity, insofar that we train basic vision into a front end, and then we add parallel front ends which are the same, and glue into the same base model, but you have added petals in base + cloned front end = domain specific visual knowledge, like individual language recognition. Current multimodal is brute-forced over enormous knowledge base and reliable, but not minimal. Visual pattern matching is concept association. Where do the concepts and their corresponding "meaning" live? should base model be modified to conceptualize the presence of visual information, or visual information adapted to the latent space of textual? What is the ultimate original underlying modality that all other knowledge should live in? language?

here's how existence in this reality works for all lifeforms:
acquire information
compare information
select based on comparison
recursively compound first three steps until certainty is reached in selection
recursively compound all but the first step to determine what to do with it.
repeat

internally, this corresponds to recursive:
front end filtration/representation
attention
hulls



unclear benefits:
attention forked into vectorhull/MLP
experiment: attention computes recursively with residual, and its product at each stage sent to vectorhull
vectorhull products accumulated as transformations on x
This has had sometimes utility

shared, interpolating attention between layers, combined with standalone vectorhull transformations
this would use alpha and beta and transformationally update weights before using them
the result is potentially smoother conceptual drifts, but also utility unknown

problems:
semantic windowing and phase shifting is complicated. we use frequency basis, but this is provably not correct.
a "condensing module" designed to "simplify" is possible, but what would it be trained to do?
this requires more research.

multimodal:
proposal:
my experiments show first layer of gabor filter with phase rotations of conv is suitable for visual input.
tokenization following is possible- requires first condensing via semantic windowing across two axes,
then performing reshaping:
take image, perform gabor filtration. provide 8 outputs of shape n,n in greyscale or individual color channel.
parallel channels as embedding channels if so desired.
cut and cut transposed to form:
nA nB
process each through "embedder" module of attention, vectorhull
take each and recompose them into 2d array, transpose it, decompose it
eA,eAt,Eb,EbT- stack all as embeddings in paralllel

you now have your visual embedding of shape:
batch,heightxwidth,colorchannelsxembeddingdimsx4
bottleneck or process as desired to obtain latent representation

Inline peer comments: OpenAIÂ Assistant (o3)

JR: problem of epistemic stability under parameter updates â€” a consolidation-versus-redistribution dilemmaâ€¦

o3:Â You are independently reinventing ideas close to Elastic Weight Consolidation (EWC) and Synaptic Intelligence â€” both measure parameter â€œimportanceâ€ and scale future gradient steps inversely. Your twoâ€‘bucket resistance (one bucket per sign) is novel; it prevents pingâ€‘pong moves while still letting weights drift when a new consensus emerges.  The viscosity â†” temperature metaphor is apt:  consider an online Fisher information estimate as â€œpressureâ€, and an exponentiallyâ€‘decaying temperature as you described.  A small proofâ€‘ofâ€‘concept can be built by storing two EMA buffers per parameter:

Î”_pos  â† EMAâ‚Š(â€–gÂ·ğŸ™[g>0]â€–)  # positiveâ€‘side credit
Î”_neg  â† EMAâ‚‹(â€–gÂ·ğŸ™[g<0]â€–)  # negativeâ€‘side credit
Î·_eff  = Î· / (1 + Î”_opposite)

where Î· is the base LR and opposite chooses the buffer opposite in sign to the current gradient.

JR: â€¦weights coolâ€¦cryogenic freezerâ€¦ percentage 1â€‘1/e â€¦

o3:Â Cooling schedule can be tied to the cumulative KLâ€‘divergence between successive parameter snapshots.  When KLÂ <Â Ï„_freeze for all weights in a petal during an epoch, mark that petal â€œsolidâ€.  The 1â€‘1/e threshold you cite emerges if we adopt an exponential survival model for weight updates â€” nice statistical fit.

Implementation sketch:

solid = (kl_ema < tau).all()
if solid:
    for p in petal.parameters():
        p.requires_grad_(False)

Expansion trigger:  when solid_petals / total_petals > Ï† (say Ï†â€¯=â€¯0.8), allocate new_P = ceil(0.5 * total_petals) fresh petals.

JR: fixed embeddingâ€‘channel ownership per headâ€¦ preserves specialization

o3:Â Yes â€” channel partitioning creates disentangled subâ€‘spaces.  To avoid wasted capacity when some heads underâ€‘utilise their slice, you can add a crossâ€‘head mixer after freezing, implemented as a learned doublyâ€‘stochastic matrix acting on channel slices.  Because it is applied postâ€‘freeze, it cannot corrupt earlier consolidation.

JR: â€œblockâ€‘ofâ€‘blocksâ€ pyramid for capacity growthâ€¦

o3:Â This naturally matches a binary tree mixtureâ€‘ofâ€‘experts.  Each BoB node is convex, so the whole tree remains convex.  Tip: store router weights as Dirichlet variables with temperature annealing â€” you get automatic loadâ€‘balancing as new leaves appear.

JR: short/medium/longâ€‘term memory outside core modulesâ€¦

o3:Â A convexâ€‘compatible external memory can be built with a keyâ€‘value DB where keys are ICNNâ€‘embedded queries and values are frozen vectors.  Retrieval uses your convex Mixer (rowâ€‘simplex) and therefore preserves global convexity.

JR: multimodal vision frontâ€‘end with Gabor filtersâ€¦

o3:Â Instead of handâ€‘crafted Gabors you might reuse the positiveâ€‘linear + Softplus stack: initialise first conv layer with frozen Gabors (positive weights), then allow higher layers (also positive) to learn convex feature maps.  The reshape & transpose trick you describe is similar to Perceiver IOâ€™s Fourier position folding â€” proven to scale.

General Intuitions & Next Steps

Resistance buffers vs. secondâ€‘order curvature.  You can approximate curvature cheaply with diagonal GGN/Fisher; merge it with your resistance scalar for a â€œviscoâ€‘elasticâ€ LR.

Petal allocation as Poisson process.  Model petalâ€‘freezing events as arrivals in Poisson(Î»).  Expected time until expansion â‰ˆ 1/Î» â€” gives principled schedule.

Convexity audit automation.  Add a unitâ€‘test that samples two random inputs xâ‚,xâ‚‚, a random Î», and checks f(Î»xâ‚+(1â€‘Î»)xâ‚‚) â‰¤ Î»f(xâ‚)+(1â€‘Î»)f(xâ‚‚)+Îµ for the whole network.  Catch regressions early.

Empirical validation.  Track hull radius per layer during training; it should monotonically contract or stay stable.

â€”Â OpenAIÂ AssistantÂ (o3)




